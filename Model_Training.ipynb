{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "lbDHBBQj7Kj6",
        "outputId": "6e067572-f5c4-443a-bb4d-c411ea243c47"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.51.3)\n",
            "Requirement already satisfied: tokenizers in /usr/local/lib/python3.11/dist-packages (0.21.1)\n",
            "Collecting datasets\n",
            "  Downloading datasets-3.6.0-py3-none-any.whl.metadata (19 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.30.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
            "Collecting xxhash (from datasets)\n",
            "  Downloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting multiprocess<0.70.17 (from datasets)\n",
            "  Downloading multiprocess-0.70.16-py311-none-any.whl.metadata (7.2 kB)\n",
            "Collecting fsspec<=2025.3.0,>=2023.1.0 (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets)\n",
            "  Downloading fsspec-2025.3.0-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.11.15)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.13.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.4.26)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.6.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.4.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.20.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
            "Downloading datasets-3.6.0-py3-none-any.whl (491 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m491.5/491.5 kB\u001b[0m \u001b[31m33.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m12.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fsspec-2025.3.0-py3-none-any.whl (193 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m193.6/193.6 kB\u001b[0m \u001b[31m16.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading multiprocess-0.70.16-py311-none-any.whl (143 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.5/143.5 kB\u001b[0m \u001b[31m14.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.8/194.8 kB\u001b[0m \u001b[31m19.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: xxhash, fsspec, dill, multiprocess, datasets\n",
            "  Attempting uninstall: fsspec\n",
            "    Found existing installation: fsspec 2025.3.2\n",
            "    Uninstalling fsspec-2025.3.2:\n",
            "      Successfully uninstalled fsspec-2025.3.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "gcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2025.3.0 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cublas-cu12==12.4.5.8; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cublas-cu12 12.5.3.2 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-cupti-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-cupti-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-nvrtc-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-nvrtc-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-runtime-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-runtime-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cudnn-cu12==9.1.0.70; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cudnn-cu12 9.3.0.75 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cufft-cu12==11.2.1.3; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cufft-cu12 11.2.3.61 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-curand-cu12==10.3.5.147; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-curand-cu12 10.3.6.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cusolver-cu12==11.6.1.9; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusolver-cu12 11.6.3.83 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cusparse-cu12==12.3.1.170; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusparse-cu12 12.5.1.3 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-nvjitlink-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-nvjitlink-cu12 12.5.82 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed datasets-3.6.0 dill-0.3.8 fsspec-2025.3.0 multiprocess-0.70.16 xxhash-3.5.0\n",
            "Mounted at /content/drive\n",
            "Tokenizer loaded from existing files.\n",
            "File size: 3269575 bytes\n",
            "First 100 characters: PASTIC :: The Information Provider Skip to main content Main navigation Home Profile About PASTIC Vi...\n",
            "Total number of tokens: 3132091\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/transformers/data/datasets/language_modeling.py:53: FutureWarning: This dataset will be removed from the library soon, preprocessing should be handled with the 🤗 Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/language-modeling/run_mlm.py\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training dataset size: 3006 examples\n",
            "Validation dataset size: 52 examples\n",
            "Total number of trainable parameters: 171,105,024\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "`loss_type=None` was set in the config but it is unrecognised.Using the default loss: `ForCausalLMLoss`.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='4509' max='4509' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [4509/4509 1:10:03, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>5.711400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>4.420500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>5.998000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>4.575600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>4.856900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>3.923800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>4.053100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8</td>\n",
              "      <td>3.882500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9</td>\n",
              "      <td>3.774700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>3.866700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>11</td>\n",
              "      <td>3.718600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>12</td>\n",
              "      <td>3.758400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>13</td>\n",
              "      <td>3.776900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>14</td>\n",
              "      <td>3.580000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>15</td>\n",
              "      <td>3.637600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>16</td>\n",
              "      <td>3.637100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>17</td>\n",
              "      <td>3.927600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>18</td>\n",
              "      <td>3.794700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>19</td>\n",
              "      <td>3.734800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20</td>\n",
              "      <td>3.724000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>21</td>\n",
              "      <td>3.519200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>22</td>\n",
              "      <td>3.407500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>23</td>\n",
              "      <td>5.895500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>24</td>\n",
              "      <td>3.281700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>25</td>\n",
              "      <td>3.468600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>26</td>\n",
              "      <td>3.441000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>27</td>\n",
              "      <td>3.331500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>28</td>\n",
              "      <td>3.371400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>29</td>\n",
              "      <td>3.532200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>30</td>\n",
              "      <td>3.249600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>31</td>\n",
              "      <td>3.249500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>32</td>\n",
              "      <td>3.244600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>33</td>\n",
              "      <td>3.233700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>34</td>\n",
              "      <td>3.134400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>35</td>\n",
              "      <td>3.337000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>36</td>\n",
              "      <td>3.795200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>37</td>\n",
              "      <td>3.357900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>38</td>\n",
              "      <td>3.269600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>39</td>\n",
              "      <td>3.217200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>40</td>\n",
              "      <td>3.314800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>41</td>\n",
              "      <td>3.065500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>42</td>\n",
              "      <td>3.118400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>43</td>\n",
              "      <td>3.112100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>44</td>\n",
              "      <td>3.030600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>45</td>\n",
              "      <td>2.896200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>46</td>\n",
              "      <td>3.201700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>47</td>\n",
              "      <td>3.115500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>48</td>\n",
              "      <td>3.163100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>49</td>\n",
              "      <td>3.131300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>2.912200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>51</td>\n",
              "      <td>3.138500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>52</td>\n",
              "      <td>2.879700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>53</td>\n",
              "      <td>2.889300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>54</td>\n",
              "      <td>2.863800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>55</td>\n",
              "      <td>3.086700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>56</td>\n",
              "      <td>2.804900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>57</td>\n",
              "      <td>3.227100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>58</td>\n",
              "      <td>2.971300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>59</td>\n",
              "      <td>3.049600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>60</td>\n",
              "      <td>2.981100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>61</td>\n",
              "      <td>2.921800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>62</td>\n",
              "      <td>2.845700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>63</td>\n",
              "      <td>2.841700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>64</td>\n",
              "      <td>2.939100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>65</td>\n",
              "      <td>2.690800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>66</td>\n",
              "      <td>2.975400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>67</td>\n",
              "      <td>2.997900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>68</td>\n",
              "      <td>2.997700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>69</td>\n",
              "      <td>3.158900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>70</td>\n",
              "      <td>3.087700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>71</td>\n",
              "      <td>2.926000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>72</td>\n",
              "      <td>2.898100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>73</td>\n",
              "      <td>2.937500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>74</td>\n",
              "      <td>2.997700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>75</td>\n",
              "      <td>3.023800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>76</td>\n",
              "      <td>3.026400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>77</td>\n",
              "      <td>2.881300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>78</td>\n",
              "      <td>2.971600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>79</td>\n",
              "      <td>2.813400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>80</td>\n",
              "      <td>3.244800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>81</td>\n",
              "      <td>3.049700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>82</td>\n",
              "      <td>2.919700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>83</td>\n",
              "      <td>2.938800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>84</td>\n",
              "      <td>2.877800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>85</td>\n",
              "      <td>2.761300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>86</td>\n",
              "      <td>2.888700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>87</td>\n",
              "      <td>2.819600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>88</td>\n",
              "      <td>2.825900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>89</td>\n",
              "      <td>2.720200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>90</td>\n",
              "      <td>2.896300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>91</td>\n",
              "      <td>2.944100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>92</td>\n",
              "      <td>2.907900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>93</td>\n",
              "      <td>2.983100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>94</td>\n",
              "      <td>2.985000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>95</td>\n",
              "      <td>2.855900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>96</td>\n",
              "      <td>2.920300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>97</td>\n",
              "      <td>2.847500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>98</td>\n",
              "      <td>2.706800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>99</td>\n",
              "      <td>2.909000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>2.895800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>101</td>\n",
              "      <td>2.647200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>102</td>\n",
              "      <td>3.090400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>103</td>\n",
              "      <td>2.797400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>104</td>\n",
              "      <td>2.718000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>105</td>\n",
              "      <td>2.881800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>106</td>\n",
              "      <td>2.939300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>107</td>\n",
              "      <td>2.934500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>108</td>\n",
              "      <td>2.861600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>109</td>\n",
              "      <td>3.079400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>110</td>\n",
              "      <td>2.645400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>111</td>\n",
              "      <td>2.949900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>112</td>\n",
              "      <td>2.770700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>113</td>\n",
              "      <td>2.784300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>114</td>\n",
              "      <td>2.898700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>115</td>\n",
              "      <td>2.733800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>116</td>\n",
              "      <td>2.678200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>117</td>\n",
              "      <td>2.912800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>118</td>\n",
              "      <td>2.849100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>119</td>\n",
              "      <td>2.853500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>120</td>\n",
              "      <td>2.633400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>121</td>\n",
              "      <td>2.964900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>122</td>\n",
              "      <td>2.718000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>123</td>\n",
              "      <td>2.745700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>124</td>\n",
              "      <td>2.653200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>125</td>\n",
              "      <td>2.819600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>126</td>\n",
              "      <td>2.954700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>127</td>\n",
              "      <td>2.592100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>128</td>\n",
              "      <td>2.913800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>129</td>\n",
              "      <td>2.857000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>130</td>\n",
              "      <td>2.868300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>131</td>\n",
              "      <td>2.839200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>132</td>\n",
              "      <td>2.827000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>133</td>\n",
              "      <td>2.894000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>134</td>\n",
              "      <td>2.712500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>135</td>\n",
              "      <td>2.831800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>136</td>\n",
              "      <td>2.921100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>137</td>\n",
              "      <td>2.796200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>138</td>\n",
              "      <td>2.807500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>139</td>\n",
              "      <td>2.644700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>140</td>\n",
              "      <td>2.805200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>141</td>\n",
              "      <td>2.815500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>142</td>\n",
              "      <td>2.732000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>143</td>\n",
              "      <td>2.665400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>144</td>\n",
              "      <td>2.583400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>145</td>\n",
              "      <td>2.878500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>146</td>\n",
              "      <td>2.869900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>147</td>\n",
              "      <td>2.807900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>148</td>\n",
              "      <td>2.856800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>149</td>\n",
              "      <td>2.885700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>150</td>\n",
              "      <td>2.858900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>151</td>\n",
              "      <td>2.833300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>152</td>\n",
              "      <td>2.730500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>153</td>\n",
              "      <td>2.793500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>154</td>\n",
              "      <td>2.756700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>155</td>\n",
              "      <td>2.775800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>156</td>\n",
              "      <td>2.817700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>157</td>\n",
              "      <td>2.559300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>158</td>\n",
              "      <td>2.813300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>159</td>\n",
              "      <td>2.741600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>160</td>\n",
              "      <td>3.021000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>161</td>\n",
              "      <td>2.768600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>162</td>\n",
              "      <td>2.870400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>163</td>\n",
              "      <td>2.719700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>164</td>\n",
              "      <td>2.829000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>165</td>\n",
              "      <td>2.886700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>166</td>\n",
              "      <td>2.715400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>167</td>\n",
              "      <td>2.977100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>168</td>\n",
              "      <td>2.775800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>169</td>\n",
              "      <td>2.660700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>170</td>\n",
              "      <td>2.614000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>171</td>\n",
              "      <td>2.588000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>172</td>\n",
              "      <td>2.846100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>173</td>\n",
              "      <td>2.747500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>174</td>\n",
              "      <td>2.986300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>175</td>\n",
              "      <td>2.741700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>176</td>\n",
              "      <td>2.869100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>177</td>\n",
              "      <td>3.065900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>178</td>\n",
              "      <td>2.684500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>179</td>\n",
              "      <td>2.600200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>180</td>\n",
              "      <td>2.749800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>181</td>\n",
              "      <td>2.893200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>182</td>\n",
              "      <td>2.767400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>183</td>\n",
              "      <td>2.826000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>184</td>\n",
              "      <td>2.728500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>185</td>\n",
              "      <td>2.743800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>186</td>\n",
              "      <td>2.841000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>187</td>\n",
              "      <td>2.855300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>188</td>\n",
              "      <td>2.662500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>189</td>\n",
              "      <td>2.823200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>190</td>\n",
              "      <td>2.764500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>191</td>\n",
              "      <td>2.735400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>192</td>\n",
              "      <td>2.561300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>193</td>\n",
              "      <td>2.838200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>194</td>\n",
              "      <td>2.673900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>195</td>\n",
              "      <td>2.994200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>196</td>\n",
              "      <td>2.670500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>197</td>\n",
              "      <td>2.791300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>198</td>\n",
              "      <td>2.646500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>199</td>\n",
              "      <td>2.697300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>200</td>\n",
              "      <td>2.700000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>201</td>\n",
              "      <td>2.782200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>202</td>\n",
              "      <td>2.690700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>203</td>\n",
              "      <td>2.660300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>204</td>\n",
              "      <td>2.604900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>205</td>\n",
              "      <td>2.768000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>206</td>\n",
              "      <td>2.720100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>207</td>\n",
              "      <td>2.616200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>208</td>\n",
              "      <td>2.773800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>209</td>\n",
              "      <td>2.659700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>210</td>\n",
              "      <td>2.708400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>211</td>\n",
              "      <td>2.716300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>212</td>\n",
              "      <td>2.806800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>213</td>\n",
              "      <td>2.480300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>214</td>\n",
              "      <td>2.673900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>215</td>\n",
              "      <td>2.879100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>216</td>\n",
              "      <td>2.867800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>217</td>\n",
              "      <td>3.009300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>218</td>\n",
              "      <td>2.593600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>219</td>\n",
              "      <td>2.422700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>220</td>\n",
              "      <td>2.608600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>221</td>\n",
              "      <td>2.722800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>222</td>\n",
              "      <td>2.598200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>223</td>\n",
              "      <td>2.878200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>224</td>\n",
              "      <td>2.710300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>225</td>\n",
              "      <td>2.774700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>226</td>\n",
              "      <td>2.691900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>227</td>\n",
              "      <td>2.527700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>228</td>\n",
              "      <td>2.557500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>229</td>\n",
              "      <td>2.740400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>230</td>\n",
              "      <td>2.782700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>231</td>\n",
              "      <td>2.767000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>232</td>\n",
              "      <td>2.730400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>233</td>\n",
              "      <td>2.775200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>234</td>\n",
              "      <td>2.685100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>235</td>\n",
              "      <td>2.672900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>236</td>\n",
              "      <td>2.686800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>237</td>\n",
              "      <td>2.744100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>238</td>\n",
              "      <td>2.755600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>239</td>\n",
              "      <td>3.060900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>240</td>\n",
              "      <td>2.607800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>241</td>\n",
              "      <td>2.858500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>242</td>\n",
              "      <td>2.743100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>243</td>\n",
              "      <td>2.657100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>244</td>\n",
              "      <td>2.742900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>245</td>\n",
              "      <td>2.861800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>246</td>\n",
              "      <td>2.831300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>247</td>\n",
              "      <td>2.572400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>248</td>\n",
              "      <td>2.597100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>249</td>\n",
              "      <td>2.598200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>250</td>\n",
              "      <td>2.576200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>251</td>\n",
              "      <td>2.828900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>252</td>\n",
              "      <td>2.856400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>253</td>\n",
              "      <td>2.719100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>254</td>\n",
              "      <td>2.879600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>255</td>\n",
              "      <td>2.598500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>256</td>\n",
              "      <td>2.810500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>257</td>\n",
              "      <td>2.677100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>258</td>\n",
              "      <td>2.490900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>259</td>\n",
              "      <td>2.723700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>260</td>\n",
              "      <td>2.785200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>261</td>\n",
              "      <td>2.578500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>262</td>\n",
              "      <td>2.975900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>263</td>\n",
              "      <td>2.644800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>264</td>\n",
              "      <td>2.612700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>265</td>\n",
              "      <td>2.739400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>266</td>\n",
              "      <td>2.654100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>267</td>\n",
              "      <td>2.970100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>268</td>\n",
              "      <td>2.748000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>269</td>\n",
              "      <td>2.700200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>270</td>\n",
              "      <td>2.737700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>271</td>\n",
              "      <td>2.814200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>272</td>\n",
              "      <td>2.814800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>273</td>\n",
              "      <td>2.706000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>274</td>\n",
              "      <td>2.610000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>275</td>\n",
              "      <td>2.731200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>276</td>\n",
              "      <td>2.786200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>277</td>\n",
              "      <td>2.679000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>278</td>\n",
              "      <td>2.585100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>279</td>\n",
              "      <td>2.572900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>280</td>\n",
              "      <td>2.753600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>281</td>\n",
              "      <td>2.771700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>282</td>\n",
              "      <td>2.939500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>283</td>\n",
              "      <td>2.721900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>284</td>\n",
              "      <td>2.715000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>285</td>\n",
              "      <td>2.689500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>286</td>\n",
              "      <td>2.649300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>287</td>\n",
              "      <td>2.644200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>288</td>\n",
              "      <td>2.660900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>289</td>\n",
              "      <td>2.510800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>290</td>\n",
              "      <td>2.670100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>291</td>\n",
              "      <td>2.457900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>292</td>\n",
              "      <td>2.703000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>293</td>\n",
              "      <td>2.739900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>294</td>\n",
              "      <td>2.512000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>295</td>\n",
              "      <td>2.639700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>296</td>\n",
              "      <td>2.910200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>297</td>\n",
              "      <td>2.739100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>298</td>\n",
              "      <td>2.777900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>299</td>\n",
              "      <td>2.685200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>300</td>\n",
              "      <td>2.743300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>301</td>\n",
              "      <td>2.804200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>302</td>\n",
              "      <td>2.658600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>303</td>\n",
              "      <td>2.675600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>304</td>\n",
              "      <td>2.701500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>305</td>\n",
              "      <td>2.790300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>306</td>\n",
              "      <td>2.753000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>307</td>\n",
              "      <td>2.847500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>308</td>\n",
              "      <td>2.395700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>309</td>\n",
              "      <td>2.757600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>310</td>\n",
              "      <td>2.704300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>311</td>\n",
              "      <td>2.520000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>312</td>\n",
              "      <td>2.744500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>313</td>\n",
              "      <td>2.735200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>314</td>\n",
              "      <td>2.746000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>315</td>\n",
              "      <td>2.647900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>316</td>\n",
              "      <td>2.778800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>317</td>\n",
              "      <td>2.821800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>318</td>\n",
              "      <td>2.792000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>319</td>\n",
              "      <td>2.764900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>320</td>\n",
              "      <td>2.766700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>321</td>\n",
              "      <td>2.777200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>322</td>\n",
              "      <td>2.513900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>323</td>\n",
              "      <td>2.766500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>324</td>\n",
              "      <td>2.685500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>325</td>\n",
              "      <td>2.692300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>326</td>\n",
              "      <td>2.808300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>327</td>\n",
              "      <td>2.706600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>328</td>\n",
              "      <td>2.642800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>329</td>\n",
              "      <td>2.848800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>330</td>\n",
              "      <td>2.840600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>331</td>\n",
              "      <td>2.693800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>332</td>\n",
              "      <td>2.719600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>333</td>\n",
              "      <td>2.477400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>334</td>\n",
              "      <td>2.787600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>335</td>\n",
              "      <td>2.487500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>336</td>\n",
              "      <td>2.700600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>337</td>\n",
              "      <td>2.741700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>338</td>\n",
              "      <td>2.533400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>339</td>\n",
              "      <td>2.779800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>340</td>\n",
              "      <td>2.668200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>341</td>\n",
              "      <td>2.759000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>342</td>\n",
              "      <td>2.900400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>343</td>\n",
              "      <td>2.762800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>344</td>\n",
              "      <td>2.616700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>345</td>\n",
              "      <td>2.657900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>346</td>\n",
              "      <td>2.543500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>347</td>\n",
              "      <td>2.570600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>348</td>\n",
              "      <td>2.650000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>349</td>\n",
              "      <td>2.803400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>350</td>\n",
              "      <td>2.486200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>351</td>\n",
              "      <td>2.812000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>352</td>\n",
              "      <td>2.636100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>353</td>\n",
              "      <td>2.739100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>354</td>\n",
              "      <td>2.807600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>355</td>\n",
              "      <td>2.761000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>356</td>\n",
              "      <td>2.515400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>357</td>\n",
              "      <td>2.438700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>358</td>\n",
              "      <td>2.432800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>359</td>\n",
              "      <td>2.400900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>360</td>\n",
              "      <td>2.646100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>361</td>\n",
              "      <td>2.783700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>362</td>\n",
              "      <td>2.715100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>363</td>\n",
              "      <td>2.735900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>364</td>\n",
              "      <td>2.191000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>365</td>\n",
              "      <td>2.480200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>366</td>\n",
              "      <td>2.682900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>367</td>\n",
              "      <td>2.819600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>368</td>\n",
              "      <td>2.702700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>369</td>\n",
              "      <td>2.768700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>370</td>\n",
              "      <td>2.769900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>371</td>\n",
              "      <td>2.737400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>372</td>\n",
              "      <td>2.634700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>373</td>\n",
              "      <td>2.724800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>374</td>\n",
              "      <td>2.926900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>375</td>\n",
              "      <td>2.711000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>376</td>\n",
              "      <td>2.660200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>377</td>\n",
              "      <td>2.685800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>378</td>\n",
              "      <td>2.634400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>379</td>\n",
              "      <td>2.930700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>380</td>\n",
              "      <td>2.707600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>381</td>\n",
              "      <td>2.735500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>382</td>\n",
              "      <td>2.556800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>383</td>\n",
              "      <td>2.815800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>384</td>\n",
              "      <td>2.764800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>385</td>\n",
              "      <td>2.728900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>386</td>\n",
              "      <td>2.539100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>387</td>\n",
              "      <td>2.760000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>388</td>\n",
              "      <td>2.810500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>389</td>\n",
              "      <td>2.788900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>390</td>\n",
              "      <td>2.663300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>391</td>\n",
              "      <td>2.673400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>392</td>\n",
              "      <td>2.460500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>393</td>\n",
              "      <td>2.764300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>394</td>\n",
              "      <td>2.726200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>395</td>\n",
              "      <td>2.618300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>396</td>\n",
              "      <td>2.664500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>397</td>\n",
              "      <td>2.736400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>398</td>\n",
              "      <td>2.550300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>399</td>\n",
              "      <td>2.707500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>400</td>\n",
              "      <td>2.758500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>401</td>\n",
              "      <td>2.668900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>402</td>\n",
              "      <td>2.768700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>403</td>\n",
              "      <td>2.495700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>404</td>\n",
              "      <td>2.808300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>405</td>\n",
              "      <td>2.673900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>406</td>\n",
              "      <td>2.819400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>407</td>\n",
              "      <td>2.783200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>408</td>\n",
              "      <td>2.602400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>409</td>\n",
              "      <td>2.654100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>410</td>\n",
              "      <td>2.803300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>411</td>\n",
              "      <td>2.519800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>412</td>\n",
              "      <td>2.940800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>413</td>\n",
              "      <td>2.467400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>414</td>\n",
              "      <td>2.579700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>415</td>\n",
              "      <td>2.912700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>416</td>\n",
              "      <td>2.472600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>417</td>\n",
              "      <td>2.533900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>418</td>\n",
              "      <td>2.802900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>419</td>\n",
              "      <td>2.498400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>420</td>\n",
              "      <td>2.632800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>421</td>\n",
              "      <td>2.529100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>422</td>\n",
              "      <td>2.835800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>423</td>\n",
              "      <td>2.708700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>424</td>\n",
              "      <td>2.688400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>425</td>\n",
              "      <td>2.580800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>426</td>\n",
              "      <td>2.839500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>427</td>\n",
              "      <td>2.712000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>428</td>\n",
              "      <td>2.631100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>429</td>\n",
              "      <td>2.441700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>430</td>\n",
              "      <td>2.775500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>431</td>\n",
              "      <td>2.654900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>432</td>\n",
              "      <td>2.829500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>433</td>\n",
              "      <td>2.729300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>434</td>\n",
              "      <td>2.631600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>435</td>\n",
              "      <td>2.707100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>436</td>\n",
              "      <td>2.525800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>437</td>\n",
              "      <td>2.653500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>438</td>\n",
              "      <td>2.456900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>439</td>\n",
              "      <td>2.817800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>440</td>\n",
              "      <td>3.058000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>441</td>\n",
              "      <td>2.842700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>442</td>\n",
              "      <td>2.623100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>443</td>\n",
              "      <td>2.776400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>444</td>\n",
              "      <td>2.648900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>445</td>\n",
              "      <td>2.694800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>446</td>\n",
              "      <td>2.784100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>447</td>\n",
              "      <td>2.261700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>448</td>\n",
              "      <td>2.651300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>449</td>\n",
              "      <td>2.535700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>450</td>\n",
              "      <td>2.679300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>451</td>\n",
              "      <td>2.694900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>452</td>\n",
              "      <td>2.697500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>453</td>\n",
              "      <td>2.668800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>454</td>\n",
              "      <td>2.716500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>455</td>\n",
              "      <td>2.636700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>456</td>\n",
              "      <td>2.717400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>457</td>\n",
              "      <td>2.769500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>458</td>\n",
              "      <td>2.654900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>459</td>\n",
              "      <td>2.683800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>460</td>\n",
              "      <td>2.445400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>461</td>\n",
              "      <td>2.674100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>462</td>\n",
              "      <td>2.266800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>463</td>\n",
              "      <td>2.720000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>464</td>\n",
              "      <td>2.625500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>465</td>\n",
              "      <td>2.596100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>466</td>\n",
              "      <td>2.683400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>467</td>\n",
              "      <td>2.291000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>468</td>\n",
              "      <td>2.675300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>469</td>\n",
              "      <td>2.683400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>470</td>\n",
              "      <td>2.701300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>471</td>\n",
              "      <td>2.605600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>472</td>\n",
              "      <td>2.544300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>473</td>\n",
              "      <td>2.697100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>474</td>\n",
              "      <td>2.932900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>475</td>\n",
              "      <td>2.765400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>476</td>\n",
              "      <td>2.671300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>477</td>\n",
              "      <td>2.597200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>478</td>\n",
              "      <td>2.652900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>479</td>\n",
              "      <td>2.694000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>480</td>\n",
              "      <td>2.802600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>481</td>\n",
              "      <td>2.701900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>482</td>\n",
              "      <td>2.874700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>483</td>\n",
              "      <td>2.650900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>484</td>\n",
              "      <td>2.396700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>485</td>\n",
              "      <td>2.716900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>486</td>\n",
              "      <td>2.666400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>487</td>\n",
              "      <td>2.595300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>488</td>\n",
              "      <td>2.663500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>489</td>\n",
              "      <td>2.678200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>490</td>\n",
              "      <td>2.749300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>491</td>\n",
              "      <td>2.792800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>492</td>\n",
              "      <td>2.435500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>493</td>\n",
              "      <td>2.744000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>494</td>\n",
              "      <td>2.672100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>495</td>\n",
              "      <td>2.635400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>496</td>\n",
              "      <td>2.371300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>497</td>\n",
              "      <td>2.480000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>498</td>\n",
              "      <td>2.649400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>499</td>\n",
              "      <td>2.489300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>500</td>\n",
              "      <td>2.579300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>501</td>\n",
              "      <td>2.622900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>502</td>\n",
              "      <td>2.744700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>503</td>\n",
              "      <td>2.405100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>504</td>\n",
              "      <td>2.655700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>505</td>\n",
              "      <td>2.672600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>506</td>\n",
              "      <td>2.820100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>507</td>\n",
              "      <td>2.678200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>508</td>\n",
              "      <td>2.591700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>509</td>\n",
              "      <td>2.536300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>510</td>\n",
              "      <td>2.705800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>511</td>\n",
              "      <td>2.602200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>512</td>\n",
              "      <td>2.713100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>513</td>\n",
              "      <td>2.733700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>514</td>\n",
              "      <td>2.709400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>515</td>\n",
              "      <td>2.354600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>516</td>\n",
              "      <td>2.661000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>517</td>\n",
              "      <td>2.659000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>518</td>\n",
              "      <td>2.748900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>519</td>\n",
              "      <td>2.474200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>520</td>\n",
              "      <td>2.714000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>521</td>\n",
              "      <td>2.747800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>522</td>\n",
              "      <td>2.672200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>523</td>\n",
              "      <td>2.410600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>524</td>\n",
              "      <td>2.739600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>525</td>\n",
              "      <td>2.684200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>526</td>\n",
              "      <td>2.699300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>527</td>\n",
              "      <td>2.560900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>528</td>\n",
              "      <td>2.495600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>529</td>\n",
              "      <td>2.569300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>530</td>\n",
              "      <td>2.589700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>531</td>\n",
              "      <td>2.690100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>532</td>\n",
              "      <td>2.701500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>533</td>\n",
              "      <td>2.652800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>534</td>\n",
              "      <td>2.839800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>535</td>\n",
              "      <td>2.677400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>536</td>\n",
              "      <td>2.538800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>537</td>\n",
              "      <td>2.475300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>538</td>\n",
              "      <td>2.407300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>539</td>\n",
              "      <td>2.646800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>540</td>\n",
              "      <td>2.508400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>541</td>\n",
              "      <td>2.353200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>542</td>\n",
              "      <td>2.679900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>543</td>\n",
              "      <td>2.580900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>544</td>\n",
              "      <td>2.621300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>545</td>\n",
              "      <td>2.653000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>546</td>\n",
              "      <td>2.665900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>547</td>\n",
              "      <td>2.713600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>548</td>\n",
              "      <td>2.664600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>549</td>\n",
              "      <td>2.627200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>550</td>\n",
              "      <td>2.509800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>551</td>\n",
              "      <td>2.596100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>552</td>\n",
              "      <td>2.693100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>553</td>\n",
              "      <td>2.768600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>554</td>\n",
              "      <td>2.426200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>555</td>\n",
              "      <td>2.718600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>556</td>\n",
              "      <td>2.726000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>557</td>\n",
              "      <td>2.636100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>558</td>\n",
              "      <td>2.503600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>559</td>\n",
              "      <td>2.709200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>560</td>\n",
              "      <td>2.658100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>561</td>\n",
              "      <td>2.685200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>562</td>\n",
              "      <td>2.610100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>563</td>\n",
              "      <td>2.703000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>564</td>\n",
              "      <td>2.168200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>565</td>\n",
              "      <td>2.627000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>566</td>\n",
              "      <td>2.518500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>567</td>\n",
              "      <td>2.772700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>568</td>\n",
              "      <td>2.289700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>569</td>\n",
              "      <td>2.358700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>570</td>\n",
              "      <td>2.657300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>571</td>\n",
              "      <td>2.587000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>572</td>\n",
              "      <td>2.677000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>573</td>\n",
              "      <td>2.936600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>574</td>\n",
              "      <td>2.745700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>575</td>\n",
              "      <td>2.392200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>576</td>\n",
              "      <td>2.756300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>577</td>\n",
              "      <td>2.696000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>578</td>\n",
              "      <td>2.770500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>579</td>\n",
              "      <td>2.472000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>580</td>\n",
              "      <td>2.690600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>581</td>\n",
              "      <td>2.507000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>582</td>\n",
              "      <td>2.328400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>583</td>\n",
              "      <td>2.762600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>584</td>\n",
              "      <td>2.734000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>585</td>\n",
              "      <td>2.688900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>586</td>\n",
              "      <td>2.619100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>587</td>\n",
              "      <td>2.297500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>588</td>\n",
              "      <td>2.612200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>589</td>\n",
              "      <td>2.448400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>590</td>\n",
              "      <td>2.795200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>591</td>\n",
              "      <td>2.736600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>592</td>\n",
              "      <td>2.591700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>593</td>\n",
              "      <td>2.463900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>594</td>\n",
              "      <td>2.735100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>595</td>\n",
              "      <td>2.720100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>596</td>\n",
              "      <td>2.714600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>597</td>\n",
              "      <td>2.641300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>598</td>\n",
              "      <td>2.725700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>599</td>\n",
              "      <td>2.624500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>600</td>\n",
              "      <td>2.456100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>601</td>\n",
              "      <td>2.647400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>602</td>\n",
              "      <td>2.614100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>603</td>\n",
              "      <td>2.541500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>604</td>\n",
              "      <td>2.443500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>605</td>\n",
              "      <td>2.641500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>606</td>\n",
              "      <td>2.683100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>607</td>\n",
              "      <td>2.643100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>608</td>\n",
              "      <td>2.778300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>609</td>\n",
              "      <td>2.675700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>610</td>\n",
              "      <td>2.719100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>611</td>\n",
              "      <td>2.717600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>612</td>\n",
              "      <td>2.780500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>613</td>\n",
              "      <td>2.703300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>614</td>\n",
              "      <td>2.632400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>615</td>\n",
              "      <td>2.709400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>616</td>\n",
              "      <td>2.728500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>617</td>\n",
              "      <td>2.530300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>618</td>\n",
              "      <td>2.784400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>619</td>\n",
              "      <td>2.643800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>620</td>\n",
              "      <td>2.389200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>621</td>\n",
              "      <td>2.365500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>622</td>\n",
              "      <td>2.677600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>623</td>\n",
              "      <td>2.751600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>624</td>\n",
              "      <td>2.399200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>625</td>\n",
              "      <td>2.418300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>626</td>\n",
              "      <td>2.602200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>627</td>\n",
              "      <td>2.601500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>628</td>\n",
              "      <td>2.677500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>629</td>\n",
              "      <td>2.772700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>630</td>\n",
              "      <td>2.645700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>631</td>\n",
              "      <td>2.693500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>632</td>\n",
              "      <td>2.403100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>633</td>\n",
              "      <td>2.752500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>634</td>\n",
              "      <td>2.887200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>635</td>\n",
              "      <td>2.702500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>636</td>\n",
              "      <td>2.705600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>637</td>\n",
              "      <td>2.422100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>638</td>\n",
              "      <td>2.688300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>639</td>\n",
              "      <td>2.682600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>640</td>\n",
              "      <td>2.722100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>641</td>\n",
              "      <td>2.432300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>642</td>\n",
              "      <td>2.734200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>643</td>\n",
              "      <td>2.731100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>644</td>\n",
              "      <td>2.427400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>645</td>\n",
              "      <td>2.657000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>646</td>\n",
              "      <td>2.477500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>647</td>\n",
              "      <td>2.656700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>648</td>\n",
              "      <td>2.661800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>649</td>\n",
              "      <td>2.803700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>650</td>\n",
              "      <td>2.668700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>651</td>\n",
              "      <td>2.668400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>652</td>\n",
              "      <td>2.633200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>653</td>\n",
              "      <td>2.595600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>654</td>\n",
              "      <td>2.527100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>655</td>\n",
              "      <td>2.644100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>656</td>\n",
              "      <td>2.524800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>657</td>\n",
              "      <td>2.723500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>658</td>\n",
              "      <td>2.705700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>659</td>\n",
              "      <td>2.691100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>660</td>\n",
              "      <td>2.623200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>661</td>\n",
              "      <td>2.645200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>662</td>\n",
              "      <td>2.657300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>663</td>\n",
              "      <td>2.728900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>664</td>\n",
              "      <td>2.737800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>665</td>\n",
              "      <td>2.655700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>666</td>\n",
              "      <td>2.754200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>667</td>\n",
              "      <td>2.600500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>668</td>\n",
              "      <td>2.721700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>669</td>\n",
              "      <td>2.550300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>670</td>\n",
              "      <td>2.671400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>671</td>\n",
              "      <td>2.696300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>672</td>\n",
              "      <td>2.574600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>673</td>\n",
              "      <td>2.747600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>674</td>\n",
              "      <td>2.401900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>675</td>\n",
              "      <td>2.703300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>676</td>\n",
              "      <td>2.592900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>677</td>\n",
              "      <td>2.744700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>678</td>\n",
              "      <td>2.617400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>679</td>\n",
              "      <td>2.674700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>680</td>\n",
              "      <td>2.430900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>681</td>\n",
              "      <td>2.670900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>682</td>\n",
              "      <td>2.395000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>683</td>\n",
              "      <td>2.659300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>684</td>\n",
              "      <td>2.724700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>685</td>\n",
              "      <td>2.685600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>686</td>\n",
              "      <td>2.165500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>687</td>\n",
              "      <td>2.745200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>688</td>\n",
              "      <td>2.646600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>689</td>\n",
              "      <td>2.552600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>690</td>\n",
              "      <td>2.695000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>691</td>\n",
              "      <td>2.741500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>692</td>\n",
              "      <td>2.633600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>693</td>\n",
              "      <td>2.686500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>694</td>\n",
              "      <td>2.680900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>695</td>\n",
              "      <td>2.719200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>696</td>\n",
              "      <td>2.571100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>697</td>\n",
              "      <td>2.543200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>698</td>\n",
              "      <td>2.450300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>699</td>\n",
              "      <td>2.619500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>700</td>\n",
              "      <td>2.448600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>701</td>\n",
              "      <td>2.751800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>702</td>\n",
              "      <td>2.586000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>703</td>\n",
              "      <td>2.428500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>704</td>\n",
              "      <td>2.643300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>705</td>\n",
              "      <td>2.629500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>706</td>\n",
              "      <td>2.444200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>707</td>\n",
              "      <td>2.475500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>708</td>\n",
              "      <td>2.735200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>709</td>\n",
              "      <td>2.395200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>710</td>\n",
              "      <td>2.383300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>711</td>\n",
              "      <td>2.755600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>712</td>\n",
              "      <td>2.327600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>713</td>\n",
              "      <td>2.697000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>714</td>\n",
              "      <td>2.830000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>715</td>\n",
              "      <td>2.549300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>716</td>\n",
              "      <td>2.655600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>717</td>\n",
              "      <td>2.633700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>718</td>\n",
              "      <td>2.403000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>719</td>\n",
              "      <td>2.506500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>720</td>\n",
              "      <td>2.666100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>721</td>\n",
              "      <td>2.672100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>722</td>\n",
              "      <td>2.682700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>723</td>\n",
              "      <td>2.725600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>724</td>\n",
              "      <td>2.607700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>725</td>\n",
              "      <td>2.573700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>726</td>\n",
              "      <td>2.675100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>727</td>\n",
              "      <td>2.812800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>728</td>\n",
              "      <td>2.441200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>729</td>\n",
              "      <td>2.647900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>730</td>\n",
              "      <td>2.713300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>731</td>\n",
              "      <td>2.861400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>732</td>\n",
              "      <td>2.673500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>733</td>\n",
              "      <td>2.715500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>734</td>\n",
              "      <td>2.614400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>735</td>\n",
              "      <td>2.698200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>736</td>\n",
              "      <td>2.591100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>737</td>\n",
              "      <td>2.515600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>738</td>\n",
              "      <td>2.739800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>739</td>\n",
              "      <td>2.595000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>740</td>\n",
              "      <td>2.743900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>741</td>\n",
              "      <td>2.777400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>742</td>\n",
              "      <td>2.741600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>743</td>\n",
              "      <td>2.595800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>744</td>\n",
              "      <td>2.690800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>745</td>\n",
              "      <td>2.644600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>746</td>\n",
              "      <td>2.213100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>747</td>\n",
              "      <td>2.640600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>748</td>\n",
              "      <td>2.562200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>749</td>\n",
              "      <td>2.712900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>750</td>\n",
              "      <td>2.642100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>751</td>\n",
              "      <td>2.711700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>752</td>\n",
              "      <td>2.661800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>753</td>\n",
              "      <td>2.399100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>754</td>\n",
              "      <td>2.711400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>755</td>\n",
              "      <td>2.684200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>756</td>\n",
              "      <td>2.664500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>757</td>\n",
              "      <td>2.795600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>758</td>\n",
              "      <td>2.629700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>759</td>\n",
              "      <td>2.718800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>760</td>\n",
              "      <td>2.606500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>761</td>\n",
              "      <td>2.372900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>762</td>\n",
              "      <td>2.550000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>763</td>\n",
              "      <td>2.661600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>764</td>\n",
              "      <td>2.456600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>765</td>\n",
              "      <td>2.753100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>766</td>\n",
              "      <td>2.651500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>767</td>\n",
              "      <td>2.606100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>768</td>\n",
              "      <td>2.321600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>769</td>\n",
              "      <td>2.599200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>770</td>\n",
              "      <td>2.903100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>771</td>\n",
              "      <td>2.768500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>772</td>\n",
              "      <td>2.418100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>773</td>\n",
              "      <td>2.542500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>774</td>\n",
              "      <td>2.611800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>775</td>\n",
              "      <td>2.649900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>776</td>\n",
              "      <td>2.470100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>777</td>\n",
              "      <td>2.699200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>778</td>\n",
              "      <td>2.726100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>779</td>\n",
              "      <td>2.700300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>780</td>\n",
              "      <td>2.315400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>781</td>\n",
              "      <td>2.668100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>782</td>\n",
              "      <td>2.672700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>783</td>\n",
              "      <td>2.698100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>784</td>\n",
              "      <td>2.405000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>785</td>\n",
              "      <td>2.565200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>786</td>\n",
              "      <td>2.715100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>787</td>\n",
              "      <td>2.481500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>788</td>\n",
              "      <td>2.696600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>789</td>\n",
              "      <td>2.783700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>790</td>\n",
              "      <td>2.640100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>791</td>\n",
              "      <td>2.395000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>792</td>\n",
              "      <td>2.725600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>793</td>\n",
              "      <td>2.461400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>794</td>\n",
              "      <td>2.691300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>795</td>\n",
              "      <td>2.617100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>796</td>\n",
              "      <td>2.677200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>797</td>\n",
              "      <td>2.677600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>798</td>\n",
              "      <td>2.692700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>799</td>\n",
              "      <td>2.859600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>800</td>\n",
              "      <td>2.555300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>801</td>\n",
              "      <td>2.657400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>802</td>\n",
              "      <td>2.705400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>803</td>\n",
              "      <td>2.571500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>804</td>\n",
              "      <td>2.568500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>805</td>\n",
              "      <td>2.536800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>806</td>\n",
              "      <td>2.410500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>807</td>\n",
              "      <td>2.337500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>808</td>\n",
              "      <td>2.729300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>809</td>\n",
              "      <td>2.716800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>810</td>\n",
              "      <td>2.657300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>811</td>\n",
              "      <td>2.763200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>812</td>\n",
              "      <td>2.451800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>813</td>\n",
              "      <td>2.700300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>814</td>\n",
              "      <td>2.711400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>815</td>\n",
              "      <td>2.658300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>816</td>\n",
              "      <td>2.571200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>817</td>\n",
              "      <td>2.814300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>818</td>\n",
              "      <td>2.756700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>819</td>\n",
              "      <td>2.346300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>820</td>\n",
              "      <td>2.420900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>821</td>\n",
              "      <td>2.380200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>822</td>\n",
              "      <td>2.775600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>823</td>\n",
              "      <td>2.629300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>824</td>\n",
              "      <td>2.552100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>825</td>\n",
              "      <td>2.958500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>826</td>\n",
              "      <td>2.711900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>827</td>\n",
              "      <td>2.642200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>828</td>\n",
              "      <td>2.592900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>829</td>\n",
              "      <td>2.667600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>830</td>\n",
              "      <td>2.620100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>831</td>\n",
              "      <td>2.639300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>832</td>\n",
              "      <td>2.801900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>833</td>\n",
              "      <td>2.564800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>834</td>\n",
              "      <td>2.372800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>835</td>\n",
              "      <td>2.543700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>836</td>\n",
              "      <td>2.613900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>837</td>\n",
              "      <td>2.678200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>838</td>\n",
              "      <td>2.790700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>839</td>\n",
              "      <td>2.683300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>840</td>\n",
              "      <td>2.383300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>841</td>\n",
              "      <td>2.631300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>842</td>\n",
              "      <td>2.652400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>843</td>\n",
              "      <td>2.781700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>844</td>\n",
              "      <td>2.625700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>845</td>\n",
              "      <td>2.659100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>846</td>\n",
              "      <td>2.577700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>847</td>\n",
              "      <td>2.616500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>848</td>\n",
              "      <td>2.545800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>849</td>\n",
              "      <td>2.758100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>850</td>\n",
              "      <td>2.732300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>851</td>\n",
              "      <td>2.410500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>852</td>\n",
              "      <td>2.557100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>853</td>\n",
              "      <td>2.597400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>854</td>\n",
              "      <td>2.764100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>855</td>\n",
              "      <td>2.688700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>856</td>\n",
              "      <td>2.327500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>857</td>\n",
              "      <td>2.542500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>858</td>\n",
              "      <td>2.773600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>859</td>\n",
              "      <td>2.602400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>860</td>\n",
              "      <td>2.456400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>861</td>\n",
              "      <td>2.751600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>862</td>\n",
              "      <td>2.571200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>863</td>\n",
              "      <td>2.684500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>864</td>\n",
              "      <td>2.427300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>865</td>\n",
              "      <td>2.746100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>866</td>\n",
              "      <td>2.371100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>867</td>\n",
              "      <td>2.618100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>868</td>\n",
              "      <td>2.509800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>869</td>\n",
              "      <td>2.635700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>870</td>\n",
              "      <td>2.681300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>871</td>\n",
              "      <td>2.386200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>872</td>\n",
              "      <td>2.383900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>873</td>\n",
              "      <td>2.404800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>874</td>\n",
              "      <td>2.551200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>875</td>\n",
              "      <td>2.771700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>876</td>\n",
              "      <td>2.213400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>877</td>\n",
              "      <td>2.631400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>878</td>\n",
              "      <td>2.333800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>879</td>\n",
              "      <td>2.702600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>880</td>\n",
              "      <td>2.470100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>881</td>\n",
              "      <td>2.597600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>882</td>\n",
              "      <td>2.640300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>883</td>\n",
              "      <td>2.641800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>884</td>\n",
              "      <td>2.304400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>885</td>\n",
              "      <td>2.583900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>886</td>\n",
              "      <td>2.586000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>887</td>\n",
              "      <td>2.670200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>888</td>\n",
              "      <td>2.728800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>889</td>\n",
              "      <td>2.636900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>890</td>\n",
              "      <td>2.510200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>891</td>\n",
              "      <td>2.663400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>892</td>\n",
              "      <td>2.351200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>893</td>\n",
              "      <td>2.649300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>894</td>\n",
              "      <td>2.700600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>895</td>\n",
              "      <td>2.648000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>896</td>\n",
              "      <td>2.585900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>897</td>\n",
              "      <td>2.451600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>898</td>\n",
              "      <td>2.600800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>899</td>\n",
              "      <td>2.475500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>900</td>\n",
              "      <td>2.624500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>901</td>\n",
              "      <td>2.606200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>902</td>\n",
              "      <td>2.529500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>903</td>\n",
              "      <td>2.659700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>904</td>\n",
              "      <td>2.684900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>905</td>\n",
              "      <td>2.696600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>906</td>\n",
              "      <td>2.692900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>907</td>\n",
              "      <td>2.643400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>908</td>\n",
              "      <td>2.123000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>909</td>\n",
              "      <td>2.629200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>910</td>\n",
              "      <td>2.783000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>911</td>\n",
              "      <td>2.639900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>912</td>\n",
              "      <td>2.381600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>913</td>\n",
              "      <td>2.597900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>914</td>\n",
              "      <td>2.671800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>915</td>\n",
              "      <td>2.324200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>916</td>\n",
              "      <td>2.747500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>917</td>\n",
              "      <td>2.601400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>918</td>\n",
              "      <td>2.675400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>919</td>\n",
              "      <td>2.747800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>920</td>\n",
              "      <td>2.666000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>921</td>\n",
              "      <td>2.615100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>922</td>\n",
              "      <td>2.603900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>923</td>\n",
              "      <td>2.691900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>924</td>\n",
              "      <td>2.564100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>925</td>\n",
              "      <td>2.375700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>926</td>\n",
              "      <td>2.649500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>927</td>\n",
              "      <td>2.471300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>928</td>\n",
              "      <td>2.712100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>929</td>\n",
              "      <td>2.503500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>930</td>\n",
              "      <td>2.692800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>931</td>\n",
              "      <td>2.565100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>932</td>\n",
              "      <td>2.615400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>933</td>\n",
              "      <td>2.871500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>934</td>\n",
              "      <td>2.752500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>935</td>\n",
              "      <td>2.428400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>936</td>\n",
              "      <td>2.737100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>937</td>\n",
              "      <td>2.664800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>938</td>\n",
              "      <td>2.595200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>939</td>\n",
              "      <td>2.587500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>940</td>\n",
              "      <td>2.621400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>941</td>\n",
              "      <td>2.450000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>942</td>\n",
              "      <td>2.338700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>943</td>\n",
              "      <td>2.692600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>944</td>\n",
              "      <td>2.690000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>945</td>\n",
              "      <td>2.402300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>946</td>\n",
              "      <td>2.694200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>947</td>\n",
              "      <td>2.632800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>948</td>\n",
              "      <td>2.599200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>949</td>\n",
              "      <td>2.669900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>950</td>\n",
              "      <td>2.580400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>951</td>\n",
              "      <td>2.797200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>952</td>\n",
              "      <td>2.819100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>953</td>\n",
              "      <td>2.717500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>954</td>\n",
              "      <td>2.534300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>955</td>\n",
              "      <td>2.648300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>956</td>\n",
              "      <td>2.675300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>957</td>\n",
              "      <td>2.700000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>958</td>\n",
              "      <td>2.579500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>959</td>\n",
              "      <td>2.651200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>960</td>\n",
              "      <td>2.533700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>961</td>\n",
              "      <td>2.649400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>962</td>\n",
              "      <td>2.365900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>963</td>\n",
              "      <td>2.832400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>964</td>\n",
              "      <td>2.689100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>965</td>\n",
              "      <td>2.546700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>966</td>\n",
              "      <td>2.726000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>967</td>\n",
              "      <td>2.591600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>968</td>\n",
              "      <td>2.629200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>969</td>\n",
              "      <td>2.673900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>970</td>\n",
              "      <td>2.839700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>971</td>\n",
              "      <td>2.877500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>972</td>\n",
              "      <td>2.496900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>973</td>\n",
              "      <td>2.726400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>974</td>\n",
              "      <td>2.734700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>975</td>\n",
              "      <td>2.490200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>976</td>\n",
              "      <td>2.739700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>977</td>\n",
              "      <td>2.580600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>978</td>\n",
              "      <td>2.377200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>979</td>\n",
              "      <td>2.635300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>980</td>\n",
              "      <td>2.669700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>981</td>\n",
              "      <td>2.873000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>982</td>\n",
              "      <td>2.668600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>983</td>\n",
              "      <td>2.557800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>984</td>\n",
              "      <td>2.615300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>985</td>\n",
              "      <td>2.569200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>986</td>\n",
              "      <td>2.673200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>987</td>\n",
              "      <td>2.614200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>988</td>\n",
              "      <td>2.704700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>989</td>\n",
              "      <td>2.499500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>990</td>\n",
              "      <td>2.482500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>991</td>\n",
              "      <td>2.657700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>992</td>\n",
              "      <td>2.640200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>993</td>\n",
              "      <td>2.453500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>994</td>\n",
              "      <td>2.798300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>995</td>\n",
              "      <td>2.476800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>996</td>\n",
              "      <td>2.427200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>997</td>\n",
              "      <td>2.677800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>998</td>\n",
              "      <td>2.594100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>999</td>\n",
              "      <td>2.566400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1000</td>\n",
              "      <td>2.394800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1001</td>\n",
              "      <td>2.630100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1002</td>\n",
              "      <td>2.685100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1003</td>\n",
              "      <td>2.384500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1004</td>\n",
              "      <td>2.692400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1005</td>\n",
              "      <td>2.533100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1006</td>\n",
              "      <td>2.661100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1007</td>\n",
              "      <td>2.407900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1008</td>\n",
              "      <td>2.590800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1009</td>\n",
              "      <td>2.503400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1010</td>\n",
              "      <td>2.626600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1011</td>\n",
              "      <td>2.661000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1012</td>\n",
              "      <td>2.563100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1013</td>\n",
              "      <td>2.586700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1014</td>\n",
              "      <td>2.700100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1015</td>\n",
              "      <td>2.662900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1016</td>\n",
              "      <td>2.461300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1017</td>\n",
              "      <td>2.637700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1018</td>\n",
              "      <td>2.614300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1019</td>\n",
              "      <td>2.441000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1020</td>\n",
              "      <td>2.681900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1021</td>\n",
              "      <td>2.617800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1022</td>\n",
              "      <td>2.352000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1023</td>\n",
              "      <td>2.608900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1024</td>\n",
              "      <td>2.608000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1025</td>\n",
              "      <td>2.643400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1026</td>\n",
              "      <td>2.606200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1027</td>\n",
              "      <td>2.758300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1028</td>\n",
              "      <td>2.653300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1029</td>\n",
              "      <td>2.602000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1030</td>\n",
              "      <td>2.508200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1031</td>\n",
              "      <td>2.633600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1032</td>\n",
              "      <td>2.623200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1033</td>\n",
              "      <td>2.610800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1034</td>\n",
              "      <td>2.586500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1035</td>\n",
              "      <td>2.605300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1036</td>\n",
              "      <td>2.591400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1037</td>\n",
              "      <td>2.723400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1038</td>\n",
              "      <td>2.415300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1039</td>\n",
              "      <td>2.426500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1040</td>\n",
              "      <td>2.468300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1041</td>\n",
              "      <td>2.645900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1042</td>\n",
              "      <td>2.595300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1043</td>\n",
              "      <td>2.778300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1044</td>\n",
              "      <td>2.406700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1045</td>\n",
              "      <td>2.540400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1046</td>\n",
              "      <td>2.549800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1047</td>\n",
              "      <td>2.590700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1048</td>\n",
              "      <td>2.408700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1049</td>\n",
              "      <td>2.561400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1050</td>\n",
              "      <td>2.661100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1051</td>\n",
              "      <td>2.564800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1052</td>\n",
              "      <td>2.450300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1053</td>\n",
              "      <td>2.622800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1054</td>\n",
              "      <td>2.587800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1055</td>\n",
              "      <td>2.364700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1056</td>\n",
              "      <td>2.642000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1057</td>\n",
              "      <td>2.602800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1058</td>\n",
              "      <td>2.324200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1059</td>\n",
              "      <td>2.647700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1060</td>\n",
              "      <td>2.565800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1061</td>\n",
              "      <td>2.851300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1062</td>\n",
              "      <td>2.749400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1063</td>\n",
              "      <td>2.312000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1064</td>\n",
              "      <td>2.535000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1065</td>\n",
              "      <td>2.024000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1066</td>\n",
              "      <td>2.658500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1067</td>\n",
              "      <td>2.765500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1068</td>\n",
              "      <td>2.601000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1069</td>\n",
              "      <td>2.437200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1070</td>\n",
              "      <td>2.810500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1071</td>\n",
              "      <td>2.439000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1072</td>\n",
              "      <td>2.132000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1073</td>\n",
              "      <td>2.711900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1074</td>\n",
              "      <td>2.628800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1075</td>\n",
              "      <td>2.529100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1076</td>\n",
              "      <td>2.602100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1077</td>\n",
              "      <td>2.669600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1078</td>\n",
              "      <td>2.728200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1079</td>\n",
              "      <td>2.639600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1080</td>\n",
              "      <td>2.273700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1081</td>\n",
              "      <td>2.581100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1082</td>\n",
              "      <td>2.269900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1083</td>\n",
              "      <td>2.670900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1084</td>\n",
              "      <td>2.344700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1085</td>\n",
              "      <td>2.641600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1086</td>\n",
              "      <td>2.476400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1087</td>\n",
              "      <td>2.629200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1088</td>\n",
              "      <td>2.333400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1089</td>\n",
              "      <td>2.572900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1090</td>\n",
              "      <td>2.688600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1091</td>\n",
              "      <td>2.665200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1092</td>\n",
              "      <td>2.688700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1093</td>\n",
              "      <td>2.599100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1094</td>\n",
              "      <td>2.650000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1095</td>\n",
              "      <td>2.898700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1096</td>\n",
              "      <td>2.637200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1097</td>\n",
              "      <td>2.676700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1098</td>\n",
              "      <td>2.562700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1099</td>\n",
              "      <td>2.682000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1100</td>\n",
              "      <td>2.299100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1101</td>\n",
              "      <td>2.313400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1102</td>\n",
              "      <td>2.746100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1103</td>\n",
              "      <td>2.661200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1104</td>\n",
              "      <td>2.726600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1105</td>\n",
              "      <td>2.646300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1106</td>\n",
              "      <td>2.487400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1107</td>\n",
              "      <td>2.776900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1108</td>\n",
              "      <td>2.409800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1109</td>\n",
              "      <td>2.352900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1110</td>\n",
              "      <td>2.660000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1111</td>\n",
              "      <td>2.594000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1112</td>\n",
              "      <td>2.604700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1113</td>\n",
              "      <td>2.562600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1114</td>\n",
              "      <td>2.390300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1115</td>\n",
              "      <td>2.639800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1116</td>\n",
              "      <td>2.840700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1117</td>\n",
              "      <td>2.527000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1118</td>\n",
              "      <td>2.518500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1119</td>\n",
              "      <td>2.533100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1120</td>\n",
              "      <td>2.619300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1121</td>\n",
              "      <td>2.595500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1122</td>\n",
              "      <td>2.368200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1123</td>\n",
              "      <td>2.562300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1124</td>\n",
              "      <td>2.602800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1125</td>\n",
              "      <td>2.611400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1126</td>\n",
              "      <td>2.451400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1127</td>\n",
              "      <td>2.365400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1128</td>\n",
              "      <td>2.321900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1129</td>\n",
              "      <td>2.489000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1130</td>\n",
              "      <td>2.568300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1131</td>\n",
              "      <td>2.579900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1132</td>\n",
              "      <td>2.716600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1133</td>\n",
              "      <td>2.630700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1134</td>\n",
              "      <td>2.500700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1135</td>\n",
              "      <td>2.636100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1136</td>\n",
              "      <td>2.751700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1137</td>\n",
              "      <td>2.525800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1138</td>\n",
              "      <td>2.659500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1139</td>\n",
              "      <td>2.646200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1140</td>\n",
              "      <td>2.364900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1141</td>\n",
              "      <td>2.586700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1142</td>\n",
              "      <td>2.593300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1143</td>\n",
              "      <td>2.698100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1144</td>\n",
              "      <td>2.514100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1145</td>\n",
              "      <td>2.757300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1146</td>\n",
              "      <td>2.454300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1147</td>\n",
              "      <td>2.567100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1148</td>\n",
              "      <td>2.598800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1149</td>\n",
              "      <td>2.609900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1150</td>\n",
              "      <td>2.468100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1151</td>\n",
              "      <td>2.626900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1152</td>\n",
              "      <td>2.584400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1153</td>\n",
              "      <td>2.595300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1154</td>\n",
              "      <td>2.646200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1155</td>\n",
              "      <td>2.702400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1156</td>\n",
              "      <td>2.607700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1157</td>\n",
              "      <td>2.661800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1158</td>\n",
              "      <td>2.571700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1159</td>\n",
              "      <td>2.590800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1160</td>\n",
              "      <td>2.760500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1161</td>\n",
              "      <td>2.618600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1162</td>\n",
              "      <td>2.342700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1163</td>\n",
              "      <td>2.569600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1164</td>\n",
              "      <td>2.341300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1165</td>\n",
              "      <td>2.588900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1166</td>\n",
              "      <td>2.673900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1167</td>\n",
              "      <td>2.505100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1168</td>\n",
              "      <td>2.555700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1169</td>\n",
              "      <td>2.279100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1170</td>\n",
              "      <td>2.642900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1171</td>\n",
              "      <td>2.707200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1172</td>\n",
              "      <td>2.618100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1173</td>\n",
              "      <td>2.356100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1174</td>\n",
              "      <td>2.525000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1175</td>\n",
              "      <td>2.579800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1176</td>\n",
              "      <td>2.468800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1177</td>\n",
              "      <td>2.580300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1178</td>\n",
              "      <td>2.635500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1179</td>\n",
              "      <td>2.328100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1180</td>\n",
              "      <td>2.601100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1181</td>\n",
              "      <td>2.710100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1182</td>\n",
              "      <td>2.597500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1183</td>\n",
              "      <td>2.649700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1184</td>\n",
              "      <td>2.444500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1185</td>\n",
              "      <td>2.637600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1186</td>\n",
              "      <td>2.772000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1187</td>\n",
              "      <td>2.538900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1188</td>\n",
              "      <td>2.550600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1189</td>\n",
              "      <td>2.587200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1190</td>\n",
              "      <td>2.583100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1191</td>\n",
              "      <td>2.438700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1192</td>\n",
              "      <td>2.577700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1193</td>\n",
              "      <td>2.390800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1194</td>\n",
              "      <td>2.457500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1195</td>\n",
              "      <td>2.627600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1196</td>\n",
              "      <td>2.551900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1197</td>\n",
              "      <td>2.355200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1198</td>\n",
              "      <td>2.594800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1199</td>\n",
              "      <td>2.586900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1200</td>\n",
              "      <td>2.661700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1201</td>\n",
              "      <td>2.625100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1202</td>\n",
              "      <td>2.516900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1203</td>\n",
              "      <td>2.558300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1204</td>\n",
              "      <td>2.714700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1205</td>\n",
              "      <td>2.673900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1206</td>\n",
              "      <td>2.718000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1207</td>\n",
              "      <td>2.635600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1208</td>\n",
              "      <td>2.627100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1209</td>\n",
              "      <td>2.549800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1210</td>\n",
              "      <td>2.706500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1211</td>\n",
              "      <td>2.397500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1212</td>\n",
              "      <td>2.477400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1213</td>\n",
              "      <td>2.529000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1214</td>\n",
              "      <td>2.647400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1215</td>\n",
              "      <td>2.604400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1216</td>\n",
              "      <td>2.774500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1217</td>\n",
              "      <td>2.322500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1218</td>\n",
              "      <td>2.537000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1219</td>\n",
              "      <td>2.615500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1220</td>\n",
              "      <td>2.628800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1221</td>\n",
              "      <td>2.549400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1222</td>\n",
              "      <td>2.590500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1223</td>\n",
              "      <td>2.688500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1224</td>\n",
              "      <td>2.537100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1225</td>\n",
              "      <td>2.627900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1226</td>\n",
              "      <td>2.737800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1227</td>\n",
              "      <td>2.660000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1228</td>\n",
              "      <td>2.650400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1229</td>\n",
              "      <td>2.379500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1230</td>\n",
              "      <td>2.428500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1231</td>\n",
              "      <td>2.325000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1232</td>\n",
              "      <td>2.666300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1233</td>\n",
              "      <td>2.630000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1234</td>\n",
              "      <td>2.657700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1235</td>\n",
              "      <td>2.633900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1236</td>\n",
              "      <td>2.473900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1237</td>\n",
              "      <td>2.674400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1238</td>\n",
              "      <td>2.537300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1239</td>\n",
              "      <td>2.066600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1240</td>\n",
              "      <td>2.827100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1241</td>\n",
              "      <td>2.540200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1242</td>\n",
              "      <td>2.483100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1243</td>\n",
              "      <td>2.607200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1244</td>\n",
              "      <td>2.549300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1245</td>\n",
              "      <td>2.802200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1246</td>\n",
              "      <td>2.609500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1247</td>\n",
              "      <td>2.636400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1248</td>\n",
              "      <td>2.527900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1249</td>\n",
              "      <td>2.683100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1250</td>\n",
              "      <td>2.527500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1251</td>\n",
              "      <td>2.652400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1252</td>\n",
              "      <td>2.694100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1253</td>\n",
              "      <td>2.738100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1254</td>\n",
              "      <td>2.751600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1255</td>\n",
              "      <td>2.681200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1256</td>\n",
              "      <td>2.334800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1257</td>\n",
              "      <td>2.641600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1258</td>\n",
              "      <td>2.088900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1259</td>\n",
              "      <td>2.695500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1260</td>\n",
              "      <td>2.390900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1261</td>\n",
              "      <td>2.883900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1262</td>\n",
              "      <td>2.650900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1263</td>\n",
              "      <td>2.630300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1264</td>\n",
              "      <td>2.509800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1265</td>\n",
              "      <td>2.548200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1266</td>\n",
              "      <td>2.256800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1267</td>\n",
              "      <td>2.525800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1268</td>\n",
              "      <td>2.522900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1269</td>\n",
              "      <td>2.574400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1270</td>\n",
              "      <td>2.418600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1271</td>\n",
              "      <td>2.460000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1272</td>\n",
              "      <td>2.695900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1273</td>\n",
              "      <td>2.488700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1274</td>\n",
              "      <td>2.417500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1275</td>\n",
              "      <td>2.363300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1276</td>\n",
              "      <td>2.650600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1277</td>\n",
              "      <td>2.590800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1278</td>\n",
              "      <td>2.503800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1279</td>\n",
              "      <td>2.326500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1280</td>\n",
              "      <td>2.401600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1281</td>\n",
              "      <td>2.645100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1282</td>\n",
              "      <td>2.749000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1283</td>\n",
              "      <td>2.353500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1284</td>\n",
              "      <td>2.371000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1285</td>\n",
              "      <td>2.594300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1286</td>\n",
              "      <td>2.648600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1287</td>\n",
              "      <td>2.674300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1288</td>\n",
              "      <td>2.330400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1289</td>\n",
              "      <td>2.561400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1290</td>\n",
              "      <td>2.579400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1291</td>\n",
              "      <td>2.786000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1292</td>\n",
              "      <td>2.637200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1293</td>\n",
              "      <td>2.756500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1294</td>\n",
              "      <td>2.633800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1295</td>\n",
              "      <td>2.681700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1296</td>\n",
              "      <td>2.459700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1297</td>\n",
              "      <td>2.515400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1298</td>\n",
              "      <td>2.660400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1299</td>\n",
              "      <td>2.557500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1300</td>\n",
              "      <td>2.534100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1301</td>\n",
              "      <td>2.486500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1302</td>\n",
              "      <td>2.719900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1303</td>\n",
              "      <td>2.450300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1304</td>\n",
              "      <td>2.672000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1305</td>\n",
              "      <td>2.794800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1306</td>\n",
              "      <td>2.468300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1307</td>\n",
              "      <td>2.344200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1308</td>\n",
              "      <td>2.710900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1309</td>\n",
              "      <td>2.266900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1310</td>\n",
              "      <td>2.484900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1311</td>\n",
              "      <td>2.541400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1312</td>\n",
              "      <td>2.514700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1313</td>\n",
              "      <td>2.526800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1314</td>\n",
              "      <td>2.782300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1315</td>\n",
              "      <td>2.624600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1316</td>\n",
              "      <td>2.586000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1317</td>\n",
              "      <td>2.332700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1318</td>\n",
              "      <td>2.239200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1319</td>\n",
              "      <td>2.755200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1320</td>\n",
              "      <td>2.566800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1321</td>\n",
              "      <td>2.468400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1322</td>\n",
              "      <td>2.629800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1323</td>\n",
              "      <td>2.784400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1324</td>\n",
              "      <td>2.517900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1325</td>\n",
              "      <td>2.456100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1326</td>\n",
              "      <td>2.616200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1327</td>\n",
              "      <td>2.657600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1328</td>\n",
              "      <td>2.548600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1329</td>\n",
              "      <td>2.605600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1330</td>\n",
              "      <td>2.636300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1331</td>\n",
              "      <td>2.619300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1332</td>\n",
              "      <td>2.535400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1333</td>\n",
              "      <td>2.614800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1334</td>\n",
              "      <td>2.677300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1335</td>\n",
              "      <td>2.615900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1336</td>\n",
              "      <td>2.466600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1337</td>\n",
              "      <td>2.354700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1338</td>\n",
              "      <td>2.286400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1339</td>\n",
              "      <td>2.577200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1340</td>\n",
              "      <td>2.468900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1341</td>\n",
              "      <td>2.700100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1342</td>\n",
              "      <td>2.522800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1343</td>\n",
              "      <td>2.578500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1344</td>\n",
              "      <td>2.453200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1345</td>\n",
              "      <td>2.605200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1346</td>\n",
              "      <td>2.544000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1347</td>\n",
              "      <td>2.648400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1348</td>\n",
              "      <td>2.331900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1349</td>\n",
              "      <td>2.412700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1350</td>\n",
              "      <td>2.553400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1351</td>\n",
              "      <td>2.506200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1352</td>\n",
              "      <td>2.544000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1353</td>\n",
              "      <td>2.488000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1354</td>\n",
              "      <td>2.694300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1355</td>\n",
              "      <td>2.586700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1356</td>\n",
              "      <td>2.141900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1357</td>\n",
              "      <td>2.596200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1358</td>\n",
              "      <td>2.728100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1359</td>\n",
              "      <td>2.655000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1360</td>\n",
              "      <td>2.506200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1361</td>\n",
              "      <td>2.309400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1362</td>\n",
              "      <td>2.623000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1363</td>\n",
              "      <td>2.572800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1364</td>\n",
              "      <td>2.471400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1365</td>\n",
              "      <td>2.533200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1366</td>\n",
              "      <td>2.563000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1367</td>\n",
              "      <td>2.611000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1368</td>\n",
              "      <td>2.607900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1369</td>\n",
              "      <td>2.786900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1370</td>\n",
              "      <td>2.148900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1371</td>\n",
              "      <td>2.604000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1372</td>\n",
              "      <td>2.896000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1373</td>\n",
              "      <td>2.687200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1374</td>\n",
              "      <td>2.607600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1375</td>\n",
              "      <td>2.410800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1376</td>\n",
              "      <td>2.783200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1377</td>\n",
              "      <td>2.256500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1378</td>\n",
              "      <td>2.270900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1379</td>\n",
              "      <td>2.717800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1380</td>\n",
              "      <td>2.376100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1381</td>\n",
              "      <td>2.710600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1382</td>\n",
              "      <td>2.536800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1383</td>\n",
              "      <td>2.256900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1384</td>\n",
              "      <td>2.619300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1385</td>\n",
              "      <td>2.715000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1386</td>\n",
              "      <td>2.124200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1387</td>\n",
              "      <td>2.619200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1388</td>\n",
              "      <td>2.607600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1389</td>\n",
              "      <td>2.372200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1390</td>\n",
              "      <td>2.285000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1391</td>\n",
              "      <td>2.063300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1392</td>\n",
              "      <td>2.569400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1393</td>\n",
              "      <td>2.576400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1394</td>\n",
              "      <td>2.495100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1395</td>\n",
              "      <td>2.125700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1396</td>\n",
              "      <td>2.694400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1397</td>\n",
              "      <td>2.519200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1398</td>\n",
              "      <td>2.540700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1399</td>\n",
              "      <td>2.519900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1400</td>\n",
              "      <td>2.585800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1401</td>\n",
              "      <td>2.617300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1402</td>\n",
              "      <td>2.614600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1403</td>\n",
              "      <td>2.574800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1404</td>\n",
              "      <td>2.574900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1405</td>\n",
              "      <td>2.432900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1406</td>\n",
              "      <td>2.595300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1407</td>\n",
              "      <td>2.546000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1408</td>\n",
              "      <td>2.659600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1409</td>\n",
              "      <td>2.604300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1410</td>\n",
              "      <td>2.457500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1411</td>\n",
              "      <td>2.683200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1412</td>\n",
              "      <td>2.641700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1413</td>\n",
              "      <td>2.574900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1414</td>\n",
              "      <td>2.676200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1415</td>\n",
              "      <td>2.328300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1416</td>\n",
              "      <td>2.579000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1417</td>\n",
              "      <td>2.563900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1418</td>\n",
              "      <td>2.687000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1419</td>\n",
              "      <td>2.380900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1420</td>\n",
              "      <td>2.505700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1421</td>\n",
              "      <td>2.559300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1422</td>\n",
              "      <td>2.600700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1423</td>\n",
              "      <td>2.446700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1424</td>\n",
              "      <td>2.511200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1425</td>\n",
              "      <td>2.529800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1426</td>\n",
              "      <td>2.619900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1427</td>\n",
              "      <td>2.533700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1428</td>\n",
              "      <td>2.716400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1429</td>\n",
              "      <td>2.680400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1430</td>\n",
              "      <td>2.340800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1431</td>\n",
              "      <td>2.268600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1432</td>\n",
              "      <td>2.676700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1433</td>\n",
              "      <td>2.653800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1434</td>\n",
              "      <td>2.613500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1435</td>\n",
              "      <td>2.605500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1436</td>\n",
              "      <td>2.592000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1437</td>\n",
              "      <td>2.624000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1438</td>\n",
              "      <td>2.388100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1439</td>\n",
              "      <td>2.753400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1440</td>\n",
              "      <td>2.384900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1441</td>\n",
              "      <td>2.523800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1442</td>\n",
              "      <td>2.552200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1443</td>\n",
              "      <td>2.583500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1444</td>\n",
              "      <td>2.614500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1445</td>\n",
              "      <td>2.521000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1446</td>\n",
              "      <td>2.549900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1447</td>\n",
              "      <td>2.601100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1448</td>\n",
              "      <td>2.659300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1449</td>\n",
              "      <td>2.484300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1450</td>\n",
              "      <td>2.456200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1451</td>\n",
              "      <td>2.456800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1452</td>\n",
              "      <td>2.370700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1453</td>\n",
              "      <td>2.292200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1454</td>\n",
              "      <td>2.333800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1455</td>\n",
              "      <td>2.194000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1456</td>\n",
              "      <td>2.572300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1457</td>\n",
              "      <td>2.714700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1458</td>\n",
              "      <td>2.549000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1459</td>\n",
              "      <td>2.365700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1460</td>\n",
              "      <td>2.433000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1461</td>\n",
              "      <td>2.485500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1462</td>\n",
              "      <td>2.691000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1463</td>\n",
              "      <td>2.677700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1464</td>\n",
              "      <td>2.347700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1465</td>\n",
              "      <td>2.796900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1466</td>\n",
              "      <td>2.795900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1467</td>\n",
              "      <td>2.253200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1468</td>\n",
              "      <td>2.290300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1469</td>\n",
              "      <td>2.616500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1470</td>\n",
              "      <td>2.548400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1471</td>\n",
              "      <td>2.525300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1472</td>\n",
              "      <td>2.659200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1473</td>\n",
              "      <td>2.567600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1474</td>\n",
              "      <td>2.571700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1475</td>\n",
              "      <td>2.633700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1476</td>\n",
              "      <td>2.637800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1477</td>\n",
              "      <td>2.440300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1478</td>\n",
              "      <td>2.537500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1479</td>\n",
              "      <td>2.326300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1480</td>\n",
              "      <td>2.588800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1481</td>\n",
              "      <td>2.680500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1482</td>\n",
              "      <td>2.609400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1483</td>\n",
              "      <td>2.500900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1484</td>\n",
              "      <td>2.557000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1485</td>\n",
              "      <td>2.561400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1486</td>\n",
              "      <td>2.769400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1487</td>\n",
              "      <td>2.342600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1488</td>\n",
              "      <td>2.536500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1489</td>\n",
              "      <td>2.719500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1490</td>\n",
              "      <td>2.541700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1491</td>\n",
              "      <td>2.618000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1492</td>\n",
              "      <td>2.412400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1493</td>\n",
              "      <td>2.373100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1494</td>\n",
              "      <td>2.731400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1495</td>\n",
              "      <td>2.288900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1496</td>\n",
              "      <td>2.390000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1497</td>\n",
              "      <td>2.601200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1498</td>\n",
              "      <td>2.708200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1499</td>\n",
              "      <td>2.346300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1500</td>\n",
              "      <td>2.302300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1501</td>\n",
              "      <td>2.493300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1502</td>\n",
              "      <td>2.619200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1503</td>\n",
              "      <td>2.511700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1504</td>\n",
              "      <td>2.585600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1505</td>\n",
              "      <td>2.661000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1506</td>\n",
              "      <td>2.662600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1507</td>\n",
              "      <td>2.627500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1508</td>\n",
              "      <td>2.529600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1509</td>\n",
              "      <td>2.676000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1510</td>\n",
              "      <td>2.227400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1511</td>\n",
              "      <td>2.671300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1512</td>\n",
              "      <td>2.641300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1513</td>\n",
              "      <td>2.681300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1514</td>\n",
              "      <td>2.324500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1515</td>\n",
              "      <td>2.540700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1516</td>\n",
              "      <td>2.544000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1517</td>\n",
              "      <td>2.553900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1518</td>\n",
              "      <td>2.776000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1519</td>\n",
              "      <td>2.598300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1520</td>\n",
              "      <td>2.678200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1521</td>\n",
              "      <td>2.623100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1522</td>\n",
              "      <td>2.734400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1523</td>\n",
              "      <td>2.574400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1524</td>\n",
              "      <td>2.640700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1525</td>\n",
              "      <td>2.370000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1526</td>\n",
              "      <td>2.556900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1527</td>\n",
              "      <td>2.625800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1528</td>\n",
              "      <td>2.559600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1529</td>\n",
              "      <td>2.511800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1530</td>\n",
              "      <td>2.572500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1531</td>\n",
              "      <td>2.349900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1532</td>\n",
              "      <td>2.509800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1533</td>\n",
              "      <td>2.555800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1534</td>\n",
              "      <td>2.620500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1535</td>\n",
              "      <td>2.496400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1536</td>\n",
              "      <td>2.550300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1537</td>\n",
              "      <td>2.475000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1538</td>\n",
              "      <td>1.950700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1539</td>\n",
              "      <td>2.572500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1540</td>\n",
              "      <td>2.320100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1541</td>\n",
              "      <td>2.587400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1542</td>\n",
              "      <td>2.542900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1543</td>\n",
              "      <td>2.614000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1544</td>\n",
              "      <td>2.481200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1545</td>\n",
              "      <td>2.729200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1546</td>\n",
              "      <td>2.568900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1547</td>\n",
              "      <td>2.636200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1548</td>\n",
              "      <td>2.610200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1549</td>\n",
              "      <td>2.258200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1550</td>\n",
              "      <td>2.279400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1551</td>\n",
              "      <td>2.022200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1552</td>\n",
              "      <td>2.426300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1553</td>\n",
              "      <td>2.464900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1554</td>\n",
              "      <td>2.835100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1555</td>\n",
              "      <td>2.649600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1556</td>\n",
              "      <td>2.619500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1557</td>\n",
              "      <td>2.739800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1558</td>\n",
              "      <td>2.410500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1559</td>\n",
              "      <td>2.566300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1560</td>\n",
              "      <td>2.542800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1561</td>\n",
              "      <td>2.625100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1562</td>\n",
              "      <td>2.558600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1563</td>\n",
              "      <td>2.512900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1564</td>\n",
              "      <td>2.382400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1565</td>\n",
              "      <td>2.458400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1566</td>\n",
              "      <td>2.708200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1567</td>\n",
              "      <td>2.543300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1568</td>\n",
              "      <td>2.583100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1569</td>\n",
              "      <td>2.783100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1570</td>\n",
              "      <td>2.561200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1571</td>\n",
              "      <td>2.687500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1572</td>\n",
              "      <td>2.688300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1573</td>\n",
              "      <td>2.480000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1574</td>\n",
              "      <td>2.618800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1575</td>\n",
              "      <td>2.498500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1576</td>\n",
              "      <td>2.258700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1577</td>\n",
              "      <td>2.291600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1578</td>\n",
              "      <td>2.735300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1579</td>\n",
              "      <td>2.594800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1580</td>\n",
              "      <td>2.589800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1581</td>\n",
              "      <td>2.405600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1582</td>\n",
              "      <td>2.613100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1583</td>\n",
              "      <td>2.558800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1584</td>\n",
              "      <td>2.616500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1585</td>\n",
              "      <td>2.583700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1586</td>\n",
              "      <td>2.322500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1587</td>\n",
              "      <td>2.583000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1588</td>\n",
              "      <td>2.511600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1589</td>\n",
              "      <td>2.588500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1590</td>\n",
              "      <td>2.347100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1591</td>\n",
              "      <td>2.566000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1592</td>\n",
              "      <td>2.674400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1593</td>\n",
              "      <td>2.661400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1594</td>\n",
              "      <td>2.505800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1595</td>\n",
              "      <td>2.406400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1596</td>\n",
              "      <td>2.331700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1597</td>\n",
              "      <td>2.641000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1598</td>\n",
              "      <td>2.619900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1599</td>\n",
              "      <td>2.090600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1600</td>\n",
              "      <td>2.600900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1601</td>\n",
              "      <td>2.620700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1602</td>\n",
              "      <td>2.664800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1603</td>\n",
              "      <td>2.567100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1604</td>\n",
              "      <td>2.344400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1605</td>\n",
              "      <td>2.365500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1606</td>\n",
              "      <td>2.624200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1607</td>\n",
              "      <td>2.666100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1608</td>\n",
              "      <td>2.343500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1609</td>\n",
              "      <td>2.383200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1610</td>\n",
              "      <td>2.535000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1611</td>\n",
              "      <td>2.629500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1612</td>\n",
              "      <td>2.457300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1613</td>\n",
              "      <td>2.466500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1614</td>\n",
              "      <td>2.487800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1615</td>\n",
              "      <td>2.640800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1616</td>\n",
              "      <td>2.715300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1617</td>\n",
              "      <td>2.255000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1618</td>\n",
              "      <td>2.648500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1619</td>\n",
              "      <td>2.646400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1620</td>\n",
              "      <td>2.631500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1621</td>\n",
              "      <td>2.644900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1622</td>\n",
              "      <td>2.509700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1623</td>\n",
              "      <td>2.645000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1624</td>\n",
              "      <td>2.367800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1625</td>\n",
              "      <td>2.737800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1626</td>\n",
              "      <td>2.568100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1627</td>\n",
              "      <td>2.431300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1628</td>\n",
              "      <td>2.386200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1629</td>\n",
              "      <td>2.508400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1630</td>\n",
              "      <td>2.660700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1631</td>\n",
              "      <td>2.475400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1632</td>\n",
              "      <td>2.604800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1633</td>\n",
              "      <td>2.558500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1634</td>\n",
              "      <td>2.505100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1635</td>\n",
              "      <td>2.920500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1636</td>\n",
              "      <td>2.489200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1637</td>\n",
              "      <td>2.547400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1638</td>\n",
              "      <td>2.446800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1639</td>\n",
              "      <td>2.570300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1640</td>\n",
              "      <td>2.683800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1641</td>\n",
              "      <td>2.719700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1642</td>\n",
              "      <td>2.635400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1643</td>\n",
              "      <td>2.346700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1644</td>\n",
              "      <td>2.598800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1645</td>\n",
              "      <td>2.546300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1646</td>\n",
              "      <td>2.818000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1647</td>\n",
              "      <td>2.521900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1648</td>\n",
              "      <td>2.509700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1649</td>\n",
              "      <td>2.737200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1650</td>\n",
              "      <td>2.217800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1651</td>\n",
              "      <td>2.502200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1652</td>\n",
              "      <td>2.657000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1653</td>\n",
              "      <td>2.653800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1654</td>\n",
              "      <td>2.847300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1655</td>\n",
              "      <td>2.273000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1656</td>\n",
              "      <td>2.742300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1657</td>\n",
              "      <td>2.695200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1658</td>\n",
              "      <td>2.518300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1659</td>\n",
              "      <td>2.562500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1660</td>\n",
              "      <td>2.576600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1661</td>\n",
              "      <td>2.668300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1662</td>\n",
              "      <td>2.812800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1663</td>\n",
              "      <td>2.515600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1664</td>\n",
              "      <td>2.292600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1665</td>\n",
              "      <td>2.372700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1666</td>\n",
              "      <td>2.273400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1667</td>\n",
              "      <td>2.272100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1668</td>\n",
              "      <td>2.506300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1669</td>\n",
              "      <td>2.792400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1670</td>\n",
              "      <td>2.766600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1671</td>\n",
              "      <td>2.599800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1672</td>\n",
              "      <td>2.498900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1673</td>\n",
              "      <td>2.321300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1674</td>\n",
              "      <td>2.332700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1675</td>\n",
              "      <td>2.607600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1676</td>\n",
              "      <td>2.620600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1677</td>\n",
              "      <td>2.458400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1678</td>\n",
              "      <td>2.184100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1679</td>\n",
              "      <td>2.602500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1680</td>\n",
              "      <td>2.578300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1681</td>\n",
              "      <td>2.484200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1682</td>\n",
              "      <td>2.556400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1683</td>\n",
              "      <td>2.750800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1684</td>\n",
              "      <td>2.211500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1685</td>\n",
              "      <td>2.397700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1686</td>\n",
              "      <td>2.548600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1687</td>\n",
              "      <td>2.525100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1688</td>\n",
              "      <td>2.509300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1689</td>\n",
              "      <td>2.553400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1690</td>\n",
              "      <td>2.387400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1691</td>\n",
              "      <td>2.488300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1692</td>\n",
              "      <td>2.622800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1693</td>\n",
              "      <td>2.362100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1694</td>\n",
              "      <td>2.536000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1695</td>\n",
              "      <td>2.564600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1696</td>\n",
              "      <td>2.322800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1697</td>\n",
              "      <td>2.268000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1698</td>\n",
              "      <td>2.568600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1699</td>\n",
              "      <td>2.623500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1700</td>\n",
              "      <td>2.640800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1701</td>\n",
              "      <td>2.354000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1702</td>\n",
              "      <td>2.621900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1703</td>\n",
              "      <td>2.282200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1704</td>\n",
              "      <td>2.339100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1705</td>\n",
              "      <td>2.546700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1706</td>\n",
              "      <td>2.555100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1707</td>\n",
              "      <td>2.581400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1708</td>\n",
              "      <td>2.368000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1709</td>\n",
              "      <td>2.620400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1710</td>\n",
              "      <td>2.270800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1711</td>\n",
              "      <td>2.708900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1712</td>\n",
              "      <td>2.581000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1713</td>\n",
              "      <td>2.602600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1714</td>\n",
              "      <td>2.602600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1715</td>\n",
              "      <td>2.581000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1716</td>\n",
              "      <td>2.190300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1717</td>\n",
              "      <td>2.098500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1718</td>\n",
              "      <td>2.471200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1719</td>\n",
              "      <td>2.570600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1720</td>\n",
              "      <td>2.687400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1721</td>\n",
              "      <td>2.294000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1722</td>\n",
              "      <td>2.653800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1723</td>\n",
              "      <td>2.621300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1724</td>\n",
              "      <td>2.503200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1725</td>\n",
              "      <td>2.220700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1726</td>\n",
              "      <td>2.529200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1727</td>\n",
              "      <td>2.505300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1728</td>\n",
              "      <td>2.549600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1729</td>\n",
              "      <td>2.325600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1730</td>\n",
              "      <td>2.556500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1731</td>\n",
              "      <td>2.358200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1732</td>\n",
              "      <td>2.281000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1733</td>\n",
              "      <td>2.328900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1734</td>\n",
              "      <td>2.495400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1735</td>\n",
              "      <td>2.336700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1736</td>\n",
              "      <td>2.480200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1737</td>\n",
              "      <td>2.621900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1738</td>\n",
              "      <td>2.629900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1739</td>\n",
              "      <td>2.346400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1740</td>\n",
              "      <td>2.501700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1741</td>\n",
              "      <td>2.161300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1742</td>\n",
              "      <td>2.655400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1743</td>\n",
              "      <td>2.501600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1744</td>\n",
              "      <td>2.297100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1745</td>\n",
              "      <td>2.570800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1746</td>\n",
              "      <td>2.538800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1747</td>\n",
              "      <td>2.562600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1748</td>\n",
              "      <td>2.570500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1749</td>\n",
              "      <td>2.332300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1750</td>\n",
              "      <td>2.609900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1751</td>\n",
              "      <td>2.646400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1752</td>\n",
              "      <td>2.487100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1753</td>\n",
              "      <td>2.496500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1754</td>\n",
              "      <td>2.358800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1755</td>\n",
              "      <td>2.500900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1756</td>\n",
              "      <td>2.339300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1757</td>\n",
              "      <td>2.445400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1758</td>\n",
              "      <td>2.728500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1759</td>\n",
              "      <td>2.600700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1760</td>\n",
              "      <td>2.686800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1761</td>\n",
              "      <td>2.382000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1762</td>\n",
              "      <td>2.200200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1763</td>\n",
              "      <td>2.676700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1764</td>\n",
              "      <td>2.390000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1765</td>\n",
              "      <td>2.427800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1766</td>\n",
              "      <td>2.557100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1767</td>\n",
              "      <td>2.346100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1768</td>\n",
              "      <td>2.451100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1769</td>\n",
              "      <td>2.292800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1770</td>\n",
              "      <td>2.754000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1771</td>\n",
              "      <td>2.614200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1772</td>\n",
              "      <td>2.515700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1773</td>\n",
              "      <td>2.610000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1774</td>\n",
              "      <td>2.534700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1775</td>\n",
              "      <td>2.563800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1776</td>\n",
              "      <td>2.519400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1777</td>\n",
              "      <td>2.569700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1778</td>\n",
              "      <td>2.027200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1779</td>\n",
              "      <td>2.570800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1780</td>\n",
              "      <td>2.482000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1781</td>\n",
              "      <td>2.661000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1782</td>\n",
              "      <td>2.426600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1783</td>\n",
              "      <td>2.428500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1784</td>\n",
              "      <td>2.303000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1785</td>\n",
              "      <td>1.946800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1786</td>\n",
              "      <td>2.246100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1787</td>\n",
              "      <td>2.618100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1788</td>\n",
              "      <td>2.555800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1789</td>\n",
              "      <td>2.249100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1790</td>\n",
              "      <td>2.627700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1791</td>\n",
              "      <td>2.218500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1792</td>\n",
              "      <td>2.715100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1793</td>\n",
              "      <td>2.662900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1794</td>\n",
              "      <td>2.680200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1795</td>\n",
              "      <td>2.607800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1796</td>\n",
              "      <td>2.488500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1797</td>\n",
              "      <td>2.504700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1798</td>\n",
              "      <td>2.562400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1799</td>\n",
              "      <td>2.497300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1800</td>\n",
              "      <td>2.790000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1801</td>\n",
              "      <td>2.590000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1802</td>\n",
              "      <td>2.251700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1803</td>\n",
              "      <td>2.533200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1804</td>\n",
              "      <td>2.628900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1805</td>\n",
              "      <td>2.570900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1806</td>\n",
              "      <td>2.802000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1807</td>\n",
              "      <td>2.592800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1808</td>\n",
              "      <td>2.759700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1809</td>\n",
              "      <td>2.662100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1810</td>\n",
              "      <td>2.215100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1811</td>\n",
              "      <td>2.321500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1812</td>\n",
              "      <td>2.482200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1813</td>\n",
              "      <td>2.685800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1814</td>\n",
              "      <td>2.760000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1815</td>\n",
              "      <td>2.544800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1816</td>\n",
              "      <td>2.199900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1817</td>\n",
              "      <td>2.710400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1818</td>\n",
              "      <td>2.237900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1819</td>\n",
              "      <td>2.395500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1820</td>\n",
              "      <td>2.584200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1821</td>\n",
              "      <td>2.655100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1822</td>\n",
              "      <td>2.503600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1823</td>\n",
              "      <td>2.506900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1824</td>\n",
              "      <td>2.684900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1825</td>\n",
              "      <td>2.664100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1826</td>\n",
              "      <td>2.490100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1827</td>\n",
              "      <td>2.648800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1828</td>\n",
              "      <td>2.565400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1829</td>\n",
              "      <td>2.640100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1830</td>\n",
              "      <td>2.573700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1831</td>\n",
              "      <td>2.459500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1832</td>\n",
              "      <td>2.516200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1833</td>\n",
              "      <td>2.505900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1834</td>\n",
              "      <td>2.542500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1835</td>\n",
              "      <td>2.601800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1836</td>\n",
              "      <td>2.424900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1837</td>\n",
              "      <td>2.464100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1838</td>\n",
              "      <td>2.546700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1839</td>\n",
              "      <td>2.429100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1840</td>\n",
              "      <td>2.643900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1841</td>\n",
              "      <td>2.462800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1842</td>\n",
              "      <td>2.392400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1843</td>\n",
              "      <td>2.526600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1844</td>\n",
              "      <td>2.707600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1845</td>\n",
              "      <td>2.575000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1846</td>\n",
              "      <td>2.349100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1847</td>\n",
              "      <td>2.350100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1848</td>\n",
              "      <td>2.300200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1849</td>\n",
              "      <td>2.571200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1850</td>\n",
              "      <td>2.503500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1851</td>\n",
              "      <td>2.581600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1852</td>\n",
              "      <td>2.565600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1853</td>\n",
              "      <td>2.542500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1854</td>\n",
              "      <td>2.671500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1855</td>\n",
              "      <td>2.501600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1856</td>\n",
              "      <td>2.709900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1857</td>\n",
              "      <td>2.655700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1858</td>\n",
              "      <td>2.417100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1859</td>\n",
              "      <td>2.305100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1860</td>\n",
              "      <td>2.472900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1861</td>\n",
              "      <td>2.760100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1862</td>\n",
              "      <td>2.571100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1863</td>\n",
              "      <td>2.430800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1864</td>\n",
              "      <td>2.717900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1865</td>\n",
              "      <td>2.591000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1866</td>\n",
              "      <td>2.671100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1867</td>\n",
              "      <td>2.702300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1868</td>\n",
              "      <td>2.604000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1869</td>\n",
              "      <td>2.582700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1870</td>\n",
              "      <td>2.624500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1871</td>\n",
              "      <td>2.604400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1872</td>\n",
              "      <td>2.501600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1873</td>\n",
              "      <td>2.522400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1874</td>\n",
              "      <td>2.185400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1875</td>\n",
              "      <td>2.507900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1876</td>\n",
              "      <td>2.531100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1877</td>\n",
              "      <td>2.442400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1878</td>\n",
              "      <td>2.494600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1879</td>\n",
              "      <td>2.231100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1880</td>\n",
              "      <td>2.585000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1881</td>\n",
              "      <td>2.498800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1882</td>\n",
              "      <td>2.456400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1883</td>\n",
              "      <td>2.394700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1884</td>\n",
              "      <td>2.578500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1885</td>\n",
              "      <td>2.618900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1886</td>\n",
              "      <td>2.401900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1887</td>\n",
              "      <td>2.599600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1888</td>\n",
              "      <td>2.645600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1889</td>\n",
              "      <td>2.471600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1890</td>\n",
              "      <td>2.576800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1891</td>\n",
              "      <td>2.625000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1892</td>\n",
              "      <td>2.518500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1893</td>\n",
              "      <td>2.515000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1894</td>\n",
              "      <td>2.365800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1895</td>\n",
              "      <td>2.229800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1896</td>\n",
              "      <td>2.353300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1897</td>\n",
              "      <td>2.579400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1898</td>\n",
              "      <td>2.090900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1899</td>\n",
              "      <td>2.669100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1900</td>\n",
              "      <td>2.615600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1901</td>\n",
              "      <td>2.482400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1902</td>\n",
              "      <td>1.993200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1903</td>\n",
              "      <td>2.564900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1904</td>\n",
              "      <td>2.293900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1905</td>\n",
              "      <td>2.304300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1906</td>\n",
              "      <td>2.236900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1907</td>\n",
              "      <td>2.573600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1908</td>\n",
              "      <td>2.723400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1909</td>\n",
              "      <td>2.660800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1910</td>\n",
              "      <td>2.607600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1911</td>\n",
              "      <td>2.628900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1912</td>\n",
              "      <td>2.670900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1913</td>\n",
              "      <td>2.651300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1914</td>\n",
              "      <td>2.619700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1915</td>\n",
              "      <td>2.467600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1916</td>\n",
              "      <td>2.564000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1917</td>\n",
              "      <td>2.692400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1918</td>\n",
              "      <td>1.997300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1919</td>\n",
              "      <td>2.531300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1920</td>\n",
              "      <td>2.585300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1921</td>\n",
              "      <td>2.356500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1922</td>\n",
              "      <td>2.509900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1923</td>\n",
              "      <td>2.585900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1924</td>\n",
              "      <td>2.194000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1925</td>\n",
              "      <td>2.274200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1926</td>\n",
              "      <td>2.384000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1927</td>\n",
              "      <td>2.501800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1928</td>\n",
              "      <td>2.198700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1929</td>\n",
              "      <td>2.590900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1930</td>\n",
              "      <td>2.351000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1931</td>\n",
              "      <td>2.556500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1932</td>\n",
              "      <td>2.290800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1933</td>\n",
              "      <td>2.450200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1934</td>\n",
              "      <td>2.734500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1935</td>\n",
              "      <td>2.709400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1936</td>\n",
              "      <td>2.517000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1937</td>\n",
              "      <td>2.631800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1938</td>\n",
              "      <td>2.588700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1939</td>\n",
              "      <td>2.056200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1940</td>\n",
              "      <td>2.480700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1941</td>\n",
              "      <td>2.416600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1942</td>\n",
              "      <td>2.617400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1943</td>\n",
              "      <td>2.198200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1944</td>\n",
              "      <td>2.568800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1945</td>\n",
              "      <td>2.345900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1946</td>\n",
              "      <td>2.599300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1947</td>\n",
              "      <td>2.552600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1948</td>\n",
              "      <td>2.255600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1949</td>\n",
              "      <td>2.544200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1950</td>\n",
              "      <td>2.514600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1951</td>\n",
              "      <td>2.623200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1952</td>\n",
              "      <td>2.506300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1953</td>\n",
              "      <td>2.008400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1954</td>\n",
              "      <td>2.473300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1955</td>\n",
              "      <td>2.533600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1956</td>\n",
              "      <td>2.532400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1957</td>\n",
              "      <td>2.608700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1958</td>\n",
              "      <td>2.615200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1959</td>\n",
              "      <td>2.643900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1960</td>\n",
              "      <td>2.355100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1961</td>\n",
              "      <td>2.520700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1962</td>\n",
              "      <td>2.488000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1963</td>\n",
              "      <td>2.508600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1964</td>\n",
              "      <td>2.558300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1965</td>\n",
              "      <td>2.179900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1966</td>\n",
              "      <td>2.289000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1967</td>\n",
              "      <td>2.353700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1968</td>\n",
              "      <td>2.602800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1969</td>\n",
              "      <td>2.320600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1970</td>\n",
              "      <td>2.781600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1971</td>\n",
              "      <td>2.204900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1972</td>\n",
              "      <td>2.283400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1973</td>\n",
              "      <td>2.726100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1974</td>\n",
              "      <td>2.614200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1975</td>\n",
              "      <td>2.557500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1976</td>\n",
              "      <td>2.470100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1977</td>\n",
              "      <td>2.609600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1978</td>\n",
              "      <td>2.430200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1979</td>\n",
              "      <td>2.566100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1980</td>\n",
              "      <td>2.456100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1981</td>\n",
              "      <td>2.675300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1982</td>\n",
              "      <td>2.218000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1983</td>\n",
              "      <td>2.652400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1984</td>\n",
              "      <td>2.323900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1985</td>\n",
              "      <td>2.516100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1986</td>\n",
              "      <td>2.562300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1987</td>\n",
              "      <td>2.149300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1988</td>\n",
              "      <td>2.253000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1989</td>\n",
              "      <td>2.462000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1990</td>\n",
              "      <td>2.522600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1991</td>\n",
              "      <td>2.583200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1992</td>\n",
              "      <td>2.589900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1993</td>\n",
              "      <td>2.710100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1994</td>\n",
              "      <td>2.229700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1995</td>\n",
              "      <td>2.675300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1996</td>\n",
              "      <td>2.513300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1997</td>\n",
              "      <td>2.698900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1998</td>\n",
              "      <td>2.555900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1999</td>\n",
              "      <td>2.222100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2000</td>\n",
              "      <td>2.322400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2001</td>\n",
              "      <td>2.646500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2002</td>\n",
              "      <td>2.200100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2003</td>\n",
              "      <td>2.535100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2004</td>\n",
              "      <td>2.420000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2005</td>\n",
              "      <td>2.744100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2006</td>\n",
              "      <td>2.599700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2007</td>\n",
              "      <td>2.418900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2008</td>\n",
              "      <td>2.516300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2009</td>\n",
              "      <td>2.588200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2010</td>\n",
              "      <td>2.679400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2011</td>\n",
              "      <td>2.393600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2012</td>\n",
              "      <td>2.473700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2013</td>\n",
              "      <td>2.657000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2014</td>\n",
              "      <td>2.656200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2015</td>\n",
              "      <td>2.625500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2016</td>\n",
              "      <td>2.473900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2017</td>\n",
              "      <td>2.652800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2018</td>\n",
              "      <td>2.471800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2019</td>\n",
              "      <td>2.433200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2020</td>\n",
              "      <td>2.692200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2021</td>\n",
              "      <td>2.451100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2022</td>\n",
              "      <td>2.439500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2023</td>\n",
              "      <td>2.728500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2024</td>\n",
              "      <td>2.234700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2025</td>\n",
              "      <td>2.812200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2026</td>\n",
              "      <td>2.576900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2027</td>\n",
              "      <td>2.650800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2028</td>\n",
              "      <td>2.542700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2029</td>\n",
              "      <td>2.795000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2030</td>\n",
              "      <td>2.432800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2031</td>\n",
              "      <td>2.683900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2032</td>\n",
              "      <td>2.476500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2033</td>\n",
              "      <td>2.575500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2034</td>\n",
              "      <td>2.519100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2035</td>\n",
              "      <td>2.631800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2036</td>\n",
              "      <td>2.619700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2037</td>\n",
              "      <td>2.224900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2038</td>\n",
              "      <td>2.580800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2039</td>\n",
              "      <td>2.547800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2040</td>\n",
              "      <td>2.225300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2041</td>\n",
              "      <td>2.013400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2042</td>\n",
              "      <td>2.631900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2043</td>\n",
              "      <td>2.598300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2044</td>\n",
              "      <td>2.591800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2045</td>\n",
              "      <td>2.619900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2046</td>\n",
              "      <td>2.612100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2047</td>\n",
              "      <td>2.681300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2048</td>\n",
              "      <td>2.457200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2049</td>\n",
              "      <td>2.569100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2050</td>\n",
              "      <td>2.681200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2051</td>\n",
              "      <td>2.387700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2052</td>\n",
              "      <td>2.544900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2053</td>\n",
              "      <td>2.518400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2054</td>\n",
              "      <td>2.337800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2055</td>\n",
              "      <td>2.573900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2056</td>\n",
              "      <td>2.616900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2057</td>\n",
              "      <td>2.042300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2058</td>\n",
              "      <td>2.331000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2059</td>\n",
              "      <td>2.484300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2060</td>\n",
              "      <td>2.285300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2061</td>\n",
              "      <td>2.251900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2062</td>\n",
              "      <td>2.538800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2063</td>\n",
              "      <td>2.519900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2064</td>\n",
              "      <td>2.745800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2065</td>\n",
              "      <td>2.450700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2066</td>\n",
              "      <td>2.641700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2067</td>\n",
              "      <td>2.615700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2068</td>\n",
              "      <td>2.592900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2069</td>\n",
              "      <td>2.700200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2070</td>\n",
              "      <td>2.038400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2071</td>\n",
              "      <td>2.321800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2072</td>\n",
              "      <td>2.581800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2073</td>\n",
              "      <td>2.666700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2074</td>\n",
              "      <td>2.138700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2075</td>\n",
              "      <td>2.505500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2076</td>\n",
              "      <td>2.330700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2077</td>\n",
              "      <td>2.468400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2078</td>\n",
              "      <td>2.277300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2079</td>\n",
              "      <td>2.382400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2080</td>\n",
              "      <td>2.524100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2081</td>\n",
              "      <td>2.130600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2082</td>\n",
              "      <td>2.544400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2083</td>\n",
              "      <td>2.203900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2084</td>\n",
              "      <td>2.563600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2085</td>\n",
              "      <td>2.436400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2086</td>\n",
              "      <td>2.644400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2087</td>\n",
              "      <td>2.729100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2088</td>\n",
              "      <td>2.524100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2089</td>\n",
              "      <td>2.806600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2090</td>\n",
              "      <td>2.624600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2091</td>\n",
              "      <td>2.344000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2092</td>\n",
              "      <td>2.468500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2093</td>\n",
              "      <td>2.083100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2094</td>\n",
              "      <td>2.569600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2095</td>\n",
              "      <td>2.491100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2096</td>\n",
              "      <td>2.593000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2097</td>\n",
              "      <td>2.484900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2098</td>\n",
              "      <td>2.398900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2099</td>\n",
              "      <td>2.486600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2100</td>\n",
              "      <td>2.442000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2101</td>\n",
              "      <td>2.626200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2102</td>\n",
              "      <td>2.533400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2103</td>\n",
              "      <td>2.513300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2104</td>\n",
              "      <td>2.595900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2105</td>\n",
              "      <td>2.394700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2106</td>\n",
              "      <td>2.563400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2107</td>\n",
              "      <td>2.769600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2108</td>\n",
              "      <td>2.512300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2109</td>\n",
              "      <td>2.579300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2110</td>\n",
              "      <td>2.505900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2111</td>\n",
              "      <td>2.592700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2112</td>\n",
              "      <td>2.239800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2113</td>\n",
              "      <td>2.648700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2114</td>\n",
              "      <td>2.414300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2115</td>\n",
              "      <td>2.561300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2116</td>\n",
              "      <td>2.478000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2117</td>\n",
              "      <td>2.571800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2118</td>\n",
              "      <td>2.493900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2119</td>\n",
              "      <td>2.585900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2120</td>\n",
              "      <td>2.333000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2121</td>\n",
              "      <td>2.569700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2122</td>\n",
              "      <td>2.328900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2123</td>\n",
              "      <td>2.695900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2124</td>\n",
              "      <td>2.513700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2125</td>\n",
              "      <td>2.588700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2126</td>\n",
              "      <td>2.459200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2127</td>\n",
              "      <td>2.754900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2128</td>\n",
              "      <td>2.406500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2129</td>\n",
              "      <td>2.431300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2130</td>\n",
              "      <td>2.499500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2131</td>\n",
              "      <td>2.536600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2132</td>\n",
              "      <td>2.571700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2133</td>\n",
              "      <td>2.566400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2134</td>\n",
              "      <td>2.438300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2135</td>\n",
              "      <td>2.392000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2136</td>\n",
              "      <td>2.025100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2137</td>\n",
              "      <td>2.614100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2138</td>\n",
              "      <td>2.494100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2139</td>\n",
              "      <td>2.239300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2140</td>\n",
              "      <td>2.643200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2141</td>\n",
              "      <td>2.523700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2142</td>\n",
              "      <td>2.292300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2143</td>\n",
              "      <td>2.321600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2144</td>\n",
              "      <td>2.314800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2145</td>\n",
              "      <td>2.642200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2146</td>\n",
              "      <td>2.624700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2147</td>\n",
              "      <td>2.567500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2148</td>\n",
              "      <td>2.498600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2149</td>\n",
              "      <td>2.499900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2150</td>\n",
              "      <td>2.488300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2151</td>\n",
              "      <td>2.397300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2152</td>\n",
              "      <td>2.823300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2153</td>\n",
              "      <td>2.573200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2154</td>\n",
              "      <td>2.595000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2155</td>\n",
              "      <td>2.567700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2156</td>\n",
              "      <td>2.478500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2157</td>\n",
              "      <td>2.501000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2158</td>\n",
              "      <td>2.636700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2159</td>\n",
              "      <td>2.510200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2160</td>\n",
              "      <td>2.078000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2161</td>\n",
              "      <td>2.698300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2162</td>\n",
              "      <td>2.565400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2163</td>\n",
              "      <td>2.571500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2164</td>\n",
              "      <td>2.415400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2165</td>\n",
              "      <td>2.445700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2166</td>\n",
              "      <td>2.141100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2167</td>\n",
              "      <td>2.432500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2168</td>\n",
              "      <td>2.167800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2169</td>\n",
              "      <td>2.497300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2170</td>\n",
              "      <td>2.426100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2171</td>\n",
              "      <td>2.589400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2172</td>\n",
              "      <td>2.282800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2173</td>\n",
              "      <td>2.322600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2174</td>\n",
              "      <td>2.235700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2175</td>\n",
              "      <td>2.589500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2176</td>\n",
              "      <td>2.650900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2177</td>\n",
              "      <td>2.591000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2178</td>\n",
              "      <td>2.400900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2179</td>\n",
              "      <td>2.455600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2180</td>\n",
              "      <td>2.518900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2181</td>\n",
              "      <td>2.416600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2182</td>\n",
              "      <td>2.353600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2183</td>\n",
              "      <td>2.512000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2184</td>\n",
              "      <td>2.595200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2185</td>\n",
              "      <td>2.432300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2186</td>\n",
              "      <td>2.303400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2187</td>\n",
              "      <td>2.194300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2188</td>\n",
              "      <td>2.382300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2189</td>\n",
              "      <td>2.388700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2190</td>\n",
              "      <td>2.571900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2191</td>\n",
              "      <td>2.513300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2192</td>\n",
              "      <td>2.440600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2193</td>\n",
              "      <td>2.629200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2194</td>\n",
              "      <td>2.514200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2195</td>\n",
              "      <td>2.523600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2196</td>\n",
              "      <td>2.693100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2197</td>\n",
              "      <td>2.540200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2198</td>\n",
              "      <td>2.598500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2199</td>\n",
              "      <td>2.663800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2200</td>\n",
              "      <td>2.655000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2201</td>\n",
              "      <td>2.513900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2202</td>\n",
              "      <td>2.448000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2203</td>\n",
              "      <td>1.963000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2204</td>\n",
              "      <td>2.439400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2205</td>\n",
              "      <td>2.353200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2206</td>\n",
              "      <td>2.479000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2207</td>\n",
              "      <td>2.402700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2208</td>\n",
              "      <td>2.556800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2209</td>\n",
              "      <td>2.649900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2210</td>\n",
              "      <td>2.143700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2211</td>\n",
              "      <td>2.170900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2212</td>\n",
              "      <td>2.532700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2213</td>\n",
              "      <td>2.503600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2214</td>\n",
              "      <td>2.496700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2215</td>\n",
              "      <td>2.639100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2216</td>\n",
              "      <td>2.311900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2217</td>\n",
              "      <td>2.546900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2218</td>\n",
              "      <td>2.611600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2219</td>\n",
              "      <td>2.494100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2220</td>\n",
              "      <td>2.460300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2221</td>\n",
              "      <td>2.616400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2222</td>\n",
              "      <td>2.657400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2223</td>\n",
              "      <td>2.323400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2224</td>\n",
              "      <td>2.385100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2225</td>\n",
              "      <td>2.497300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2226</td>\n",
              "      <td>2.529500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2227</td>\n",
              "      <td>2.523200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2228</td>\n",
              "      <td>2.462400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2229</td>\n",
              "      <td>2.383800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2230</td>\n",
              "      <td>2.638200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2231</td>\n",
              "      <td>2.591500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2232</td>\n",
              "      <td>2.243100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2233</td>\n",
              "      <td>2.609600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2234</td>\n",
              "      <td>2.571700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2235</td>\n",
              "      <td>2.377600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2236</td>\n",
              "      <td>2.657700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2237</td>\n",
              "      <td>2.536600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2238</td>\n",
              "      <td>2.478100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2239</td>\n",
              "      <td>2.617400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2240</td>\n",
              "      <td>2.248800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2241</td>\n",
              "      <td>2.484200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2242</td>\n",
              "      <td>2.665100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2243</td>\n",
              "      <td>2.533600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2244</td>\n",
              "      <td>2.477400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2245</td>\n",
              "      <td>2.408700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2246</td>\n",
              "      <td>2.568100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2247</td>\n",
              "      <td>2.488400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2248</td>\n",
              "      <td>2.563500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2249</td>\n",
              "      <td>2.267600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2250</td>\n",
              "      <td>2.417200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2251</td>\n",
              "      <td>2.617000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2252</td>\n",
              "      <td>2.451100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2253</td>\n",
              "      <td>2.334700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2254</td>\n",
              "      <td>2.455300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2255</td>\n",
              "      <td>2.529200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2256</td>\n",
              "      <td>2.188800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2257</td>\n",
              "      <td>2.453900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2258</td>\n",
              "      <td>2.462600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2259</td>\n",
              "      <td>2.322600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2260</td>\n",
              "      <td>2.615900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2261</td>\n",
              "      <td>2.657200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2262</td>\n",
              "      <td>2.557900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2263</td>\n",
              "      <td>2.552200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2264</td>\n",
              "      <td>2.660900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2265</td>\n",
              "      <td>2.534000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2266</td>\n",
              "      <td>2.562400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2267</td>\n",
              "      <td>2.481500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2268</td>\n",
              "      <td>2.540300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2269</td>\n",
              "      <td>2.608600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2270</td>\n",
              "      <td>2.569200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2271</td>\n",
              "      <td>2.611900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2272</td>\n",
              "      <td>2.474600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2273</td>\n",
              "      <td>2.555600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2274</td>\n",
              "      <td>2.429300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2275</td>\n",
              "      <td>2.577600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2276</td>\n",
              "      <td>2.378100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2277</td>\n",
              "      <td>2.508100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2278</td>\n",
              "      <td>2.299900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2279</td>\n",
              "      <td>2.596800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2280</td>\n",
              "      <td>2.475400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2281</td>\n",
              "      <td>2.494000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2282</td>\n",
              "      <td>2.644300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2283</td>\n",
              "      <td>2.290800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2284</td>\n",
              "      <td>2.506300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2285</td>\n",
              "      <td>2.616000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2286</td>\n",
              "      <td>2.508900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2287</td>\n",
              "      <td>2.559400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2288</td>\n",
              "      <td>2.482100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2289</td>\n",
              "      <td>2.493100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2290</td>\n",
              "      <td>2.557000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2291</td>\n",
              "      <td>2.545300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2292</td>\n",
              "      <td>2.370400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2293</td>\n",
              "      <td>2.378900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2294</td>\n",
              "      <td>2.243600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2295</td>\n",
              "      <td>2.545000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2296</td>\n",
              "      <td>2.679500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2297</td>\n",
              "      <td>2.275000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2298</td>\n",
              "      <td>2.519100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2299</td>\n",
              "      <td>2.595200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2300</td>\n",
              "      <td>2.663800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2301</td>\n",
              "      <td>2.456000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2302</td>\n",
              "      <td>2.354300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2303</td>\n",
              "      <td>2.488900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2304</td>\n",
              "      <td>2.412500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2305</td>\n",
              "      <td>2.496500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2306</td>\n",
              "      <td>2.650800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2307</td>\n",
              "      <td>2.503300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2308</td>\n",
              "      <td>2.533300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2309</td>\n",
              "      <td>2.521600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2310</td>\n",
              "      <td>2.528900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2311</td>\n",
              "      <td>2.621000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2312</td>\n",
              "      <td>2.322100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2313</td>\n",
              "      <td>2.578600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2314</td>\n",
              "      <td>2.725700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2315</td>\n",
              "      <td>2.355900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2316</td>\n",
              "      <td>2.608400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2317</td>\n",
              "      <td>2.585800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2318</td>\n",
              "      <td>2.320800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2319</td>\n",
              "      <td>2.578000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2320</td>\n",
              "      <td>2.639300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2321</td>\n",
              "      <td>2.463600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2322</td>\n",
              "      <td>2.366400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2323</td>\n",
              "      <td>2.590900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2324</td>\n",
              "      <td>2.382700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2325</td>\n",
              "      <td>2.640100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2326</td>\n",
              "      <td>2.514000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2327</td>\n",
              "      <td>2.244000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2328</td>\n",
              "      <td>2.530400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2329</td>\n",
              "      <td>2.561400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2330</td>\n",
              "      <td>2.840900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2331</td>\n",
              "      <td>2.675800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2332</td>\n",
              "      <td>2.587500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2333</td>\n",
              "      <td>2.375600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2334</td>\n",
              "      <td>2.251500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2335</td>\n",
              "      <td>2.468300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2336</td>\n",
              "      <td>2.277600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2337</td>\n",
              "      <td>2.783300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2338</td>\n",
              "      <td>2.414300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2339</td>\n",
              "      <td>2.351900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2340</td>\n",
              "      <td>2.553300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2341</td>\n",
              "      <td>2.631600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2342</td>\n",
              "      <td>2.524200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2343</td>\n",
              "      <td>2.619600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2344</td>\n",
              "      <td>2.527900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2345</td>\n",
              "      <td>2.532100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2346</td>\n",
              "      <td>2.506300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2347</td>\n",
              "      <td>2.538000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2348</td>\n",
              "      <td>2.445600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2349</td>\n",
              "      <td>2.269500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2350</td>\n",
              "      <td>2.480700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2351</td>\n",
              "      <td>2.604900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2352</td>\n",
              "      <td>2.384500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2353</td>\n",
              "      <td>2.577900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2354</td>\n",
              "      <td>2.479500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2355</td>\n",
              "      <td>2.214900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2356</td>\n",
              "      <td>2.342000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2357</td>\n",
              "      <td>1.985900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2358</td>\n",
              "      <td>2.121300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2359</td>\n",
              "      <td>2.530700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2360</td>\n",
              "      <td>2.465500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2361</td>\n",
              "      <td>2.561900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2362</td>\n",
              "      <td>2.531400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2363</td>\n",
              "      <td>2.546500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2364</td>\n",
              "      <td>2.568400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2365</td>\n",
              "      <td>2.610700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2366</td>\n",
              "      <td>2.514100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2367</td>\n",
              "      <td>2.501800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2368</td>\n",
              "      <td>2.591800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2369</td>\n",
              "      <td>2.267900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2370</td>\n",
              "      <td>2.532000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2371</td>\n",
              "      <td>2.522300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2372</td>\n",
              "      <td>2.529400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2373</td>\n",
              "      <td>2.421200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2374</td>\n",
              "      <td>2.605600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2375</td>\n",
              "      <td>2.429200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2376</td>\n",
              "      <td>2.526400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2377</td>\n",
              "      <td>2.464700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2378</td>\n",
              "      <td>2.026700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2379</td>\n",
              "      <td>2.094300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2380</td>\n",
              "      <td>2.723000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2381</td>\n",
              "      <td>2.653700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2382</td>\n",
              "      <td>2.514400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2383</td>\n",
              "      <td>2.396400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2384</td>\n",
              "      <td>2.776800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2385</td>\n",
              "      <td>2.454800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2386</td>\n",
              "      <td>2.517100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2387</td>\n",
              "      <td>2.639200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2388</td>\n",
              "      <td>2.549600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2389</td>\n",
              "      <td>2.271200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2390</td>\n",
              "      <td>2.642300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2391</td>\n",
              "      <td>2.310700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2392</td>\n",
              "      <td>2.560200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2393</td>\n",
              "      <td>2.661100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2394</td>\n",
              "      <td>2.668900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2395</td>\n",
              "      <td>2.272600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2396</td>\n",
              "      <td>2.303700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2397</td>\n",
              "      <td>2.610100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2398</td>\n",
              "      <td>2.606500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2399</td>\n",
              "      <td>2.649900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2400</td>\n",
              "      <td>2.381500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2401</td>\n",
              "      <td>2.472600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2402</td>\n",
              "      <td>2.560700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2403</td>\n",
              "      <td>2.016400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2404</td>\n",
              "      <td>2.503500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2405</td>\n",
              "      <td>2.676500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2406</td>\n",
              "      <td>2.305600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2407</td>\n",
              "      <td>2.221600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2408</td>\n",
              "      <td>2.527500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2409</td>\n",
              "      <td>2.557600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2410</td>\n",
              "      <td>2.253600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2411</td>\n",
              "      <td>2.527500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2412</td>\n",
              "      <td>2.329100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2413</td>\n",
              "      <td>2.526000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2414</td>\n",
              "      <td>2.422900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2415</td>\n",
              "      <td>2.552300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2416</td>\n",
              "      <td>2.437100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2417</td>\n",
              "      <td>2.490400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2418</td>\n",
              "      <td>2.344500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2419</td>\n",
              "      <td>2.567300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2420</td>\n",
              "      <td>2.490900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2421</td>\n",
              "      <td>2.674700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2422</td>\n",
              "      <td>2.652300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2423</td>\n",
              "      <td>2.550800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2424</td>\n",
              "      <td>2.182900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2425</td>\n",
              "      <td>2.534000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2426</td>\n",
              "      <td>2.467100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2427</td>\n",
              "      <td>2.407200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2428</td>\n",
              "      <td>2.598000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2429</td>\n",
              "      <td>2.475200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2430</td>\n",
              "      <td>2.680000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2431</td>\n",
              "      <td>1.976500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2432</td>\n",
              "      <td>2.601100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2433</td>\n",
              "      <td>1.921700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2434</td>\n",
              "      <td>2.161400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2435</td>\n",
              "      <td>2.433900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2436</td>\n",
              "      <td>2.628000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2437</td>\n",
              "      <td>2.290800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2438</td>\n",
              "      <td>2.346100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2439</td>\n",
              "      <td>2.341900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2440</td>\n",
              "      <td>2.472900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2441</td>\n",
              "      <td>2.469400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2442</td>\n",
              "      <td>2.492600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2443</td>\n",
              "      <td>2.371700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2444</td>\n",
              "      <td>2.649900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2445</td>\n",
              "      <td>2.496400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2446</td>\n",
              "      <td>2.563400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2447</td>\n",
              "      <td>2.485200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2448</td>\n",
              "      <td>2.404800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2449</td>\n",
              "      <td>2.474900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2450</td>\n",
              "      <td>2.403900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2451</td>\n",
              "      <td>2.554400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2452</td>\n",
              "      <td>2.864000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2453</td>\n",
              "      <td>2.381300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2454</td>\n",
              "      <td>2.236500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2455</td>\n",
              "      <td>2.431500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2456</td>\n",
              "      <td>2.296300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2457</td>\n",
              "      <td>2.304300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2458</td>\n",
              "      <td>2.408800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2459</td>\n",
              "      <td>2.210400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2460</td>\n",
              "      <td>2.557100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2461</td>\n",
              "      <td>2.559500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2462</td>\n",
              "      <td>2.204800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2463</td>\n",
              "      <td>2.613600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2464</td>\n",
              "      <td>2.533600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2465</td>\n",
              "      <td>2.477700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2466</td>\n",
              "      <td>2.179600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2467</td>\n",
              "      <td>2.299300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2468</td>\n",
              "      <td>2.091900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2469</td>\n",
              "      <td>2.329300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2470</td>\n",
              "      <td>2.571700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2471</td>\n",
              "      <td>2.052700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2472</td>\n",
              "      <td>2.637100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2473</td>\n",
              "      <td>2.385200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2474</td>\n",
              "      <td>2.219700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2475</td>\n",
              "      <td>2.505300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2476</td>\n",
              "      <td>2.521100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2477</td>\n",
              "      <td>2.369000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2478</td>\n",
              "      <td>2.570400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2479</td>\n",
              "      <td>2.415300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2480</td>\n",
              "      <td>2.593400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2481</td>\n",
              "      <td>2.696500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2482</td>\n",
              "      <td>2.510400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2483</td>\n",
              "      <td>2.568100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2484</td>\n",
              "      <td>2.371300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2485</td>\n",
              "      <td>2.636500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2486</td>\n",
              "      <td>2.563700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2487</td>\n",
              "      <td>2.536200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2488</td>\n",
              "      <td>2.092300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2489</td>\n",
              "      <td>2.576000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2490</td>\n",
              "      <td>2.695900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2491</td>\n",
              "      <td>2.567400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2492</td>\n",
              "      <td>2.544800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2493</td>\n",
              "      <td>2.379500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2494</td>\n",
              "      <td>2.592600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2495</td>\n",
              "      <td>2.641000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2496</td>\n",
              "      <td>2.200700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2497</td>\n",
              "      <td>2.279000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2498</td>\n",
              "      <td>2.348800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2499</td>\n",
              "      <td>2.587900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2500</td>\n",
              "      <td>2.486900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2501</td>\n",
              "      <td>2.504300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2502</td>\n",
              "      <td>2.508000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2503</td>\n",
              "      <td>2.190000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2504</td>\n",
              "      <td>2.611700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2505</td>\n",
              "      <td>2.435500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2506</td>\n",
              "      <td>2.630900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2507</td>\n",
              "      <td>2.213700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2508</td>\n",
              "      <td>2.593900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2509</td>\n",
              "      <td>2.558500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2510</td>\n",
              "      <td>2.329300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2511</td>\n",
              "      <td>2.288600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2512</td>\n",
              "      <td>2.625400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2513</td>\n",
              "      <td>2.530300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2514</td>\n",
              "      <td>2.537300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2515</td>\n",
              "      <td>2.338300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2516</td>\n",
              "      <td>2.616200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2517</td>\n",
              "      <td>2.561500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2518</td>\n",
              "      <td>2.396900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2519</td>\n",
              "      <td>2.258500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2520</td>\n",
              "      <td>2.624100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2521</td>\n",
              "      <td>2.463600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2522</td>\n",
              "      <td>2.697800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2523</td>\n",
              "      <td>2.598000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2524</td>\n",
              "      <td>2.208600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2525</td>\n",
              "      <td>2.618200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2526</td>\n",
              "      <td>2.300400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2527</td>\n",
              "      <td>1.979300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2528</td>\n",
              "      <td>2.639600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2529</td>\n",
              "      <td>2.433500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2530</td>\n",
              "      <td>2.166500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2531</td>\n",
              "      <td>2.819800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2532</td>\n",
              "      <td>2.486400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2533</td>\n",
              "      <td>2.651200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2534</td>\n",
              "      <td>2.357000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2535</td>\n",
              "      <td>2.510300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2536</td>\n",
              "      <td>2.473400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2537</td>\n",
              "      <td>2.395200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2538</td>\n",
              "      <td>2.406400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2539</td>\n",
              "      <td>2.544000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2540</td>\n",
              "      <td>2.442400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2541</td>\n",
              "      <td>2.239400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2542</td>\n",
              "      <td>2.630700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2543</td>\n",
              "      <td>2.587300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2544</td>\n",
              "      <td>2.346600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2545</td>\n",
              "      <td>2.546200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2546</td>\n",
              "      <td>2.555200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2547</td>\n",
              "      <td>2.438000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2548</td>\n",
              "      <td>2.603000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2549</td>\n",
              "      <td>2.504100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2550</td>\n",
              "      <td>2.256100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2551</td>\n",
              "      <td>2.619500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2552</td>\n",
              "      <td>2.564300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2553</td>\n",
              "      <td>2.566800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2554</td>\n",
              "      <td>2.504800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2555</td>\n",
              "      <td>2.423400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2556</td>\n",
              "      <td>2.642700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2557</td>\n",
              "      <td>2.574200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2558</td>\n",
              "      <td>2.328500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2559</td>\n",
              "      <td>2.567000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2560</td>\n",
              "      <td>2.867500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2561</td>\n",
              "      <td>2.257000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2562</td>\n",
              "      <td>2.526700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2563</td>\n",
              "      <td>2.375000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2564</td>\n",
              "      <td>2.238200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2565</td>\n",
              "      <td>2.387600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2566</td>\n",
              "      <td>2.272400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2567</td>\n",
              "      <td>2.484300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2568</td>\n",
              "      <td>2.222400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2569</td>\n",
              "      <td>2.465500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2570</td>\n",
              "      <td>2.617900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2571</td>\n",
              "      <td>2.204200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2572</td>\n",
              "      <td>2.573400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2573</td>\n",
              "      <td>2.239400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2574</td>\n",
              "      <td>2.089900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2575</td>\n",
              "      <td>2.391800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2576</td>\n",
              "      <td>2.609500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2577</td>\n",
              "      <td>2.592700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2578</td>\n",
              "      <td>2.333000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2579</td>\n",
              "      <td>2.576600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2580</td>\n",
              "      <td>2.257400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2581</td>\n",
              "      <td>2.551900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2582</td>\n",
              "      <td>2.628300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2583</td>\n",
              "      <td>2.190600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2584</td>\n",
              "      <td>2.257600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2585</td>\n",
              "      <td>2.465400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2586</td>\n",
              "      <td>2.256500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2587</td>\n",
              "      <td>2.263800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2588</td>\n",
              "      <td>2.346200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2589</td>\n",
              "      <td>2.576500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2590</td>\n",
              "      <td>2.582200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2591</td>\n",
              "      <td>2.465800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2592</td>\n",
              "      <td>2.204800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2593</td>\n",
              "      <td>2.516900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2594</td>\n",
              "      <td>2.547600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2595</td>\n",
              "      <td>2.151300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2596</td>\n",
              "      <td>2.568100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2597</td>\n",
              "      <td>2.684700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2598</td>\n",
              "      <td>2.867400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2599</td>\n",
              "      <td>2.171300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2600</td>\n",
              "      <td>2.306500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2601</td>\n",
              "      <td>2.298700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2602</td>\n",
              "      <td>2.652500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2603</td>\n",
              "      <td>2.634400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2604</td>\n",
              "      <td>2.495000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2605</td>\n",
              "      <td>2.582800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2606</td>\n",
              "      <td>2.372500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2607</td>\n",
              "      <td>2.421100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2608</td>\n",
              "      <td>2.566400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2609</td>\n",
              "      <td>2.606800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2610</td>\n",
              "      <td>2.546400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2611</td>\n",
              "      <td>2.616800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2612</td>\n",
              "      <td>2.639400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2613</td>\n",
              "      <td>2.599800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2614</td>\n",
              "      <td>2.524100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2615</td>\n",
              "      <td>2.461000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2616</td>\n",
              "      <td>2.635500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2617</td>\n",
              "      <td>2.494500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2618</td>\n",
              "      <td>2.447000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2619</td>\n",
              "      <td>2.515500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2620</td>\n",
              "      <td>2.577100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2621</td>\n",
              "      <td>2.605700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2622</td>\n",
              "      <td>2.588000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2623</td>\n",
              "      <td>2.602100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2624</td>\n",
              "      <td>2.571800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2625</td>\n",
              "      <td>2.443600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2626</td>\n",
              "      <td>2.830700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2627</td>\n",
              "      <td>2.665000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2628</td>\n",
              "      <td>2.532000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2629</td>\n",
              "      <td>2.126000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2630</td>\n",
              "      <td>2.556100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2631</td>\n",
              "      <td>2.279100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2632</td>\n",
              "      <td>2.569600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2633</td>\n",
              "      <td>2.591900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2634</td>\n",
              "      <td>2.634000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2635</td>\n",
              "      <td>2.290200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2636</td>\n",
              "      <td>2.409000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2637</td>\n",
              "      <td>2.370900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2638</td>\n",
              "      <td>2.332000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2639</td>\n",
              "      <td>2.479800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2640</td>\n",
              "      <td>2.426600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2641</td>\n",
              "      <td>2.524900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2642</td>\n",
              "      <td>2.570100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2643</td>\n",
              "      <td>2.294200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2644</td>\n",
              "      <td>2.253400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2645</td>\n",
              "      <td>2.552100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2646</td>\n",
              "      <td>2.177900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2647</td>\n",
              "      <td>2.157700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2648</td>\n",
              "      <td>2.184900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2649</td>\n",
              "      <td>2.521500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2650</td>\n",
              "      <td>2.599600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2651</td>\n",
              "      <td>2.557600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2652</td>\n",
              "      <td>2.658700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2653</td>\n",
              "      <td>2.548200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2654</td>\n",
              "      <td>2.539000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2655</td>\n",
              "      <td>2.564900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2656</td>\n",
              "      <td>2.279200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2657</td>\n",
              "      <td>2.272000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2658</td>\n",
              "      <td>2.512100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2659</td>\n",
              "      <td>2.154200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2660</td>\n",
              "      <td>2.622600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2661</td>\n",
              "      <td>2.530000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2662</td>\n",
              "      <td>1.956200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2663</td>\n",
              "      <td>2.587900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2664</td>\n",
              "      <td>2.607100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2665</td>\n",
              "      <td>2.510100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2666</td>\n",
              "      <td>2.358600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2667</td>\n",
              "      <td>2.461200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2668</td>\n",
              "      <td>2.199900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2669</td>\n",
              "      <td>2.361000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2670</td>\n",
              "      <td>2.686400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2671</td>\n",
              "      <td>2.378700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2672</td>\n",
              "      <td>2.456600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2673</td>\n",
              "      <td>2.286200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2674</td>\n",
              "      <td>2.788900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2675</td>\n",
              "      <td>2.274900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2676</td>\n",
              "      <td>2.637200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2677</td>\n",
              "      <td>2.484200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2678</td>\n",
              "      <td>2.333800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2679</td>\n",
              "      <td>2.554900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2680</td>\n",
              "      <td>2.492300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2681</td>\n",
              "      <td>2.475300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2682</td>\n",
              "      <td>2.633400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2683</td>\n",
              "      <td>2.570000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2684</td>\n",
              "      <td>2.430200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2685</td>\n",
              "      <td>2.518000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2686</td>\n",
              "      <td>2.551200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2687</td>\n",
              "      <td>2.217000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2688</td>\n",
              "      <td>2.484200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2689</td>\n",
              "      <td>2.522600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2690</td>\n",
              "      <td>2.300300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2691</td>\n",
              "      <td>2.543400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2692</td>\n",
              "      <td>2.590500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2693</td>\n",
              "      <td>2.673000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2694</td>\n",
              "      <td>2.305700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2695</td>\n",
              "      <td>2.600800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2696</td>\n",
              "      <td>2.183300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2697</td>\n",
              "      <td>2.644300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2698</td>\n",
              "      <td>2.611000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2699</td>\n",
              "      <td>2.493200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2700</td>\n",
              "      <td>2.471200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2701</td>\n",
              "      <td>2.517000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2702</td>\n",
              "      <td>1.936500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2703</td>\n",
              "      <td>2.447200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2704</td>\n",
              "      <td>2.501100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2705</td>\n",
              "      <td>2.558400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2706</td>\n",
              "      <td>2.559600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2707</td>\n",
              "      <td>2.664800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2708</td>\n",
              "      <td>2.422000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2709</td>\n",
              "      <td>2.167300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2710</td>\n",
              "      <td>2.595300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2711</td>\n",
              "      <td>2.489400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2712</td>\n",
              "      <td>2.151000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2713</td>\n",
              "      <td>2.422800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2714</td>\n",
              "      <td>2.501500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2715</td>\n",
              "      <td>2.602300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2716</td>\n",
              "      <td>2.572500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2717</td>\n",
              "      <td>2.315700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2718</td>\n",
              "      <td>2.576800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2719</td>\n",
              "      <td>2.502000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2720</td>\n",
              "      <td>2.449200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2721</td>\n",
              "      <td>2.555600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2722</td>\n",
              "      <td>2.518900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2723</td>\n",
              "      <td>2.300000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2724</td>\n",
              "      <td>2.486100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2725</td>\n",
              "      <td>2.433800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2726</td>\n",
              "      <td>2.660300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2727</td>\n",
              "      <td>2.333800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2728</td>\n",
              "      <td>2.038400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2729</td>\n",
              "      <td>2.543700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2730</td>\n",
              "      <td>2.395800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2731</td>\n",
              "      <td>2.316600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2732</td>\n",
              "      <td>2.534900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2733</td>\n",
              "      <td>2.254700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2734</td>\n",
              "      <td>2.529800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2735</td>\n",
              "      <td>2.205800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2736</td>\n",
              "      <td>2.225700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2737</td>\n",
              "      <td>2.559700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2738</td>\n",
              "      <td>2.210300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2739</td>\n",
              "      <td>2.767400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2740</td>\n",
              "      <td>2.525900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2741</td>\n",
              "      <td>2.578500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2742</td>\n",
              "      <td>2.200400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2743</td>\n",
              "      <td>2.533600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2744</td>\n",
              "      <td>2.586700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2745</td>\n",
              "      <td>2.600400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2746</td>\n",
              "      <td>2.607100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2747</td>\n",
              "      <td>2.544100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2748</td>\n",
              "      <td>2.508200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2749</td>\n",
              "      <td>2.578400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2750</td>\n",
              "      <td>2.469600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2751</td>\n",
              "      <td>2.639800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2752</td>\n",
              "      <td>2.581200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2753</td>\n",
              "      <td>2.243100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2754</td>\n",
              "      <td>2.520000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2755</td>\n",
              "      <td>2.642500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2756</td>\n",
              "      <td>2.549300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2757</td>\n",
              "      <td>2.591300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2758</td>\n",
              "      <td>2.393300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2759</td>\n",
              "      <td>2.523200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2760</td>\n",
              "      <td>2.405700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2761</td>\n",
              "      <td>2.473300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2762</td>\n",
              "      <td>2.582400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2763</td>\n",
              "      <td>2.521200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2764</td>\n",
              "      <td>2.130500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2765</td>\n",
              "      <td>2.219700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2766</td>\n",
              "      <td>2.131600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2767</td>\n",
              "      <td>2.475700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2768</td>\n",
              "      <td>2.611900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2769</td>\n",
              "      <td>2.397100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2770</td>\n",
              "      <td>2.425200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2771</td>\n",
              "      <td>2.500200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2772</td>\n",
              "      <td>2.658700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2773</td>\n",
              "      <td>2.604400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2774</td>\n",
              "      <td>2.319600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2775</td>\n",
              "      <td>2.469100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2776</td>\n",
              "      <td>2.490800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2777</td>\n",
              "      <td>2.553000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2778</td>\n",
              "      <td>2.312500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2779</td>\n",
              "      <td>2.535400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2780</td>\n",
              "      <td>2.548300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2781</td>\n",
              "      <td>2.474100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2782</td>\n",
              "      <td>2.638700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2783</td>\n",
              "      <td>2.452400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2784</td>\n",
              "      <td>2.387600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2785</td>\n",
              "      <td>2.576400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2786</td>\n",
              "      <td>2.571300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2787</td>\n",
              "      <td>2.501200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2788</td>\n",
              "      <td>2.642200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2789</td>\n",
              "      <td>2.487900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2790</td>\n",
              "      <td>2.364700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2791</td>\n",
              "      <td>2.617400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2792</td>\n",
              "      <td>2.472200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2793</td>\n",
              "      <td>2.549300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2794</td>\n",
              "      <td>2.254400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2795</td>\n",
              "      <td>2.388400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2796</td>\n",
              "      <td>2.207900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2797</td>\n",
              "      <td>2.215900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2798</td>\n",
              "      <td>1.928100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2799</td>\n",
              "      <td>2.508600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2800</td>\n",
              "      <td>2.631100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2801</td>\n",
              "      <td>2.689100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2802</td>\n",
              "      <td>2.546000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2803</td>\n",
              "      <td>2.371700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2804</td>\n",
              "      <td>2.474900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2805</td>\n",
              "      <td>2.511500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2806</td>\n",
              "      <td>2.495400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2807</td>\n",
              "      <td>2.682500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2808</td>\n",
              "      <td>2.229800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2809</td>\n",
              "      <td>2.462300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2810</td>\n",
              "      <td>2.563300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2811</td>\n",
              "      <td>2.164900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2812</td>\n",
              "      <td>2.619700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2813</td>\n",
              "      <td>2.471300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2814</td>\n",
              "      <td>2.571400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2815</td>\n",
              "      <td>2.526800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2816</td>\n",
              "      <td>2.515200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2817</td>\n",
              "      <td>2.323600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2818</td>\n",
              "      <td>2.586000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2819</td>\n",
              "      <td>2.508300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2820</td>\n",
              "      <td>2.382400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2821</td>\n",
              "      <td>2.521800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2822</td>\n",
              "      <td>2.333700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2823</td>\n",
              "      <td>2.205200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2824</td>\n",
              "      <td>2.151600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2825</td>\n",
              "      <td>2.236200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2826</td>\n",
              "      <td>2.460100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2827</td>\n",
              "      <td>2.488200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2828</td>\n",
              "      <td>2.373200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2829</td>\n",
              "      <td>2.616100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2830</td>\n",
              "      <td>2.631000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2831</td>\n",
              "      <td>2.259600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2832</td>\n",
              "      <td>2.526800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2833</td>\n",
              "      <td>2.659700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2834</td>\n",
              "      <td>2.234400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2835</td>\n",
              "      <td>2.515600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2836</td>\n",
              "      <td>2.206500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2837</td>\n",
              "      <td>2.278200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2838</td>\n",
              "      <td>2.452800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2839</td>\n",
              "      <td>2.565000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2840</td>\n",
              "      <td>2.690000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2841</td>\n",
              "      <td>2.268800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2842</td>\n",
              "      <td>2.545600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2843</td>\n",
              "      <td>2.494100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2844</td>\n",
              "      <td>2.421500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2845</td>\n",
              "      <td>2.494900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2846</td>\n",
              "      <td>2.157900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2847</td>\n",
              "      <td>2.223800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2848</td>\n",
              "      <td>2.440200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2849</td>\n",
              "      <td>2.273400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2850</td>\n",
              "      <td>2.577100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2851</td>\n",
              "      <td>2.484300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2852</td>\n",
              "      <td>2.203600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2853</td>\n",
              "      <td>2.499500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2854</td>\n",
              "      <td>2.174500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2855</td>\n",
              "      <td>2.499000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2856</td>\n",
              "      <td>2.332700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2857</td>\n",
              "      <td>2.335800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2858</td>\n",
              "      <td>2.518000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2859</td>\n",
              "      <td>2.166100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2860</td>\n",
              "      <td>2.364400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2861</td>\n",
              "      <td>2.569000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2862</td>\n",
              "      <td>2.344200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2863</td>\n",
              "      <td>2.583000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2864</td>\n",
              "      <td>2.545700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2865</td>\n",
              "      <td>2.503500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2866</td>\n",
              "      <td>2.506500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2867</td>\n",
              "      <td>2.308400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2868</td>\n",
              "      <td>2.460900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2869</td>\n",
              "      <td>2.484900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2870</td>\n",
              "      <td>2.678300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2871</td>\n",
              "      <td>2.605700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2872</td>\n",
              "      <td>2.186500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2873</td>\n",
              "      <td>2.280000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2874</td>\n",
              "      <td>2.603500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2875</td>\n",
              "      <td>2.437100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2876</td>\n",
              "      <td>2.597500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2877</td>\n",
              "      <td>2.520800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2878</td>\n",
              "      <td>2.561800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2879</td>\n",
              "      <td>2.516600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2880</td>\n",
              "      <td>2.668400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2881</td>\n",
              "      <td>2.384400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2882</td>\n",
              "      <td>2.345200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2883</td>\n",
              "      <td>2.626900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2884</td>\n",
              "      <td>2.436300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2885</td>\n",
              "      <td>2.158400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2886</td>\n",
              "      <td>2.513500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2887</td>\n",
              "      <td>2.849100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2888</td>\n",
              "      <td>2.539400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2889</td>\n",
              "      <td>2.435000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2890</td>\n",
              "      <td>2.555700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2891</td>\n",
              "      <td>2.560400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2892</td>\n",
              "      <td>2.505800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2893</td>\n",
              "      <td>2.226900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2894</td>\n",
              "      <td>2.379200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2895</td>\n",
              "      <td>2.158900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2896</td>\n",
              "      <td>2.618000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2897</td>\n",
              "      <td>2.441500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2898</td>\n",
              "      <td>2.507100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2899</td>\n",
              "      <td>2.527900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2900</td>\n",
              "      <td>2.673600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2901</td>\n",
              "      <td>2.313600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2902</td>\n",
              "      <td>2.140600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2903</td>\n",
              "      <td>2.344000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2904</td>\n",
              "      <td>2.107000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2905</td>\n",
              "      <td>2.699000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2906</td>\n",
              "      <td>2.589000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2907</td>\n",
              "      <td>2.338200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2908</td>\n",
              "      <td>2.206900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2909</td>\n",
              "      <td>2.210600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2910</td>\n",
              "      <td>2.609600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2911</td>\n",
              "      <td>2.203000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2912</td>\n",
              "      <td>2.354100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2913</td>\n",
              "      <td>2.226900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2914</td>\n",
              "      <td>2.218700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2915</td>\n",
              "      <td>2.471100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2916</td>\n",
              "      <td>2.442000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2917</td>\n",
              "      <td>2.232800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2918</td>\n",
              "      <td>2.578200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2919</td>\n",
              "      <td>2.485600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2920</td>\n",
              "      <td>2.445800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2921</td>\n",
              "      <td>2.652300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2922</td>\n",
              "      <td>2.172700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2923</td>\n",
              "      <td>2.474300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2924</td>\n",
              "      <td>2.555800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2925</td>\n",
              "      <td>2.279900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2926</td>\n",
              "      <td>2.237100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2927</td>\n",
              "      <td>2.171300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2928</td>\n",
              "      <td>2.619900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2929</td>\n",
              "      <td>2.538100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2930</td>\n",
              "      <td>2.634700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2931</td>\n",
              "      <td>2.354600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2932</td>\n",
              "      <td>2.485400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2933</td>\n",
              "      <td>2.434500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2934</td>\n",
              "      <td>2.504800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2935</td>\n",
              "      <td>2.020900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2936</td>\n",
              "      <td>2.489100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2937</td>\n",
              "      <td>2.493100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2938</td>\n",
              "      <td>2.491900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2939</td>\n",
              "      <td>2.342900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2940</td>\n",
              "      <td>2.621600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2941</td>\n",
              "      <td>2.448800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2942</td>\n",
              "      <td>2.610400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2943</td>\n",
              "      <td>2.377400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2944</td>\n",
              "      <td>2.471700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2945</td>\n",
              "      <td>2.558100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2946</td>\n",
              "      <td>2.357500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2947</td>\n",
              "      <td>2.383900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2948</td>\n",
              "      <td>1.932900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2949</td>\n",
              "      <td>2.618200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2950</td>\n",
              "      <td>2.351100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2951</td>\n",
              "      <td>2.891100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2952</td>\n",
              "      <td>2.392300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2953</td>\n",
              "      <td>2.408800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2954</td>\n",
              "      <td>2.323200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2955</td>\n",
              "      <td>2.460400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2956</td>\n",
              "      <td>2.512700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2957</td>\n",
              "      <td>2.304200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2958</td>\n",
              "      <td>2.548000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2959</td>\n",
              "      <td>2.401900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2960</td>\n",
              "      <td>1.888900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2961</td>\n",
              "      <td>2.496300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2962</td>\n",
              "      <td>2.602800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2963</td>\n",
              "      <td>2.172600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2964</td>\n",
              "      <td>2.525000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2965</td>\n",
              "      <td>2.496200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2966</td>\n",
              "      <td>2.455200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2967</td>\n",
              "      <td>2.247200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2968</td>\n",
              "      <td>2.293200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2969</td>\n",
              "      <td>2.287300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2970</td>\n",
              "      <td>2.036800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2971</td>\n",
              "      <td>2.244300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2972</td>\n",
              "      <td>2.573700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2973</td>\n",
              "      <td>2.302900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2974</td>\n",
              "      <td>2.623300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2975</td>\n",
              "      <td>2.434500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2976</td>\n",
              "      <td>2.342400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2977</td>\n",
              "      <td>2.725800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2978</td>\n",
              "      <td>2.129900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2979</td>\n",
              "      <td>2.221200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2980</td>\n",
              "      <td>2.524200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2981</td>\n",
              "      <td>2.634200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2982</td>\n",
              "      <td>2.283400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2983</td>\n",
              "      <td>2.505100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2984</td>\n",
              "      <td>2.559000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2985</td>\n",
              "      <td>2.447300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2986</td>\n",
              "      <td>2.529700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2987</td>\n",
              "      <td>2.565000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2988</td>\n",
              "      <td>2.091800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2989</td>\n",
              "      <td>2.469000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2990</td>\n",
              "      <td>2.324400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2991</td>\n",
              "      <td>2.560400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2992</td>\n",
              "      <td>2.385200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2993</td>\n",
              "      <td>2.502600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2994</td>\n",
              "      <td>2.483300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2995</td>\n",
              "      <td>2.133900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2996</td>\n",
              "      <td>2.442900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2997</td>\n",
              "      <td>2.143200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2998</td>\n",
              "      <td>2.156300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2999</td>\n",
              "      <td>2.586300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3000</td>\n",
              "      <td>2.570000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3001</td>\n",
              "      <td>2.598900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3002</td>\n",
              "      <td>2.142300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3003</td>\n",
              "      <td>2.465400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3004</td>\n",
              "      <td>2.475900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3005</td>\n",
              "      <td>2.544800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3006</td>\n",
              "      <td>2.560600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3007</td>\n",
              "      <td>2.412300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3008</td>\n",
              "      <td>2.534600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3009</td>\n",
              "      <td>2.470400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3010</td>\n",
              "      <td>2.127000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3011</td>\n",
              "      <td>2.325300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3012</td>\n",
              "      <td>2.519000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3013</td>\n",
              "      <td>2.507000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3014</td>\n",
              "      <td>2.004300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3015</td>\n",
              "      <td>2.269300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3016</td>\n",
              "      <td>2.576100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3017</td>\n",
              "      <td>2.155300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3018</td>\n",
              "      <td>2.569200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3019</td>\n",
              "      <td>2.567700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3020</td>\n",
              "      <td>2.646800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3021</td>\n",
              "      <td>2.577500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3022</td>\n",
              "      <td>1.979500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3023</td>\n",
              "      <td>2.498500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3024</td>\n",
              "      <td>2.403200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3025</td>\n",
              "      <td>2.221900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3026</td>\n",
              "      <td>2.118000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3027</td>\n",
              "      <td>2.466400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3028</td>\n",
              "      <td>2.166400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3029</td>\n",
              "      <td>2.493300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3030</td>\n",
              "      <td>2.249300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3031</td>\n",
              "      <td>2.544600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3032</td>\n",
              "      <td>2.347900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3033</td>\n",
              "      <td>2.213000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3034</td>\n",
              "      <td>2.217000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3035</td>\n",
              "      <td>2.545600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3036</td>\n",
              "      <td>2.557400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3037</td>\n",
              "      <td>2.410800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3038</td>\n",
              "      <td>2.532700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3039</td>\n",
              "      <td>2.406100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3040</td>\n",
              "      <td>2.540200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3041</td>\n",
              "      <td>2.624700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3042</td>\n",
              "      <td>2.385200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3043</td>\n",
              "      <td>1.865700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3044</td>\n",
              "      <td>2.436900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3045</td>\n",
              "      <td>2.277100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3046</td>\n",
              "      <td>2.270900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3047</td>\n",
              "      <td>2.150500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3048</td>\n",
              "      <td>1.968300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3049</td>\n",
              "      <td>2.126400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3050</td>\n",
              "      <td>2.527400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3051</td>\n",
              "      <td>2.210500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3052</td>\n",
              "      <td>2.491400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3053</td>\n",
              "      <td>2.549000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3054</td>\n",
              "      <td>2.391800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3055</td>\n",
              "      <td>2.477400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3056</td>\n",
              "      <td>2.390800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3057</td>\n",
              "      <td>2.525800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3058</td>\n",
              "      <td>2.672000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3059</td>\n",
              "      <td>1.986600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3060</td>\n",
              "      <td>2.357800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3061</td>\n",
              "      <td>2.344000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3062</td>\n",
              "      <td>2.254800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3063</td>\n",
              "      <td>2.531700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3064</td>\n",
              "      <td>2.511800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3065</td>\n",
              "      <td>2.319300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3066</td>\n",
              "      <td>2.496800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3067</td>\n",
              "      <td>2.129000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3068</td>\n",
              "      <td>2.541400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3069</td>\n",
              "      <td>1.957600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3070</td>\n",
              "      <td>2.356200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3071</td>\n",
              "      <td>2.621600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3072</td>\n",
              "      <td>2.509200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3073</td>\n",
              "      <td>2.550500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3074</td>\n",
              "      <td>2.727500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3075</td>\n",
              "      <td>2.257300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3076</td>\n",
              "      <td>2.560400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3077</td>\n",
              "      <td>2.027700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3078</td>\n",
              "      <td>2.558800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3079</td>\n",
              "      <td>2.633600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3080</td>\n",
              "      <td>2.573700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3081</td>\n",
              "      <td>2.610800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3082</td>\n",
              "      <td>2.485100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3083</td>\n",
              "      <td>2.261100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3084</td>\n",
              "      <td>2.340400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3085</td>\n",
              "      <td>2.566000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3086</td>\n",
              "      <td>2.562000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3087</td>\n",
              "      <td>2.103600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3088</td>\n",
              "      <td>2.468800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3089</td>\n",
              "      <td>2.482800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3090</td>\n",
              "      <td>2.542400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3091</td>\n",
              "      <td>2.171500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3092</td>\n",
              "      <td>2.377800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3093</td>\n",
              "      <td>2.504300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3094</td>\n",
              "      <td>2.125300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3095</td>\n",
              "      <td>2.465400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3096</td>\n",
              "      <td>2.098900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3097</td>\n",
              "      <td>2.367800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3098</td>\n",
              "      <td>2.588400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3099</td>\n",
              "      <td>2.493700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3100</td>\n",
              "      <td>2.029800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3101</td>\n",
              "      <td>1.994700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3102</td>\n",
              "      <td>2.548800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3103</td>\n",
              "      <td>2.217000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3104</td>\n",
              "      <td>2.314600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3105</td>\n",
              "      <td>2.215500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3106</td>\n",
              "      <td>2.503500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3107</td>\n",
              "      <td>2.580500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3108</td>\n",
              "      <td>2.443600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3109</td>\n",
              "      <td>2.356700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3110</td>\n",
              "      <td>2.350000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3111</td>\n",
              "      <td>2.282100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3112</td>\n",
              "      <td>2.838500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3113</td>\n",
              "      <td>2.334600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3114</td>\n",
              "      <td>2.175000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3115</td>\n",
              "      <td>2.611100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3116</td>\n",
              "      <td>2.117300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3117</td>\n",
              "      <td>2.141700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3118</td>\n",
              "      <td>2.687200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3119</td>\n",
              "      <td>1.958400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3120</td>\n",
              "      <td>2.413900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3121</td>\n",
              "      <td>2.522100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3122</td>\n",
              "      <td>2.435800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3123</td>\n",
              "      <td>2.237100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3124</td>\n",
              "      <td>2.577900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3125</td>\n",
              "      <td>2.543300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3126</td>\n",
              "      <td>2.218600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3127</td>\n",
              "      <td>2.474900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3128</td>\n",
              "      <td>2.571100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3129</td>\n",
              "      <td>2.665600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3130</td>\n",
              "      <td>2.401800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3131</td>\n",
              "      <td>2.144500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3132</td>\n",
              "      <td>2.483800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3133</td>\n",
              "      <td>2.456000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3134</td>\n",
              "      <td>2.701900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3135</td>\n",
              "      <td>2.625800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3136</td>\n",
              "      <td>2.288800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3137</td>\n",
              "      <td>2.594300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3138</td>\n",
              "      <td>2.305800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3139</td>\n",
              "      <td>2.523600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3140</td>\n",
              "      <td>2.483000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3141</td>\n",
              "      <td>2.569700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3142</td>\n",
              "      <td>2.340600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3143</td>\n",
              "      <td>2.442700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3144</td>\n",
              "      <td>2.566200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3145</td>\n",
              "      <td>2.635200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3146</td>\n",
              "      <td>2.357400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3147</td>\n",
              "      <td>2.088100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3148</td>\n",
              "      <td>2.254700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3149</td>\n",
              "      <td>2.466000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3150</td>\n",
              "      <td>2.439600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3151</td>\n",
              "      <td>2.587800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3152</td>\n",
              "      <td>2.545800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3153</td>\n",
              "      <td>2.509900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3154</td>\n",
              "      <td>2.544200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3155</td>\n",
              "      <td>2.116400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3156</td>\n",
              "      <td>2.459900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3157</td>\n",
              "      <td>2.504600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3158</td>\n",
              "      <td>2.580300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3159</td>\n",
              "      <td>2.487000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3160</td>\n",
              "      <td>2.526000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3161</td>\n",
              "      <td>2.430300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3162</td>\n",
              "      <td>2.110300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3163</td>\n",
              "      <td>2.302800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3164</td>\n",
              "      <td>2.339400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3165</td>\n",
              "      <td>2.572400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3166</td>\n",
              "      <td>2.426000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3167</td>\n",
              "      <td>2.472200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3168</td>\n",
              "      <td>2.492000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3169</td>\n",
              "      <td>2.554800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3170</td>\n",
              "      <td>2.453800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3171</td>\n",
              "      <td>2.471000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3172</td>\n",
              "      <td>2.441100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3173</td>\n",
              "      <td>2.417400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3174</td>\n",
              "      <td>2.562600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3175</td>\n",
              "      <td>2.460700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3176</td>\n",
              "      <td>2.444300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3177</td>\n",
              "      <td>2.532000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3178</td>\n",
              "      <td>2.145000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3179</td>\n",
              "      <td>2.415900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3180</td>\n",
              "      <td>2.409700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3181</td>\n",
              "      <td>2.271000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3182</td>\n",
              "      <td>2.242900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3183</td>\n",
              "      <td>2.570400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3184</td>\n",
              "      <td>2.319800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3185</td>\n",
              "      <td>2.526500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3186</td>\n",
              "      <td>2.222600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3187</td>\n",
              "      <td>2.531700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3188</td>\n",
              "      <td>2.540500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3189</td>\n",
              "      <td>2.525200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3190</td>\n",
              "      <td>2.480600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3191</td>\n",
              "      <td>2.567000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3192</td>\n",
              "      <td>2.522100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3193</td>\n",
              "      <td>2.676300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3194</td>\n",
              "      <td>2.637600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3195</td>\n",
              "      <td>2.128300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3196</td>\n",
              "      <td>2.591500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3197</td>\n",
              "      <td>2.220300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3198</td>\n",
              "      <td>2.446000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3199</td>\n",
              "      <td>2.139400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3200</td>\n",
              "      <td>2.417100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3201</td>\n",
              "      <td>2.020000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3202</td>\n",
              "      <td>2.191200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3203</td>\n",
              "      <td>2.162300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3204</td>\n",
              "      <td>2.378400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3205</td>\n",
              "      <td>2.246100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3206</td>\n",
              "      <td>2.586000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3207</td>\n",
              "      <td>2.231800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3208</td>\n",
              "      <td>2.400300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3209</td>\n",
              "      <td>1.931400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3210</td>\n",
              "      <td>2.527300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3211</td>\n",
              "      <td>2.290100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3212</td>\n",
              "      <td>2.596100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3213</td>\n",
              "      <td>2.612600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3214</td>\n",
              "      <td>2.579700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3215</td>\n",
              "      <td>2.478800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3216</td>\n",
              "      <td>2.233800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3217</td>\n",
              "      <td>2.691900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3218</td>\n",
              "      <td>2.501000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3219</td>\n",
              "      <td>2.520400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3220</td>\n",
              "      <td>2.384600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3221</td>\n",
              "      <td>2.463000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3222</td>\n",
              "      <td>2.472100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3223</td>\n",
              "      <td>2.446000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3224</td>\n",
              "      <td>2.527900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3225</td>\n",
              "      <td>2.094400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3226</td>\n",
              "      <td>2.275700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3227</td>\n",
              "      <td>2.571400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3228</td>\n",
              "      <td>2.587400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3229</td>\n",
              "      <td>2.295900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3230</td>\n",
              "      <td>2.475200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3231</td>\n",
              "      <td>1.936600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3232</td>\n",
              "      <td>2.097900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3233</td>\n",
              "      <td>2.618400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3234</td>\n",
              "      <td>2.104300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3235</td>\n",
              "      <td>2.526900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3236</td>\n",
              "      <td>2.209300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3237</td>\n",
              "      <td>2.136500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3238</td>\n",
              "      <td>2.513100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3239</td>\n",
              "      <td>2.729800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3240</td>\n",
              "      <td>2.408100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3241</td>\n",
              "      <td>2.534300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3242</td>\n",
              "      <td>2.659200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3243</td>\n",
              "      <td>2.458100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3244</td>\n",
              "      <td>2.324600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3245</td>\n",
              "      <td>2.155700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3246</td>\n",
              "      <td>2.148800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3247</td>\n",
              "      <td>2.555100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3248</td>\n",
              "      <td>2.549600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3249</td>\n",
              "      <td>1.841100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3250</td>\n",
              "      <td>2.701900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3251</td>\n",
              "      <td>2.347000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3252</td>\n",
              "      <td>2.818300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3253</td>\n",
              "      <td>2.120400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3254</td>\n",
              "      <td>2.435600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3255</td>\n",
              "      <td>2.512700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3256</td>\n",
              "      <td>2.556500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3257</td>\n",
              "      <td>2.549600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3258</td>\n",
              "      <td>2.441000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3259</td>\n",
              "      <td>2.760800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3260</td>\n",
              "      <td>2.412200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3261</td>\n",
              "      <td>2.518100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3262</td>\n",
              "      <td>2.184600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3263</td>\n",
              "      <td>2.521600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3264</td>\n",
              "      <td>2.484200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3265</td>\n",
              "      <td>2.506400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3266</td>\n",
              "      <td>2.488600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3267</td>\n",
              "      <td>2.199900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3268</td>\n",
              "      <td>2.482500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3269</td>\n",
              "      <td>2.308800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3270</td>\n",
              "      <td>2.114500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3271</td>\n",
              "      <td>2.384800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3272</td>\n",
              "      <td>2.416300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3273</td>\n",
              "      <td>2.622000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3274</td>\n",
              "      <td>2.201800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3275</td>\n",
              "      <td>2.596400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3276</td>\n",
              "      <td>2.085800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3277</td>\n",
              "      <td>2.547000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3278</td>\n",
              "      <td>2.197500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3279</td>\n",
              "      <td>2.120400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3280</td>\n",
              "      <td>2.528000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3281</td>\n",
              "      <td>2.524600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3282</td>\n",
              "      <td>2.409900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3283</td>\n",
              "      <td>2.459000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3284</td>\n",
              "      <td>2.192500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3285</td>\n",
              "      <td>2.518700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3286</td>\n",
              "      <td>2.068300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3287</td>\n",
              "      <td>2.650500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3288</td>\n",
              "      <td>2.590500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3289</td>\n",
              "      <td>2.440600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3290</td>\n",
              "      <td>2.315100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3291</td>\n",
              "      <td>2.034500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3292</td>\n",
              "      <td>2.437200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3293</td>\n",
              "      <td>2.518000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3294</td>\n",
              "      <td>2.294700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3295</td>\n",
              "      <td>2.445600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3296</td>\n",
              "      <td>2.502700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3297</td>\n",
              "      <td>2.312100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3298</td>\n",
              "      <td>2.154300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3299</td>\n",
              "      <td>2.154500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3300</td>\n",
              "      <td>2.592500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3301</td>\n",
              "      <td>2.187100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3302</td>\n",
              "      <td>2.610500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3303</td>\n",
              "      <td>2.520200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3304</td>\n",
              "      <td>1.914500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3305</td>\n",
              "      <td>2.408800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3306</td>\n",
              "      <td>2.518500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3307</td>\n",
              "      <td>2.571200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3308</td>\n",
              "      <td>2.420000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3309</td>\n",
              "      <td>2.516100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3310</td>\n",
              "      <td>2.512600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3311</td>\n",
              "      <td>2.389300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3312</td>\n",
              "      <td>2.480600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3313</td>\n",
              "      <td>2.517600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3314</td>\n",
              "      <td>2.406800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3315</td>\n",
              "      <td>2.256700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3316</td>\n",
              "      <td>2.114500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3317</td>\n",
              "      <td>2.337100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3318</td>\n",
              "      <td>2.142600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3319</td>\n",
              "      <td>2.248200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3320</td>\n",
              "      <td>2.509400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3321</td>\n",
              "      <td>2.661300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3322</td>\n",
              "      <td>2.412100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3323</td>\n",
              "      <td>2.308000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3324</td>\n",
              "      <td>2.174300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3325</td>\n",
              "      <td>2.575300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3326</td>\n",
              "      <td>2.356500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3327</td>\n",
              "      <td>2.091600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3328</td>\n",
              "      <td>2.351000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3329</td>\n",
              "      <td>2.395800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3330</td>\n",
              "      <td>2.219800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3331</td>\n",
              "      <td>2.479300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3332</td>\n",
              "      <td>2.295600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3333</td>\n",
              "      <td>2.218100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3334</td>\n",
              "      <td>2.408600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3335</td>\n",
              "      <td>2.138700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3336</td>\n",
              "      <td>2.028600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3337</td>\n",
              "      <td>2.466600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3338</td>\n",
              "      <td>2.395400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3339</td>\n",
              "      <td>2.630300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3340</td>\n",
              "      <td>2.696900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3341</td>\n",
              "      <td>2.325100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3342</td>\n",
              "      <td>2.486600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3343</td>\n",
              "      <td>1.912900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3344</td>\n",
              "      <td>2.199200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3345</td>\n",
              "      <td>2.127500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3346</td>\n",
              "      <td>2.423800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3347</td>\n",
              "      <td>2.458800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3348</td>\n",
              "      <td>2.088800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3349</td>\n",
              "      <td>2.254400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3350</td>\n",
              "      <td>2.192800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3351</td>\n",
              "      <td>2.424900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3352</td>\n",
              "      <td>2.267300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3353</td>\n",
              "      <td>2.428600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3354</td>\n",
              "      <td>2.511500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3355</td>\n",
              "      <td>2.492400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3356</td>\n",
              "      <td>2.461700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3357</td>\n",
              "      <td>2.398100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3358</td>\n",
              "      <td>2.209400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3359</td>\n",
              "      <td>2.205200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3360</td>\n",
              "      <td>2.591300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3361</td>\n",
              "      <td>2.053300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3362</td>\n",
              "      <td>2.443900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3363</td>\n",
              "      <td>2.475300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3364</td>\n",
              "      <td>2.139200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3365</td>\n",
              "      <td>2.126300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3366</td>\n",
              "      <td>2.027800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3367</td>\n",
              "      <td>2.157300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3368</td>\n",
              "      <td>2.129800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3369</td>\n",
              "      <td>2.582100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3370</td>\n",
              "      <td>2.402500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3371</td>\n",
              "      <td>2.436700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3372</td>\n",
              "      <td>2.477400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3373</td>\n",
              "      <td>2.406300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3374</td>\n",
              "      <td>2.289100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3375</td>\n",
              "      <td>2.195600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3376</td>\n",
              "      <td>2.443900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3377</td>\n",
              "      <td>2.362600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3378</td>\n",
              "      <td>2.645000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3379</td>\n",
              "      <td>2.470300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3380</td>\n",
              "      <td>2.520800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3381</td>\n",
              "      <td>2.243100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3382</td>\n",
              "      <td>2.368100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3383</td>\n",
              "      <td>2.523900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3384</td>\n",
              "      <td>2.347800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3385</td>\n",
              "      <td>2.479600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3386</td>\n",
              "      <td>2.095800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3387</td>\n",
              "      <td>2.445100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3388</td>\n",
              "      <td>2.365100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3389</td>\n",
              "      <td>2.502600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3390</td>\n",
              "      <td>2.509500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3391</td>\n",
              "      <td>2.066400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3392</td>\n",
              "      <td>2.512500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3393</td>\n",
              "      <td>2.206100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3394</td>\n",
              "      <td>1.941900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3395</td>\n",
              "      <td>2.030800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3396</td>\n",
              "      <td>2.601800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3397</td>\n",
              "      <td>2.549300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3398</td>\n",
              "      <td>2.171900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3399</td>\n",
              "      <td>2.382400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3400</td>\n",
              "      <td>2.562700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3401</td>\n",
              "      <td>2.100300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3402</td>\n",
              "      <td>2.276200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3403</td>\n",
              "      <td>2.381200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3404</td>\n",
              "      <td>2.305200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3405</td>\n",
              "      <td>2.380400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3406</td>\n",
              "      <td>2.509600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3407</td>\n",
              "      <td>1.876200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3408</td>\n",
              "      <td>2.543800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3409</td>\n",
              "      <td>2.436700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3410</td>\n",
              "      <td>2.102900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3411</td>\n",
              "      <td>2.554000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3412</td>\n",
              "      <td>2.141500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3413</td>\n",
              "      <td>2.179600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3414</td>\n",
              "      <td>2.638600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3415</td>\n",
              "      <td>2.355700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3416</td>\n",
              "      <td>1.933100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3417</td>\n",
              "      <td>2.646100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3418</td>\n",
              "      <td>2.386700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3419</td>\n",
              "      <td>2.540200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3420</td>\n",
              "      <td>2.555800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3421</td>\n",
              "      <td>2.067300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3422</td>\n",
              "      <td>2.522600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3423</td>\n",
              "      <td>2.118300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3424</td>\n",
              "      <td>2.107300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3425</td>\n",
              "      <td>2.583700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3426</td>\n",
              "      <td>2.126300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3427</td>\n",
              "      <td>2.445300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3428</td>\n",
              "      <td>2.419900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3429</td>\n",
              "      <td>2.574300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3430</td>\n",
              "      <td>2.153700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3431</td>\n",
              "      <td>2.374800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3432</td>\n",
              "      <td>2.060700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3433</td>\n",
              "      <td>2.578400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3434</td>\n",
              "      <td>2.561700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3435</td>\n",
              "      <td>2.528000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3436</td>\n",
              "      <td>2.201300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3437</td>\n",
              "      <td>2.228500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3438</td>\n",
              "      <td>2.565700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3439</td>\n",
              "      <td>1.854300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3440</td>\n",
              "      <td>2.471300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3441</td>\n",
              "      <td>2.510600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3442</td>\n",
              "      <td>2.197100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3443</td>\n",
              "      <td>2.399100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3444</td>\n",
              "      <td>2.402000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3445</td>\n",
              "      <td>2.523300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3446</td>\n",
              "      <td>2.328200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3447</td>\n",
              "      <td>2.579400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3448</td>\n",
              "      <td>2.366700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3449</td>\n",
              "      <td>2.377100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3450</td>\n",
              "      <td>2.472000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3451</td>\n",
              "      <td>2.505700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3452</td>\n",
              "      <td>2.394300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3453</td>\n",
              "      <td>2.261900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3454</td>\n",
              "      <td>2.609500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3455</td>\n",
              "      <td>2.431500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3456</td>\n",
              "      <td>2.458400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3457</td>\n",
              "      <td>2.444100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3458</td>\n",
              "      <td>2.551000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3459</td>\n",
              "      <td>2.450300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3460</td>\n",
              "      <td>2.443700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3461</td>\n",
              "      <td>2.476700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3462</td>\n",
              "      <td>2.589200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3463</td>\n",
              "      <td>2.139500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3464</td>\n",
              "      <td>2.051000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3465</td>\n",
              "      <td>2.500300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3466</td>\n",
              "      <td>2.171400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3467</td>\n",
              "      <td>2.683800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3468</td>\n",
              "      <td>2.394900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3469</td>\n",
              "      <td>2.438200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3470</td>\n",
              "      <td>2.336600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3471</td>\n",
              "      <td>2.424900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3472</td>\n",
              "      <td>2.278900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3473</td>\n",
              "      <td>2.322300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3474</td>\n",
              "      <td>2.046500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3475</td>\n",
              "      <td>2.506300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3476</td>\n",
              "      <td>2.125300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3477</td>\n",
              "      <td>2.074200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3478</td>\n",
              "      <td>2.590000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3479</td>\n",
              "      <td>2.541500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3480</td>\n",
              "      <td>2.466700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3481</td>\n",
              "      <td>2.125300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3482</td>\n",
              "      <td>2.468800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3483</td>\n",
              "      <td>1.991400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3484</td>\n",
              "      <td>2.080900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3485</td>\n",
              "      <td>2.530900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3486</td>\n",
              "      <td>2.120400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3487</td>\n",
              "      <td>2.486500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3488</td>\n",
              "      <td>2.240300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3489</td>\n",
              "      <td>1.799200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3490</td>\n",
              "      <td>2.531900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3491</td>\n",
              "      <td>2.349700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3492</td>\n",
              "      <td>2.445600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3493</td>\n",
              "      <td>1.946100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3494</td>\n",
              "      <td>2.190400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3495</td>\n",
              "      <td>2.004600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3496</td>\n",
              "      <td>1.748400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3497</td>\n",
              "      <td>2.394000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3498</td>\n",
              "      <td>2.514700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3499</td>\n",
              "      <td>2.025500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3500</td>\n",
              "      <td>2.267500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3501</td>\n",
              "      <td>1.737300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3502</td>\n",
              "      <td>2.309900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3503</td>\n",
              "      <td>2.353000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3504</td>\n",
              "      <td>1.772200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3505</td>\n",
              "      <td>2.037000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3506</td>\n",
              "      <td>2.392300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3507</td>\n",
              "      <td>2.503100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3508</td>\n",
              "      <td>2.503400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3509</td>\n",
              "      <td>2.043700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3510</td>\n",
              "      <td>2.493300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3511</td>\n",
              "      <td>2.465200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3512</td>\n",
              "      <td>2.583900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3513</td>\n",
              "      <td>2.472400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3514</td>\n",
              "      <td>1.903300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3515</td>\n",
              "      <td>2.306600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3516</td>\n",
              "      <td>2.323800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3517</td>\n",
              "      <td>2.108900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3518</td>\n",
              "      <td>2.469800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3519</td>\n",
              "      <td>2.139300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3520</td>\n",
              "      <td>2.331700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3521</td>\n",
              "      <td>2.465100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3522</td>\n",
              "      <td>2.306100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3523</td>\n",
              "      <td>2.526500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3524</td>\n",
              "      <td>2.126600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3525</td>\n",
              "      <td>2.576900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3526</td>\n",
              "      <td>2.549900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3527</td>\n",
              "      <td>2.162800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3528</td>\n",
              "      <td>1.992300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3529</td>\n",
              "      <td>2.637200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3530</td>\n",
              "      <td>2.263800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3531</td>\n",
              "      <td>2.543400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3532</td>\n",
              "      <td>2.455900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3533</td>\n",
              "      <td>2.631600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3534</td>\n",
              "      <td>2.282700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3535</td>\n",
              "      <td>2.371200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3536</td>\n",
              "      <td>2.462700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3537</td>\n",
              "      <td>2.471200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3538</td>\n",
              "      <td>2.298100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3539</td>\n",
              "      <td>2.517600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3540</td>\n",
              "      <td>2.587600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3541</td>\n",
              "      <td>2.504700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3542</td>\n",
              "      <td>2.224800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3543</td>\n",
              "      <td>2.692600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3544</td>\n",
              "      <td>2.543300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3545</td>\n",
              "      <td>2.170900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3546</td>\n",
              "      <td>2.093600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3547</td>\n",
              "      <td>2.508900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3548</td>\n",
              "      <td>2.169100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3549</td>\n",
              "      <td>2.156200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3550</td>\n",
              "      <td>2.259800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3551</td>\n",
              "      <td>2.386100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3552</td>\n",
              "      <td>2.301600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3553</td>\n",
              "      <td>2.486100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3554</td>\n",
              "      <td>2.152500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3555</td>\n",
              "      <td>2.147100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3556</td>\n",
              "      <td>2.553700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3557</td>\n",
              "      <td>2.590800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3558</td>\n",
              "      <td>2.558500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3559</td>\n",
              "      <td>1.964900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3560</td>\n",
              "      <td>2.448700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3561</td>\n",
              "      <td>2.689700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3562</td>\n",
              "      <td>2.350200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3563</td>\n",
              "      <td>2.500100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3564</td>\n",
              "      <td>1.674100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3565</td>\n",
              "      <td>2.316400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3566</td>\n",
              "      <td>2.502300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3567</td>\n",
              "      <td>2.359000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3568</td>\n",
              "      <td>2.065900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3569</td>\n",
              "      <td>2.627000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3570</td>\n",
              "      <td>2.557700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3571</td>\n",
              "      <td>2.508700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3572</td>\n",
              "      <td>2.095400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3573</td>\n",
              "      <td>2.556500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3574</td>\n",
              "      <td>2.477500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3575</td>\n",
              "      <td>2.572500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3576</td>\n",
              "      <td>2.336400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3577</td>\n",
              "      <td>2.265600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3578</td>\n",
              "      <td>2.302300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3579</td>\n",
              "      <td>2.600000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3580</td>\n",
              "      <td>2.386000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3581</td>\n",
              "      <td>2.428700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3582</td>\n",
              "      <td>1.998100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3583</td>\n",
              "      <td>2.362700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3584</td>\n",
              "      <td>2.609900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3585</td>\n",
              "      <td>2.353900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3586</td>\n",
              "      <td>1.930600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3587</td>\n",
              "      <td>2.210700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3588</td>\n",
              "      <td>2.497500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3589</td>\n",
              "      <td>2.437100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3590</td>\n",
              "      <td>2.589800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3591</td>\n",
              "      <td>2.722300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3592</td>\n",
              "      <td>2.062100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3593</td>\n",
              "      <td>1.973100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3594</td>\n",
              "      <td>2.169300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3595</td>\n",
              "      <td>2.486100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3596</td>\n",
              "      <td>2.318900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3597</td>\n",
              "      <td>1.885600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3598</td>\n",
              "      <td>2.406000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3599</td>\n",
              "      <td>2.389900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3600</td>\n",
              "      <td>2.179900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3601</td>\n",
              "      <td>2.519700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3602</td>\n",
              "      <td>2.464800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3603</td>\n",
              "      <td>2.420400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3604</td>\n",
              "      <td>2.411500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3605</td>\n",
              "      <td>2.348200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3606</td>\n",
              "      <td>2.035300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3607</td>\n",
              "      <td>2.517900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3608</td>\n",
              "      <td>2.461900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3609</td>\n",
              "      <td>2.042300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3610</td>\n",
              "      <td>2.355000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3611</td>\n",
              "      <td>2.096700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3612</td>\n",
              "      <td>2.031800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3613</td>\n",
              "      <td>2.339400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3614</td>\n",
              "      <td>2.193200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3615</td>\n",
              "      <td>2.512900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3616</td>\n",
              "      <td>2.235600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3617</td>\n",
              "      <td>2.052900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3618</td>\n",
              "      <td>2.536100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3619</td>\n",
              "      <td>2.006300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3620</td>\n",
              "      <td>1.958800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3621</td>\n",
              "      <td>2.460000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3622</td>\n",
              "      <td>2.403400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3623</td>\n",
              "      <td>2.546600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3624</td>\n",
              "      <td>1.896300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3625</td>\n",
              "      <td>2.319200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3626</td>\n",
              "      <td>2.463100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3627</td>\n",
              "      <td>2.317100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3628</td>\n",
              "      <td>1.932400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3629</td>\n",
              "      <td>2.082900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3630</td>\n",
              "      <td>2.469600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3631</td>\n",
              "      <td>2.499400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3632</td>\n",
              "      <td>2.522300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3633</td>\n",
              "      <td>2.527900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3634</td>\n",
              "      <td>2.522100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3635</td>\n",
              "      <td>2.453500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3636</td>\n",
              "      <td>2.509400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3637</td>\n",
              "      <td>2.438300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3638</td>\n",
              "      <td>2.538300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3639</td>\n",
              "      <td>2.060800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3640</td>\n",
              "      <td>2.378800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3641</td>\n",
              "      <td>2.026900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3642</td>\n",
              "      <td>2.278500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3643</td>\n",
              "      <td>2.431600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3644</td>\n",
              "      <td>2.533000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3645</td>\n",
              "      <td>2.040400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3646</td>\n",
              "      <td>2.395200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3647</td>\n",
              "      <td>2.484700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3648</td>\n",
              "      <td>1.980100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3649</td>\n",
              "      <td>2.302600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3650</td>\n",
              "      <td>2.625800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3651</td>\n",
              "      <td>2.366600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3652</td>\n",
              "      <td>2.412200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3653</td>\n",
              "      <td>2.458200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3654</td>\n",
              "      <td>2.559900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3655</td>\n",
              "      <td>2.470700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3656</td>\n",
              "      <td>2.508000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3657</td>\n",
              "      <td>2.358400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3658</td>\n",
              "      <td>2.515000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3659</td>\n",
              "      <td>2.107800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3660</td>\n",
              "      <td>2.062800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3661</td>\n",
              "      <td>2.420500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3662</td>\n",
              "      <td>2.358500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3663</td>\n",
              "      <td>2.278400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3664</td>\n",
              "      <td>2.498500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3665</td>\n",
              "      <td>2.216300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3666</td>\n",
              "      <td>2.426700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3667</td>\n",
              "      <td>2.374400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3668</td>\n",
              "      <td>2.501300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3669</td>\n",
              "      <td>2.158200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3670</td>\n",
              "      <td>2.565100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3671</td>\n",
              "      <td>2.508100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3672</td>\n",
              "      <td>2.082200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3673</td>\n",
              "      <td>2.587900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3674</td>\n",
              "      <td>2.403200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3675</td>\n",
              "      <td>2.235400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3676</td>\n",
              "      <td>2.328800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3677</td>\n",
              "      <td>1.946100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3678</td>\n",
              "      <td>1.602200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3679</td>\n",
              "      <td>2.084200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3680</td>\n",
              "      <td>2.003800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3681</td>\n",
              "      <td>2.214100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3682</td>\n",
              "      <td>2.377100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3683</td>\n",
              "      <td>2.391400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3684</td>\n",
              "      <td>2.425500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3685</td>\n",
              "      <td>2.297500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3686</td>\n",
              "      <td>2.289800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3687</td>\n",
              "      <td>2.273700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3688</td>\n",
              "      <td>2.091800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3689</td>\n",
              "      <td>2.464900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3690</td>\n",
              "      <td>2.076600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3691</td>\n",
              "      <td>1.912500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3692</td>\n",
              "      <td>2.657500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3693</td>\n",
              "      <td>2.579000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3694</td>\n",
              "      <td>2.445200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3695</td>\n",
              "      <td>2.391600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3696</td>\n",
              "      <td>2.249700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3697</td>\n",
              "      <td>2.443600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3698</td>\n",
              "      <td>2.470000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3699</td>\n",
              "      <td>2.533700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3700</td>\n",
              "      <td>2.484300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3701</td>\n",
              "      <td>2.441100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3702</td>\n",
              "      <td>2.266800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3703</td>\n",
              "      <td>2.462000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3704</td>\n",
              "      <td>2.201500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3705</td>\n",
              "      <td>2.268800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3706</td>\n",
              "      <td>2.052300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3707</td>\n",
              "      <td>2.238800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3708</td>\n",
              "      <td>2.126400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3709</td>\n",
              "      <td>2.002700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3710</td>\n",
              "      <td>1.968700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3711</td>\n",
              "      <td>2.235400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3712</td>\n",
              "      <td>2.255600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3713</td>\n",
              "      <td>2.432300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3714</td>\n",
              "      <td>2.535200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3715</td>\n",
              "      <td>2.479600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3716</td>\n",
              "      <td>1.902700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3717</td>\n",
              "      <td>2.526400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3718</td>\n",
              "      <td>2.398600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3719</td>\n",
              "      <td>2.209300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3720</td>\n",
              "      <td>2.331700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3721</td>\n",
              "      <td>2.007500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3722</td>\n",
              "      <td>1.865000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3723</td>\n",
              "      <td>2.245100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3724</td>\n",
              "      <td>2.427000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3725</td>\n",
              "      <td>2.401800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3726</td>\n",
              "      <td>2.463700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3727</td>\n",
              "      <td>2.535500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3728</td>\n",
              "      <td>2.289800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3729</td>\n",
              "      <td>2.495000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3730</td>\n",
              "      <td>2.507800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3731</td>\n",
              "      <td>2.469400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3732</td>\n",
              "      <td>2.516400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3733</td>\n",
              "      <td>1.984200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3734</td>\n",
              "      <td>2.537600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3735</td>\n",
              "      <td>2.021900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3736</td>\n",
              "      <td>2.390500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3737</td>\n",
              "      <td>1.737400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3738</td>\n",
              "      <td>2.435400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3739</td>\n",
              "      <td>2.035200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3740</td>\n",
              "      <td>2.335100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3741</td>\n",
              "      <td>2.508900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3742</td>\n",
              "      <td>2.394300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3743</td>\n",
              "      <td>2.220700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3744</td>\n",
              "      <td>2.361500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3745</td>\n",
              "      <td>2.532600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3746</td>\n",
              "      <td>2.513900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3747</td>\n",
              "      <td>2.521200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3748</td>\n",
              "      <td>2.378600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3749</td>\n",
              "      <td>2.255800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3750</td>\n",
              "      <td>2.386100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3751</td>\n",
              "      <td>2.406300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3752</td>\n",
              "      <td>2.254000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3753</td>\n",
              "      <td>2.311800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3754</td>\n",
              "      <td>2.565100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3755</td>\n",
              "      <td>1.930200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3756</td>\n",
              "      <td>2.374600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3757</td>\n",
              "      <td>2.296800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3758</td>\n",
              "      <td>2.230600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3759</td>\n",
              "      <td>2.428600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3760</td>\n",
              "      <td>2.119800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3761</td>\n",
              "      <td>2.182800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3762</td>\n",
              "      <td>2.276400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3763</td>\n",
              "      <td>2.454800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3764</td>\n",
              "      <td>2.539800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3765</td>\n",
              "      <td>2.510200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3766</td>\n",
              "      <td>1.984000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3767</td>\n",
              "      <td>2.498200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3768</td>\n",
              "      <td>2.553900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3769</td>\n",
              "      <td>2.223400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3770</td>\n",
              "      <td>2.251000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3771</td>\n",
              "      <td>2.288200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3772</td>\n",
              "      <td>2.074000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3773</td>\n",
              "      <td>2.142100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3774</td>\n",
              "      <td>2.063700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3775</td>\n",
              "      <td>2.409300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3776</td>\n",
              "      <td>2.319500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3777</td>\n",
              "      <td>2.238200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3778</td>\n",
              "      <td>2.536500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3779</td>\n",
              "      <td>2.049400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3780</td>\n",
              "      <td>2.412500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3781</td>\n",
              "      <td>1.950000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3782</td>\n",
              "      <td>2.111900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3783</td>\n",
              "      <td>1.981700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3784</td>\n",
              "      <td>2.495200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3785</td>\n",
              "      <td>2.297800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3786</td>\n",
              "      <td>2.389000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3787</td>\n",
              "      <td>2.333600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3788</td>\n",
              "      <td>2.602200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3789</td>\n",
              "      <td>2.463300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3790</td>\n",
              "      <td>1.589500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3791</td>\n",
              "      <td>1.777500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3792</td>\n",
              "      <td>2.418600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3793</td>\n",
              "      <td>2.515200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3794</td>\n",
              "      <td>2.652600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3795</td>\n",
              "      <td>2.479400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3796</td>\n",
              "      <td>2.171700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3797</td>\n",
              "      <td>2.347000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3798</td>\n",
              "      <td>2.458700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3799</td>\n",
              "      <td>2.373700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3800</td>\n",
              "      <td>2.430800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3801</td>\n",
              "      <td>2.410100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3802</td>\n",
              "      <td>2.353500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3803</td>\n",
              "      <td>2.101800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3804</td>\n",
              "      <td>2.223600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3805</td>\n",
              "      <td>1.926300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3806</td>\n",
              "      <td>1.968400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3807</td>\n",
              "      <td>2.144600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3808</td>\n",
              "      <td>2.316000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3809</td>\n",
              "      <td>2.436500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3810</td>\n",
              "      <td>2.287100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3811</td>\n",
              "      <td>2.541600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3812</td>\n",
              "      <td>2.062700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3813</td>\n",
              "      <td>2.093500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3814</td>\n",
              "      <td>2.380500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3815</td>\n",
              "      <td>2.380500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3816</td>\n",
              "      <td>2.600800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3817</td>\n",
              "      <td>2.260700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3818</td>\n",
              "      <td>2.485200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3819</td>\n",
              "      <td>2.740600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3820</td>\n",
              "      <td>2.400700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3821</td>\n",
              "      <td>2.005100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3822</td>\n",
              "      <td>2.520500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3823</td>\n",
              "      <td>2.566400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3824</td>\n",
              "      <td>2.099000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3825</td>\n",
              "      <td>2.346100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3826</td>\n",
              "      <td>2.018300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3827</td>\n",
              "      <td>2.505900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3828</td>\n",
              "      <td>2.088800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3829</td>\n",
              "      <td>2.093700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3830</td>\n",
              "      <td>2.412500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3831</td>\n",
              "      <td>2.432600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3832</td>\n",
              "      <td>2.426300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3833</td>\n",
              "      <td>2.350600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3834</td>\n",
              "      <td>2.143900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3835</td>\n",
              "      <td>2.482200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3836</td>\n",
              "      <td>2.399300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3837</td>\n",
              "      <td>2.102100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3838</td>\n",
              "      <td>1.964800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3839</td>\n",
              "      <td>2.339500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3840</td>\n",
              "      <td>2.222300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3841</td>\n",
              "      <td>2.083800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3842</td>\n",
              "      <td>1.909000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3843</td>\n",
              "      <td>2.646500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3844</td>\n",
              "      <td>2.405200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3845</td>\n",
              "      <td>2.341900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3846</td>\n",
              "      <td>2.526600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3847</td>\n",
              "      <td>2.397500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3848</td>\n",
              "      <td>2.579700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3849</td>\n",
              "      <td>2.379300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3850</td>\n",
              "      <td>2.363700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3851</td>\n",
              "      <td>2.530000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3852</td>\n",
              "      <td>2.434800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3853</td>\n",
              "      <td>2.424000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3854</td>\n",
              "      <td>2.335400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3855</td>\n",
              "      <td>2.338700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3856</td>\n",
              "      <td>2.441800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3857</td>\n",
              "      <td>2.283200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3858</td>\n",
              "      <td>2.410000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3859</td>\n",
              "      <td>2.304300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3860</td>\n",
              "      <td>2.293400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3861</td>\n",
              "      <td>2.029800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3862</td>\n",
              "      <td>2.338100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3863</td>\n",
              "      <td>2.091000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3864</td>\n",
              "      <td>2.092400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3865</td>\n",
              "      <td>2.316600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3866</td>\n",
              "      <td>2.462600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3867</td>\n",
              "      <td>2.100500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3868</td>\n",
              "      <td>2.309000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3869</td>\n",
              "      <td>2.395200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3870</td>\n",
              "      <td>2.580000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3871</td>\n",
              "      <td>2.385600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3872</td>\n",
              "      <td>1.833400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3873</td>\n",
              "      <td>2.515400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3874</td>\n",
              "      <td>2.429100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3875</td>\n",
              "      <td>2.556200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3876</td>\n",
              "      <td>2.431900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3877</td>\n",
              "      <td>2.382000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3878</td>\n",
              "      <td>2.410600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3879</td>\n",
              "      <td>2.314000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3880</td>\n",
              "      <td>2.460700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3881</td>\n",
              "      <td>1.659200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3882</td>\n",
              "      <td>2.401600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3883</td>\n",
              "      <td>2.308800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3884</td>\n",
              "      <td>2.281000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3885</td>\n",
              "      <td>2.330700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3886</td>\n",
              "      <td>1.795800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3887</td>\n",
              "      <td>2.278300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3888</td>\n",
              "      <td>1.973900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3889</td>\n",
              "      <td>2.186400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3890</td>\n",
              "      <td>2.653900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3891</td>\n",
              "      <td>1.704600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3892</td>\n",
              "      <td>2.426700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3893</td>\n",
              "      <td>2.424700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3894</td>\n",
              "      <td>1.542500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3895</td>\n",
              "      <td>2.500300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3896</td>\n",
              "      <td>1.801700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3897</td>\n",
              "      <td>2.491700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3898</td>\n",
              "      <td>2.603900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3899</td>\n",
              "      <td>1.531400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3900</td>\n",
              "      <td>2.272800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3901</td>\n",
              "      <td>2.419400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3902</td>\n",
              "      <td>2.105400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3903</td>\n",
              "      <td>2.520300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3904</td>\n",
              "      <td>2.179400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3905</td>\n",
              "      <td>2.182300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3906</td>\n",
              "      <td>2.502200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3907</td>\n",
              "      <td>2.297000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3908</td>\n",
              "      <td>2.138200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3909</td>\n",
              "      <td>2.483800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3910</td>\n",
              "      <td>2.390500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3911</td>\n",
              "      <td>2.569800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3912</td>\n",
              "      <td>2.550300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3913</td>\n",
              "      <td>2.263000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3914</td>\n",
              "      <td>2.569400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3915</td>\n",
              "      <td>2.301500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3916</td>\n",
              "      <td>2.322900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3917</td>\n",
              "      <td>2.036400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3918</td>\n",
              "      <td>2.312700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3919</td>\n",
              "      <td>2.505600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3920</td>\n",
              "      <td>2.492700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3921</td>\n",
              "      <td>2.522800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3922</td>\n",
              "      <td>2.467100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3923</td>\n",
              "      <td>2.547400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3924</td>\n",
              "      <td>2.361200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3925</td>\n",
              "      <td>2.443000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3926</td>\n",
              "      <td>2.587700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3927</td>\n",
              "      <td>2.506700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3928</td>\n",
              "      <td>1.608100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3929</td>\n",
              "      <td>1.580200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3930</td>\n",
              "      <td>2.351000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3931</td>\n",
              "      <td>2.460900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3932</td>\n",
              "      <td>2.294900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3933</td>\n",
              "      <td>2.159200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3934</td>\n",
              "      <td>2.396400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3935</td>\n",
              "      <td>2.550000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3936</td>\n",
              "      <td>2.514300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3937</td>\n",
              "      <td>2.655300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3938</td>\n",
              "      <td>2.074900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3939</td>\n",
              "      <td>2.452800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3940</td>\n",
              "      <td>2.360100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3941</td>\n",
              "      <td>2.651100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3942</td>\n",
              "      <td>1.884200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3943</td>\n",
              "      <td>2.023500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3944</td>\n",
              "      <td>2.340000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3945</td>\n",
              "      <td>2.411500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3946</td>\n",
              "      <td>2.454000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3947</td>\n",
              "      <td>2.215200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3948</td>\n",
              "      <td>2.312800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3949</td>\n",
              "      <td>2.286400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3950</td>\n",
              "      <td>2.300800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3951</td>\n",
              "      <td>1.836200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3952</td>\n",
              "      <td>2.529300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3953</td>\n",
              "      <td>2.369400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3954</td>\n",
              "      <td>2.394800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3955</td>\n",
              "      <td>2.397700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3956</td>\n",
              "      <td>2.477100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3957</td>\n",
              "      <td>2.473700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3958</td>\n",
              "      <td>2.076600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3959</td>\n",
              "      <td>2.014700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3960</td>\n",
              "      <td>2.607900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3961</td>\n",
              "      <td>2.349200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3962</td>\n",
              "      <td>1.975100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3963</td>\n",
              "      <td>2.122000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3964</td>\n",
              "      <td>1.973900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3965</td>\n",
              "      <td>2.439300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3966</td>\n",
              "      <td>2.561800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3967</td>\n",
              "      <td>2.439600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3968</td>\n",
              "      <td>2.468600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3969</td>\n",
              "      <td>2.373600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3970</td>\n",
              "      <td>2.680200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3971</td>\n",
              "      <td>2.419100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3972</td>\n",
              "      <td>2.362400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3973</td>\n",
              "      <td>2.241200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3974</td>\n",
              "      <td>2.553900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3975</td>\n",
              "      <td>1.635100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3976</td>\n",
              "      <td>2.259500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3977</td>\n",
              "      <td>2.306600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3978</td>\n",
              "      <td>2.137700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3979</td>\n",
              "      <td>2.434600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3980</td>\n",
              "      <td>2.457800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3981</td>\n",
              "      <td>2.431400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3982</td>\n",
              "      <td>2.517400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3983</td>\n",
              "      <td>2.189000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3984</td>\n",
              "      <td>2.220300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3985</td>\n",
              "      <td>2.218600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3986</td>\n",
              "      <td>2.310100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3987</td>\n",
              "      <td>2.304400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3988</td>\n",
              "      <td>2.532100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3989</td>\n",
              "      <td>2.237400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3990</td>\n",
              "      <td>2.381800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3991</td>\n",
              "      <td>2.221000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3992</td>\n",
              "      <td>2.595900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3993</td>\n",
              "      <td>2.570900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3994</td>\n",
              "      <td>1.791000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3995</td>\n",
              "      <td>2.509100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3996</td>\n",
              "      <td>1.952400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3997</td>\n",
              "      <td>2.516000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3998</td>\n",
              "      <td>2.549300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3999</td>\n",
              "      <td>2.532500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4000</td>\n",
              "      <td>2.148700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4001</td>\n",
              "      <td>2.255600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4002</td>\n",
              "      <td>2.277300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4003</td>\n",
              "      <td>2.617800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4004</td>\n",
              "      <td>2.381100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4005</td>\n",
              "      <td>2.425300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4006</td>\n",
              "      <td>2.558900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4007</td>\n",
              "      <td>2.382000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4008</td>\n",
              "      <td>1.981200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4009</td>\n",
              "      <td>2.377000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4010</td>\n",
              "      <td>2.008500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4011</td>\n",
              "      <td>1.951700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4012</td>\n",
              "      <td>1.904700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4013</td>\n",
              "      <td>2.383800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4014</td>\n",
              "      <td>2.396100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4015</td>\n",
              "      <td>2.269000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4016</td>\n",
              "      <td>2.365100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4017</td>\n",
              "      <td>2.340400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4018</td>\n",
              "      <td>1.953600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4019</td>\n",
              "      <td>1.911500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4020</td>\n",
              "      <td>2.251500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4021</td>\n",
              "      <td>2.732800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4022</td>\n",
              "      <td>2.513300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4023</td>\n",
              "      <td>2.476800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4024</td>\n",
              "      <td>2.416300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4025</td>\n",
              "      <td>2.284200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4026</td>\n",
              "      <td>2.358000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4027</td>\n",
              "      <td>2.399100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4028</td>\n",
              "      <td>2.526000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4029</td>\n",
              "      <td>1.941800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4030</td>\n",
              "      <td>1.884200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4031</td>\n",
              "      <td>2.235300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4032</td>\n",
              "      <td>2.445300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4033</td>\n",
              "      <td>2.598000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4034</td>\n",
              "      <td>2.115500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4035</td>\n",
              "      <td>2.393900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4036</td>\n",
              "      <td>1.552700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4037</td>\n",
              "      <td>2.239400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4038</td>\n",
              "      <td>2.425700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4039</td>\n",
              "      <td>2.428000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4040</td>\n",
              "      <td>2.340100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4041</td>\n",
              "      <td>2.380700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4042</td>\n",
              "      <td>2.405600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4043</td>\n",
              "      <td>1.872700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4044</td>\n",
              "      <td>2.544400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4045</td>\n",
              "      <td>2.491600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4046</td>\n",
              "      <td>2.406300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4047</td>\n",
              "      <td>2.547600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4048</td>\n",
              "      <td>2.047400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4049</td>\n",
              "      <td>2.369300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4050</td>\n",
              "      <td>1.965700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4051</td>\n",
              "      <td>2.075600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4052</td>\n",
              "      <td>2.018000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4053</td>\n",
              "      <td>2.347700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4054</td>\n",
              "      <td>2.127500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4055</td>\n",
              "      <td>2.505200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4056</td>\n",
              "      <td>2.174700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4057</td>\n",
              "      <td>2.392700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4058</td>\n",
              "      <td>2.442800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4059</td>\n",
              "      <td>2.320400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4060</td>\n",
              "      <td>2.480000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4061</td>\n",
              "      <td>2.379900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4062</td>\n",
              "      <td>2.545700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4063</td>\n",
              "      <td>1.788000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4064</td>\n",
              "      <td>2.019500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4065</td>\n",
              "      <td>2.243000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4066</td>\n",
              "      <td>2.563900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4067</td>\n",
              "      <td>2.329800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4068</td>\n",
              "      <td>2.274200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4069</td>\n",
              "      <td>2.300600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4070</td>\n",
              "      <td>2.377000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4071</td>\n",
              "      <td>2.513900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4072</td>\n",
              "      <td>1.982100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4073</td>\n",
              "      <td>2.334100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4074</td>\n",
              "      <td>2.030700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4075</td>\n",
              "      <td>2.132000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4076</td>\n",
              "      <td>1.920700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4077</td>\n",
              "      <td>2.521700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4078</td>\n",
              "      <td>2.559500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4079</td>\n",
              "      <td>2.143900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4080</td>\n",
              "      <td>1.684000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4081</td>\n",
              "      <td>2.202200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4082</td>\n",
              "      <td>2.365900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4083</td>\n",
              "      <td>2.158600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4084</td>\n",
              "      <td>2.356700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4085</td>\n",
              "      <td>2.584900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4086</td>\n",
              "      <td>2.362600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4087</td>\n",
              "      <td>2.282700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4088</td>\n",
              "      <td>2.451900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4089</td>\n",
              "      <td>2.452600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4090</td>\n",
              "      <td>2.063300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4091</td>\n",
              "      <td>2.643900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4092</td>\n",
              "      <td>2.502100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4093</td>\n",
              "      <td>2.451600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4094</td>\n",
              "      <td>2.436700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4095</td>\n",
              "      <td>1.570900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4096</td>\n",
              "      <td>1.848400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4097</td>\n",
              "      <td>2.524000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4098</td>\n",
              "      <td>2.485700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4099</td>\n",
              "      <td>2.347200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4100</td>\n",
              "      <td>2.374300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4101</td>\n",
              "      <td>1.812400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4102</td>\n",
              "      <td>2.150100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4103</td>\n",
              "      <td>2.143800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4104</td>\n",
              "      <td>2.281700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4105</td>\n",
              "      <td>2.470600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4106</td>\n",
              "      <td>2.103300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4107</td>\n",
              "      <td>2.311600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4108</td>\n",
              "      <td>1.879100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4109</td>\n",
              "      <td>1.666800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4110</td>\n",
              "      <td>2.434000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4111</td>\n",
              "      <td>2.448000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4112</td>\n",
              "      <td>1.605900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4113</td>\n",
              "      <td>2.003800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4114</td>\n",
              "      <td>2.333900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4115</td>\n",
              "      <td>2.448700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4116</td>\n",
              "      <td>1.824500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4117</td>\n",
              "      <td>2.003800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4118</td>\n",
              "      <td>2.286200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4119</td>\n",
              "      <td>2.398100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4120</td>\n",
              "      <td>1.991800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4121</td>\n",
              "      <td>2.116200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4122</td>\n",
              "      <td>2.356700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4123</td>\n",
              "      <td>2.298800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4124</td>\n",
              "      <td>2.059000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4125</td>\n",
              "      <td>2.297300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4126</td>\n",
              "      <td>2.005900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4127</td>\n",
              "      <td>1.803800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4128</td>\n",
              "      <td>2.133300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4129</td>\n",
              "      <td>1.796800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4130</td>\n",
              "      <td>2.597400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4131</td>\n",
              "      <td>2.354200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4132</td>\n",
              "      <td>2.248700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4133</td>\n",
              "      <td>2.459400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4134</td>\n",
              "      <td>2.594300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4135</td>\n",
              "      <td>1.718000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4136</td>\n",
              "      <td>1.914000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4137</td>\n",
              "      <td>2.547200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4138</td>\n",
              "      <td>2.463200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4139</td>\n",
              "      <td>2.439700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4140</td>\n",
              "      <td>2.407100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4141</td>\n",
              "      <td>2.128600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4142</td>\n",
              "      <td>1.825100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4143</td>\n",
              "      <td>2.354600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4144</td>\n",
              "      <td>2.108400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4145</td>\n",
              "      <td>2.372500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4146</td>\n",
              "      <td>2.072400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4147</td>\n",
              "      <td>2.428900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4148</td>\n",
              "      <td>2.554800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4149</td>\n",
              "      <td>2.385200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4150</td>\n",
              "      <td>2.164900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4151</td>\n",
              "      <td>2.373700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4152</td>\n",
              "      <td>2.381900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4153</td>\n",
              "      <td>2.077200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4154</td>\n",
              "      <td>2.282200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4155</td>\n",
              "      <td>2.160600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4156</td>\n",
              "      <td>2.504000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4157</td>\n",
              "      <td>2.108800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4158</td>\n",
              "      <td>2.467900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4159</td>\n",
              "      <td>1.595400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4160</td>\n",
              "      <td>2.630400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4161</td>\n",
              "      <td>1.942900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4162</td>\n",
              "      <td>2.431400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4163</td>\n",
              "      <td>2.421000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4164</td>\n",
              "      <td>2.589000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4165</td>\n",
              "      <td>2.189200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4166</td>\n",
              "      <td>2.420700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4167</td>\n",
              "      <td>1.883700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4168</td>\n",
              "      <td>2.468900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4169</td>\n",
              "      <td>2.408500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4170</td>\n",
              "      <td>2.273600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4171</td>\n",
              "      <td>2.514200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4172</td>\n",
              "      <td>2.390100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4173</td>\n",
              "      <td>1.596800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4174</td>\n",
              "      <td>2.527200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4175</td>\n",
              "      <td>2.496100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4176</td>\n",
              "      <td>2.231000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4177</td>\n",
              "      <td>2.407400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4178</td>\n",
              "      <td>2.310300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4179</td>\n",
              "      <td>2.257900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4180</td>\n",
              "      <td>2.504700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4181</td>\n",
              "      <td>2.392900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4182</td>\n",
              "      <td>2.472600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4183</td>\n",
              "      <td>2.234000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4184</td>\n",
              "      <td>2.340400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4185</td>\n",
              "      <td>2.406700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4186</td>\n",
              "      <td>2.332000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4187</td>\n",
              "      <td>1.844200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4188</td>\n",
              "      <td>2.462600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4189</td>\n",
              "      <td>2.352100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4190</td>\n",
              "      <td>2.506300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4191</td>\n",
              "      <td>2.352300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4192</td>\n",
              "      <td>2.370700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4193</td>\n",
              "      <td>2.436900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4194</td>\n",
              "      <td>2.218300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4195</td>\n",
              "      <td>1.982700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4196</td>\n",
              "      <td>1.979600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4197</td>\n",
              "      <td>2.612300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4198</td>\n",
              "      <td>2.293200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4199</td>\n",
              "      <td>2.505200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4200</td>\n",
              "      <td>2.514500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4201</td>\n",
              "      <td>1.919700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4202</td>\n",
              "      <td>1.829000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4203</td>\n",
              "      <td>2.486900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4204</td>\n",
              "      <td>2.357800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4205</td>\n",
              "      <td>2.415000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4206</td>\n",
              "      <td>2.431300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4207</td>\n",
              "      <td>2.408700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4208</td>\n",
              "      <td>2.384100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4209</td>\n",
              "      <td>2.468500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4210</td>\n",
              "      <td>2.509500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4211</td>\n",
              "      <td>2.386900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4212</td>\n",
              "      <td>2.001500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4213</td>\n",
              "      <td>2.412000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4214</td>\n",
              "      <td>2.352600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4215</td>\n",
              "      <td>2.365600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4216</td>\n",
              "      <td>1.800700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4217</td>\n",
              "      <td>2.415600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4218</td>\n",
              "      <td>2.249000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4219</td>\n",
              "      <td>2.070100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4220</td>\n",
              "      <td>1.667800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4221</td>\n",
              "      <td>1.947300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4222</td>\n",
              "      <td>2.619800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4223</td>\n",
              "      <td>1.900600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4224</td>\n",
              "      <td>2.377300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4225</td>\n",
              "      <td>2.516700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4226</td>\n",
              "      <td>2.226100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4227</td>\n",
              "      <td>2.460600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4228</td>\n",
              "      <td>2.327500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4229</td>\n",
              "      <td>2.519300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4230</td>\n",
              "      <td>2.456800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4231</td>\n",
              "      <td>2.418200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4232</td>\n",
              "      <td>2.414500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4233</td>\n",
              "      <td>2.495000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4234</td>\n",
              "      <td>1.424600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4235</td>\n",
              "      <td>2.549900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4236</td>\n",
              "      <td>1.910500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4237</td>\n",
              "      <td>2.353300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4238</td>\n",
              "      <td>2.397600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4239</td>\n",
              "      <td>2.049500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4240</td>\n",
              "      <td>2.310200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4241</td>\n",
              "      <td>2.276000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4242</td>\n",
              "      <td>2.377000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4243</td>\n",
              "      <td>2.453700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4244</td>\n",
              "      <td>2.442600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4245</td>\n",
              "      <td>2.426300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4246</td>\n",
              "      <td>2.449300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4247</td>\n",
              "      <td>1.691200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4248</td>\n",
              "      <td>1.972100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4249</td>\n",
              "      <td>2.541800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4250</td>\n",
              "      <td>1.935300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4251</td>\n",
              "      <td>2.506700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4252</td>\n",
              "      <td>1.795100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4253</td>\n",
              "      <td>1.692300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4254</td>\n",
              "      <td>2.098400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4255</td>\n",
              "      <td>2.002600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4256</td>\n",
              "      <td>2.453000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4257</td>\n",
              "      <td>2.451300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4258</td>\n",
              "      <td>1.533700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4259</td>\n",
              "      <td>2.457800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4260</td>\n",
              "      <td>2.370400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4261</td>\n",
              "      <td>2.366700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4262</td>\n",
              "      <td>2.213600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4263</td>\n",
              "      <td>2.429100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4264</td>\n",
              "      <td>2.376900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4265</td>\n",
              "      <td>2.497700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4266</td>\n",
              "      <td>2.421400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4267</td>\n",
              "      <td>2.364700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4268</td>\n",
              "      <td>2.387200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4269</td>\n",
              "      <td>2.213200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4270</td>\n",
              "      <td>2.373200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4271</td>\n",
              "      <td>2.008100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4272</td>\n",
              "      <td>2.360700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4273</td>\n",
              "      <td>1.926400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4274</td>\n",
              "      <td>2.396200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4275</td>\n",
              "      <td>2.206700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4276</td>\n",
              "      <td>2.254200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4277</td>\n",
              "      <td>2.579500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4278</td>\n",
              "      <td>2.569400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4279</td>\n",
              "      <td>2.412300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4280</td>\n",
              "      <td>2.222500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4281</td>\n",
              "      <td>1.931100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4282</td>\n",
              "      <td>2.522500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4283</td>\n",
              "      <td>2.345600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4284</td>\n",
              "      <td>1.937500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4285</td>\n",
              "      <td>2.543600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4286</td>\n",
              "      <td>2.397100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4287</td>\n",
              "      <td>2.513000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4288</td>\n",
              "      <td>1.961200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4289</td>\n",
              "      <td>2.423700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4290</td>\n",
              "      <td>2.340500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4291</td>\n",
              "      <td>2.376100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4292</td>\n",
              "      <td>2.283100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4293</td>\n",
              "      <td>2.390100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4294</td>\n",
              "      <td>2.437500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4295</td>\n",
              "      <td>2.111400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4296</td>\n",
              "      <td>2.615700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4297</td>\n",
              "      <td>2.202600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4298</td>\n",
              "      <td>2.074200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4299</td>\n",
              "      <td>2.242600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4300</td>\n",
              "      <td>2.409100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4301</td>\n",
              "      <td>2.302100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4302</td>\n",
              "      <td>2.032500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4303</td>\n",
              "      <td>2.391300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4304</td>\n",
              "      <td>2.122700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4305</td>\n",
              "      <td>1.876000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4306</td>\n",
              "      <td>2.413700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4307</td>\n",
              "      <td>2.563500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4308</td>\n",
              "      <td>2.366000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4309</td>\n",
              "      <td>2.098400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4310</td>\n",
              "      <td>1.870800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4311</td>\n",
              "      <td>2.560300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4312</td>\n",
              "      <td>2.387800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4313</td>\n",
              "      <td>1.946000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4314</td>\n",
              "      <td>2.108000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4315</td>\n",
              "      <td>1.973800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4316</td>\n",
              "      <td>2.618200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4317</td>\n",
              "      <td>2.501300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4318</td>\n",
              "      <td>2.502600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4319</td>\n",
              "      <td>2.589300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4320</td>\n",
              "      <td>2.075100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4321</td>\n",
              "      <td>2.346700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4322</td>\n",
              "      <td>2.524500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4323</td>\n",
              "      <td>2.097900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4324</td>\n",
              "      <td>2.266300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4325</td>\n",
              "      <td>2.376500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4326</td>\n",
              "      <td>2.448200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4327</td>\n",
              "      <td>2.260100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4328</td>\n",
              "      <td>2.445100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4329</td>\n",
              "      <td>2.380300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4330</td>\n",
              "      <td>1.978600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4331</td>\n",
              "      <td>2.406300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4332</td>\n",
              "      <td>2.617300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4333</td>\n",
              "      <td>2.179800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4334</td>\n",
              "      <td>1.787400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4335</td>\n",
              "      <td>2.505400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4336</td>\n",
              "      <td>2.072600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4337</td>\n",
              "      <td>2.387600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4338</td>\n",
              "      <td>2.638500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4339</td>\n",
              "      <td>2.456500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4340</td>\n",
              "      <td>2.185300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4341</td>\n",
              "      <td>2.432900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4342</td>\n",
              "      <td>2.289500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4343</td>\n",
              "      <td>2.085500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4344</td>\n",
              "      <td>1.945900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4345</td>\n",
              "      <td>1.948600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4346</td>\n",
              "      <td>2.472100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4347</td>\n",
              "      <td>2.328400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4348</td>\n",
              "      <td>2.384400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4349</td>\n",
              "      <td>1.881400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4350</td>\n",
              "      <td>1.828600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4351</td>\n",
              "      <td>2.156500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4352</td>\n",
              "      <td>2.082900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4353</td>\n",
              "      <td>2.620500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4354</td>\n",
              "      <td>2.378900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4355</td>\n",
              "      <td>2.446600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4356</td>\n",
              "      <td>2.257900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4357</td>\n",
              "      <td>2.293800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4358</td>\n",
              "      <td>2.422900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4359</td>\n",
              "      <td>2.148700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4360</td>\n",
              "      <td>2.301600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4361</td>\n",
              "      <td>2.255800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4362</td>\n",
              "      <td>2.280900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4363</td>\n",
              "      <td>2.367400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4364</td>\n",
              "      <td>2.691800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4365</td>\n",
              "      <td>2.374600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4366</td>\n",
              "      <td>2.263800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4367</td>\n",
              "      <td>1.877800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4368</td>\n",
              "      <td>2.196100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4369</td>\n",
              "      <td>2.580300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4370</td>\n",
              "      <td>2.523200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4371</td>\n",
              "      <td>2.404900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4372</td>\n",
              "      <td>2.145900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4373</td>\n",
              "      <td>2.474500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4374</td>\n",
              "      <td>2.536400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4375</td>\n",
              "      <td>1.488600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4376</td>\n",
              "      <td>2.287900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4377</td>\n",
              "      <td>2.196400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4378</td>\n",
              "      <td>2.043100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4379</td>\n",
              "      <td>2.136200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4380</td>\n",
              "      <td>2.368300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4381</td>\n",
              "      <td>2.437600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4382</td>\n",
              "      <td>1.951200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4383</td>\n",
              "      <td>2.523900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4384</td>\n",
              "      <td>2.519900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4385</td>\n",
              "      <td>2.275400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4386</td>\n",
              "      <td>2.581600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4387</td>\n",
              "      <td>2.569000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4388</td>\n",
              "      <td>1.966900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4389</td>\n",
              "      <td>1.841800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4390</td>\n",
              "      <td>2.082800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4391</td>\n",
              "      <td>2.368300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4392</td>\n",
              "      <td>2.263200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4393</td>\n",
              "      <td>2.493200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4394</td>\n",
              "      <td>2.402900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4395</td>\n",
              "      <td>2.414300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4396</td>\n",
              "      <td>2.564300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4397</td>\n",
              "      <td>1.967200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4398</td>\n",
              "      <td>2.185100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4399</td>\n",
              "      <td>2.375200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4400</td>\n",
              "      <td>2.478200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4401</td>\n",
              "      <td>2.048600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4402</td>\n",
              "      <td>2.232400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4403</td>\n",
              "      <td>2.706100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4404</td>\n",
              "      <td>1.485700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4405</td>\n",
              "      <td>2.194700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4406</td>\n",
              "      <td>2.501800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4407</td>\n",
              "      <td>2.404200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4408</td>\n",
              "      <td>2.181900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4409</td>\n",
              "      <td>2.181000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4410</td>\n",
              "      <td>2.359300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4411</td>\n",
              "      <td>2.395000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4412</td>\n",
              "      <td>1.774600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4413</td>\n",
              "      <td>2.152700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4414</td>\n",
              "      <td>1.918500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4415</td>\n",
              "      <td>2.305200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4416</td>\n",
              "      <td>2.387800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4417</td>\n",
              "      <td>2.021300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4418</td>\n",
              "      <td>2.449400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4419</td>\n",
              "      <td>2.206800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4420</td>\n",
              "      <td>2.547100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4421</td>\n",
              "      <td>2.125800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4422</td>\n",
              "      <td>2.341000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4423</td>\n",
              "      <td>1.824400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4424</td>\n",
              "      <td>2.383300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4425</td>\n",
              "      <td>1.680700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4426</td>\n",
              "      <td>1.865600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4427</td>\n",
              "      <td>2.019200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4428</td>\n",
              "      <td>1.479500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4429</td>\n",
              "      <td>2.556400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4430</td>\n",
              "      <td>2.468600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4431</td>\n",
              "      <td>2.024000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4432</td>\n",
              "      <td>2.430300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4433</td>\n",
              "      <td>2.437300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4434</td>\n",
              "      <td>2.049300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4435</td>\n",
              "      <td>2.528300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4436</td>\n",
              "      <td>2.454100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4437</td>\n",
              "      <td>2.467900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4438</td>\n",
              "      <td>2.239900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4439</td>\n",
              "      <td>2.562900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4440</td>\n",
              "      <td>2.377400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4441</td>\n",
              "      <td>1.912100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4442</td>\n",
              "      <td>1.932300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4443</td>\n",
              "      <td>2.458400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4444</td>\n",
              "      <td>2.519400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4445</td>\n",
              "      <td>2.463200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4446</td>\n",
              "      <td>1.748700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4447</td>\n",
              "      <td>1.866000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4448</td>\n",
              "      <td>2.638300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4449</td>\n",
              "      <td>2.512200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4450</td>\n",
              "      <td>2.172100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4451</td>\n",
              "      <td>2.205900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4452</td>\n",
              "      <td>2.376000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4453</td>\n",
              "      <td>2.439600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4454</td>\n",
              "      <td>2.442700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4455</td>\n",
              "      <td>2.387000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4456</td>\n",
              "      <td>2.352100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4457</td>\n",
              "      <td>2.381100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4458</td>\n",
              "      <td>1.875200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4459</td>\n",
              "      <td>2.064400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4460</td>\n",
              "      <td>2.350000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4461</td>\n",
              "      <td>2.334600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4462</td>\n",
              "      <td>2.505200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4463</td>\n",
              "      <td>2.359400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4464</td>\n",
              "      <td>2.236500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4465</td>\n",
              "      <td>2.639100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4466</td>\n",
              "      <td>2.512900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4467</td>\n",
              "      <td>1.876500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4468</td>\n",
              "      <td>1.949600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4469</td>\n",
              "      <td>2.429000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4470</td>\n",
              "      <td>2.502000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4471</td>\n",
              "      <td>2.298000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4472</td>\n",
              "      <td>2.216800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4473</td>\n",
              "      <td>2.276500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4474</td>\n",
              "      <td>2.180600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4475</td>\n",
              "      <td>1.845600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4476</td>\n",
              "      <td>1.999200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4477</td>\n",
              "      <td>2.568600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4478</td>\n",
              "      <td>2.541600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4479</td>\n",
              "      <td>2.272400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4480</td>\n",
              "      <td>2.458100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4481</td>\n",
              "      <td>1.867100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4482</td>\n",
              "      <td>1.791900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4483</td>\n",
              "      <td>2.347100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4484</td>\n",
              "      <td>2.649800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4485</td>\n",
              "      <td>2.171600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4486</td>\n",
              "      <td>2.475400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4487</td>\n",
              "      <td>1.954600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4488</td>\n",
              "      <td>2.012100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4489</td>\n",
              "      <td>2.355400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4490</td>\n",
              "      <td>2.336500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4491</td>\n",
              "      <td>2.402800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4492</td>\n",
              "      <td>2.075300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4493</td>\n",
              "      <td>2.298900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4494</td>\n",
              "      <td>1.958400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4495</td>\n",
              "      <td>2.563700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4496</td>\n",
              "      <td>2.454400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4497</td>\n",
              "      <td>2.256800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4498</td>\n",
              "      <td>2.258100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4499</td>\n",
              "      <td>2.039600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4500</td>\n",
              "      <td>2.151000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4501</td>\n",
              "      <td>2.393500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4502</td>\n",
              "      <td>2.028900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4503</td>\n",
              "      <td>2.376900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4504</td>\n",
              "      <td>2.250900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4505</td>\n",
              "      <td>2.112500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4506</td>\n",
              "      <td>2.302600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4507</td>\n",
              "      <td>2.265900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4508</td>\n",
              "      <td>2.482400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4509</td>\n",
              "      <td>2.497300</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "Logs at step 2012: {'loss': 2.4737, 'grad_norm': 1.2077821493148804, 'learning_rate': 2.7700155245065422e-05, 'epoch': 1.338656021290752}\n",
            "Captured loss: 2.4737 at step 2012\n",
            "Logs at step 2013: {'loss': 2.657, 'grad_norm': 1.5322693586349487, 'learning_rate': 2.7689066311820806e-05, 'epoch': 1.3393213572854292}\n",
            "Captured loss: 2.657 at step 2013\n",
            "Logs at step 2014: {'loss': 2.6562, 'grad_norm': 1.316685676574707, 'learning_rate': 2.767797737857618e-05, 'epoch': 1.3399866932801063}\n",
            "Captured loss: 2.6562 at step 2014\n",
            "Logs at step 2015: {'loss': 2.6255, 'grad_norm': 1.3456860780715942, 'learning_rate': 2.766688844533156e-05, 'epoch': 1.3406520292747839}\n",
            "Captured loss: 2.6255 at step 2015\n",
            "Logs at step 2016: {'loss': 2.4739, 'grad_norm': 1.7118213176727295, 'learning_rate': 2.765579951208694e-05, 'epoch': 1.341317365269461}\n",
            "Captured loss: 2.4739 at step 2016\n",
            "Logs at step 2017: {'loss': 2.6528, 'grad_norm': 1.4005275964736938, 'learning_rate': 2.7644710578842313e-05, 'epoch': 1.3419827012641383}\n",
            "Captured loss: 2.6528 at step 2017\n",
            "Logs at step 2018: {'loss': 2.4718, 'grad_norm': 1.636483907699585, 'learning_rate': 2.7633621645597697e-05, 'epoch': 1.3426480372588157}\n",
            "Captured loss: 2.4718 at step 2018\n",
            "Logs at step 2019: {'loss': 2.4332, 'grad_norm': 1.0488755702972412, 'learning_rate': 2.762253271235307e-05, 'epoch': 1.343313373253493}\n",
            "Captured loss: 2.4332 at step 2019\n",
            "Logs at step 2020: {'loss': 2.6922, 'grad_norm': 1.2030953168869019, 'learning_rate': 2.7611443779108453e-05, 'epoch': 1.3439787092481703}\n",
            "Captured loss: 2.6922 at step 2020\n",
            "Logs at step 2021: {'loss': 2.4511, 'grad_norm': 1.314436435699463, 'learning_rate': 2.760035484586383e-05, 'epoch': 1.3446440452428476}\n",
            "Captured loss: 2.4511 at step 2021\n",
            "Logs at step 2022: {'loss': 2.4395, 'grad_norm': 1.9199495315551758, 'learning_rate': 2.7589265912619204e-05, 'epoch': 1.345309381237525}\n",
            "Captured loss: 2.4395 at step 2022\n",
            "Logs at step 2023: {'loss': 2.7285, 'grad_norm': 1.6009132862091064, 'learning_rate': 2.757817697937459e-05, 'epoch': 1.3459747172322023}\n",
            "Captured loss: 2.7285 at step 2023\n",
            "Logs at step 2024: {'loss': 2.2347, 'grad_norm': 1.3185646533966064, 'learning_rate': 2.7567088046129963e-05, 'epoch': 1.3466400532268796}\n",
            "Captured loss: 2.2347 at step 2024\n",
            "Logs at step 2025: {'loss': 2.8122, 'grad_norm': 1.5639870166778564, 'learning_rate': 2.7555999112885344e-05, 'epoch': 1.347305389221557}\n",
            "Captured loss: 2.8122 at step 2025\n",
            "Logs at step 2026: {'loss': 2.5769, 'grad_norm': 1.0857394933700562, 'learning_rate': 2.754491017964072e-05, 'epoch': 1.3479707252162343}\n",
            "Captured loss: 2.5769 at step 2026\n",
            "Logs at step 2027: {'loss': 2.6508, 'grad_norm': 1.4445635080337524, 'learning_rate': 2.7533821246396095e-05, 'epoch': 1.3486360612109114}\n",
            "Captured loss: 2.6508 at step 2027\n",
            "Logs at step 2028: {'loss': 2.5427, 'grad_norm': 1.2806620597839355, 'learning_rate': 2.752273231315148e-05, 'epoch': 1.349301397205589}\n",
            "Captured loss: 2.5427 at step 2028\n",
            "Logs at step 2029: {'loss': 2.795, 'grad_norm': 1.572275161743164, 'learning_rate': 2.7511643379906854e-05, 'epoch': 1.349966733200266}\n",
            "Captured loss: 2.795 at step 2029\n",
            "Logs at step 2030: {'loss': 2.4328, 'grad_norm': 1.3489186763763428, 'learning_rate': 2.750055444666223e-05, 'epoch': 1.3506320691949434}\n",
            "Captured loss: 2.4328 at step 2030\n",
            "Logs at step 2031: {'loss': 2.6839, 'grad_norm': 1.4681490659713745, 'learning_rate': 2.7489465513417612e-05, 'epoch': 1.3512974051896207}\n",
            "Captured loss: 2.6839 at step 2031\n",
            "Logs at step 2032: {'loss': 2.4765, 'grad_norm': 1.2521553039550781, 'learning_rate': 2.7478376580172987e-05, 'epoch': 1.351962741184298}\n",
            "Captured loss: 2.4765 at step 2032\n",
            "Logs at step 2033: {'loss': 2.5755, 'grad_norm': 0.9923328757286072, 'learning_rate': 2.7467287646928367e-05, 'epoch': 1.3526280771789754}\n",
            "Captured loss: 2.5755 at step 2033\n",
            "Logs at step 2034: {'loss': 2.5191, 'grad_norm': 0.9565601944923401, 'learning_rate': 2.7456198713683745e-05, 'epoch': 1.3532934131736527}\n",
            "Captured loss: 2.5191 at step 2034\n",
            "Logs at step 2035: {'loss': 2.6318, 'grad_norm': 1.4405299425125122, 'learning_rate': 2.7445109780439123e-05, 'epoch': 1.35395874916833}\n",
            "Captured loss: 2.6318 at step 2035\n",
            "Logs at step 2036: {'loss': 2.6197, 'grad_norm': 2.0753908157348633, 'learning_rate': 2.7434020847194504e-05, 'epoch': 1.3546240851630074}\n",
            "Captured loss: 2.6197 at step 2036\n",
            "Logs at step 2037: {'loss': 2.2249, 'grad_norm': 1.721295714378357, 'learning_rate': 2.7422931913949878e-05, 'epoch': 1.3552894211576847}\n",
            "Captured loss: 2.2249 at step 2037\n",
            "Logs at step 2038: {'loss': 2.5808, 'grad_norm': 1.1475706100463867, 'learning_rate': 2.741184298070526e-05, 'epoch': 1.355954757152362}\n",
            "Captured loss: 2.5808 at step 2038\n",
            "Logs at step 2039: {'loss': 2.5478, 'grad_norm': 1.1573551893234253, 'learning_rate': 2.7400754047460636e-05, 'epoch': 1.3566200931470394}\n",
            "Captured loss: 2.5478 at step 2039\n",
            "Logs at step 2040: {'loss': 2.2253, 'grad_norm': 1.3443249464035034, 'learning_rate': 2.7389665114216014e-05, 'epoch': 1.3572854291417165}\n",
            "Captured loss: 2.2253 at step 2040\n",
            "Logs at step 2041: {'loss': 2.0134, 'grad_norm': 1.6445419788360596, 'learning_rate': 2.7378576180971395e-05, 'epoch': 1.357950765136394}\n",
            "Captured loss: 2.0134 at step 2041\n",
            "Logs at step 2042: {'loss': 2.6319, 'grad_norm': 1.8254027366638184, 'learning_rate': 2.736748724772677e-05, 'epoch': 1.3586161011310711}\n",
            "Captured loss: 2.6319 at step 2042\n",
            "Logs at step 2043: {'loss': 2.5983, 'grad_norm': 1.2211289405822754, 'learning_rate': 2.7356398314482146e-05, 'epoch': 1.3592814371257484}\n",
            "Captured loss: 2.5983 at step 2043\n",
            "Logs at step 2044: {'loss': 2.5918, 'grad_norm': 1.0917365550994873, 'learning_rate': 2.7345309381237527e-05, 'epoch': 1.3599467731204258}\n",
            "Captured loss: 2.5918 at step 2044\n",
            "Logs at step 2045: {'loss': 2.6199, 'grad_norm': 1.0940879583358765, 'learning_rate': 2.7334220447992905e-05, 'epoch': 1.360612109115103}\n",
            "Captured loss: 2.6199 at step 2045\n",
            "Logs at step 2046: {'loss': 2.6121, 'grad_norm': 1.1785037517547607, 'learning_rate': 2.7323131514748286e-05, 'epoch': 1.3612774451097804}\n",
            "Captured loss: 2.6121 at step 2046\n",
            "Logs at step 2047: {'loss': 2.6813, 'grad_norm': 1.4016743898391724, 'learning_rate': 2.731204258150366e-05, 'epoch': 1.3619427811044578}\n",
            "Captured loss: 2.6813 at step 2047\n",
            "Logs at step 2048: {'loss': 2.4572, 'grad_norm': 1.4606831073760986, 'learning_rate': 2.7300953648259037e-05, 'epoch': 1.362608117099135}\n",
            "Captured loss: 2.4572 at step 2048\n",
            "Logs at step 2049: {'loss': 2.5691, 'grad_norm': 1.3598387241363525, 'learning_rate': 2.728986471501442e-05, 'epoch': 1.3632734530938124}\n",
            "Captured loss: 2.5691 at step 2049\n",
            "Logs at step 2050: {'loss': 2.6812, 'grad_norm': 1.039766788482666, 'learning_rate': 2.7278775781769793e-05, 'epoch': 1.3639387890884898}\n",
            "Captured loss: 2.6812 at step 2050\n",
            "Logs at step 2051: {'loss': 2.3877, 'grad_norm': 1.4482157230377197, 'learning_rate': 2.7267686848525177e-05, 'epoch': 1.3646041250831669}\n",
            "Captured loss: 2.3877 at step 2051\n",
            "Logs at step 2052: {'loss': 2.5449, 'grad_norm': 1.5825605392456055, 'learning_rate': 2.725659791528055e-05, 'epoch': 1.3652694610778444}\n",
            "Captured loss: 2.5449 at step 2052\n",
            "Logs at step 2053: {'loss': 2.5184, 'grad_norm': 1.4906985759735107, 'learning_rate': 2.724550898203593e-05, 'epoch': 1.3659347970725215}\n",
            "Captured loss: 2.5184 at step 2053\n",
            "Logs at step 2054: {'loss': 2.3378, 'grad_norm': 1.352096438407898, 'learning_rate': 2.723442004879131e-05, 'epoch': 1.3666001330671989}\n",
            "Captured loss: 2.3378 at step 2054\n",
            "Logs at step 2055: {'loss': 2.5739, 'grad_norm': 1.5562998056411743, 'learning_rate': 2.7223331115546684e-05, 'epoch': 1.3672654690618762}\n",
            "Captured loss: 2.5739 at step 2055\n",
            "Logs at step 2056: {'loss': 2.6169, 'grad_norm': 1.248749017715454, 'learning_rate': 2.721224218230206e-05, 'epoch': 1.3679308050565535}\n",
            "Captured loss: 2.6169 at step 2056\n",
            "Logs at step 2057: {'loss': 2.0423, 'grad_norm': 1.767085075378418, 'learning_rate': 2.7201153249057442e-05, 'epoch': 1.3685961410512308}\n",
            "Captured loss: 2.0423 at step 2057\n",
            "Logs at step 2058: {'loss': 2.331, 'grad_norm': 1.1984866857528687, 'learning_rate': 2.719006431581282e-05, 'epoch': 1.3692614770459082}\n",
            "Captured loss: 2.331 at step 2058\n",
            "Logs at step 2059: {'loss': 2.4843, 'grad_norm': 1.1254727840423584, 'learning_rate': 2.71789753825682e-05, 'epoch': 1.3699268130405855}\n",
            "Captured loss: 2.4843 at step 2059\n",
            "Logs at step 2060: {'loss': 2.2853, 'grad_norm': 1.2094517946243286, 'learning_rate': 2.7167886449323575e-05, 'epoch': 1.3705921490352628}\n",
            "Captured loss: 2.2853 at step 2060\n",
            "Logs at step 2061: {'loss': 2.2519, 'grad_norm': 1.2788325548171997, 'learning_rate': 2.7156797516078952e-05, 'epoch': 1.3712574850299402}\n",
            "Captured loss: 2.2519 at step 2061\n",
            "Logs at step 2062: {'loss': 2.5388, 'grad_norm': 1.191409945487976, 'learning_rate': 2.7145708582834333e-05, 'epoch': 1.3719228210246175}\n",
            "Captured loss: 2.5388 at step 2062\n",
            "Logs at step 2063: {'loss': 2.5199, 'grad_norm': 1.2014127969741821, 'learning_rate': 2.713461964958971e-05, 'epoch': 1.3725881570192948}\n",
            "Captured loss: 2.5199 at step 2063\n",
            "Logs at step 2064: {'loss': 2.7458, 'grad_norm': 1.3315997123718262, 'learning_rate': 2.7123530716345092e-05, 'epoch': 1.373253493013972}\n",
            "Captured loss: 2.7458 at step 2064\n",
            "Logs at step 2065: {'loss': 2.4507, 'grad_norm': 1.462995171546936, 'learning_rate': 2.7112441783100466e-05, 'epoch': 1.3739188290086495}\n",
            "Captured loss: 2.4507 at step 2065\n",
            "Logs at step 2066: {'loss': 2.6417, 'grad_norm': 1.0760269165039062, 'learning_rate': 2.7101352849855844e-05, 'epoch': 1.3745841650033266}\n",
            "Captured loss: 2.6417 at step 2066\n",
            "Logs at step 2067: {'loss': 2.6157, 'grad_norm': 1.5781912803649902, 'learning_rate': 2.7090263916611225e-05, 'epoch': 1.375249500998004}\n",
            "Captured loss: 2.6157 at step 2067\n",
            "Logs at step 2068: {'loss': 2.5929, 'grad_norm': 1.3591485023498535, 'learning_rate': 2.7079174983366602e-05, 'epoch': 1.3759148369926812}\n",
            "Captured loss: 2.5929 at step 2068\n",
            "Logs at step 2069: {'loss': 2.7002, 'grad_norm': 1.3542919158935547, 'learning_rate': 2.7068086050121976e-05, 'epoch': 1.3765801729873586}\n",
            "Captured loss: 2.7002 at step 2069\n",
            "Logs at step 2070: {'loss': 2.0384, 'grad_norm': 1.9231805801391602, 'learning_rate': 2.7056997116877357e-05, 'epoch': 1.377245508982036}\n",
            "Captured loss: 2.0384 at step 2070\n",
            "Logs at step 2071: {'loss': 2.3218, 'grad_norm': 1.406714677810669, 'learning_rate': 2.7045908183632735e-05, 'epoch': 1.3779108449767132}\n",
            "Captured loss: 2.3218 at step 2071\n",
            "Logs at step 2072: {'loss': 2.5818, 'grad_norm': 1.1850917339324951, 'learning_rate': 2.7034819250388116e-05, 'epoch': 1.3785761809713906}\n",
            "Captured loss: 2.5818 at step 2072\n",
            "Logs at step 2073: {'loss': 2.6667, 'grad_norm': 1.238338589668274, 'learning_rate': 2.7023730317143493e-05, 'epoch': 1.379241516966068}\n",
            "Captured loss: 2.6667 at step 2073\n",
            "Logs at step 2074: {'loss': 2.1387, 'grad_norm': 1.458797812461853, 'learning_rate': 2.7012641383898867e-05, 'epoch': 1.3799068529607452}\n",
            "Captured loss: 2.1387 at step 2074\n",
            "Logs at step 2075: {'loss': 2.5055, 'grad_norm': 1.3305752277374268, 'learning_rate': 2.7001552450654248e-05, 'epoch': 1.3805721889554226}\n",
            "Captured loss: 2.5055 at step 2075\n",
            "Logs at step 2076: {'loss': 2.3307, 'grad_norm': 1.1609411239624023, 'learning_rate': 2.6990463517409626e-05, 'epoch': 1.3812375249500999}\n",
            "Captured loss: 2.3307 at step 2076\n",
            "Logs at step 2077: {'loss': 2.4684, 'grad_norm': 1.3074020147323608, 'learning_rate': 2.6979374584165007e-05, 'epoch': 1.381902860944777}\n",
            "Captured loss: 2.4684 at step 2077\n",
            "Logs at step 2078: {'loss': 2.2773, 'grad_norm': 1.2742087841033936, 'learning_rate': 2.6968285650920384e-05, 'epoch': 1.3825681969394545}\n",
            "Captured loss: 2.2773 at step 2078\n",
            "Logs at step 2079: {'loss': 2.3824, 'grad_norm': 1.9043114185333252, 'learning_rate': 2.695719671767576e-05, 'epoch': 1.3832335329341316}\n",
            "Captured loss: 2.3824 at step 2079\n",
            "Logs at step 2080: {'loss': 2.5241, 'grad_norm': 1.326512098312378, 'learning_rate': 2.694610778443114e-05, 'epoch': 1.383898868928809}\n",
            "Captured loss: 2.5241 at step 2080\n",
            "Logs at step 2081: {'loss': 2.1306, 'grad_norm': 1.693671464920044, 'learning_rate': 2.6935018851186517e-05, 'epoch': 1.3845642049234863}\n",
            "Captured loss: 2.1306 at step 2081\n",
            "Logs at step 2082: {'loss': 2.5444, 'grad_norm': 1.4547573328018188, 'learning_rate': 2.6923929917941898e-05, 'epoch': 1.3852295409181636}\n",
            "Captured loss: 2.5444 at step 2082\n",
            "Logs at step 2083: {'loss': 2.2039, 'grad_norm': 1.5983322858810425, 'learning_rate': 2.6912840984697275e-05, 'epoch': 1.385894876912841}\n",
            "Captured loss: 2.2039 at step 2083\n",
            "Logs at step 2084: {'loss': 2.5636, 'grad_norm': 1.378900408744812, 'learning_rate': 2.690175205145265e-05, 'epoch': 1.3865602129075183}\n",
            "Captured loss: 2.5636 at step 2084\n",
            "Logs at step 2085: {'loss': 2.4364, 'grad_norm': 1.5788639783859253, 'learning_rate': 2.689066311820803e-05, 'epoch': 1.3872255489021956}\n",
            "Captured loss: 2.4364 at step 2085\n",
            "Logs at step 2086: {'loss': 2.6444, 'grad_norm': 2.151750087738037, 'learning_rate': 2.6879574184963408e-05, 'epoch': 1.387890884896873}\n",
            "Captured loss: 2.6444 at step 2086\n",
            "Logs at step 2087: {'loss': 2.7291, 'grad_norm': 1.8138419389724731, 'learning_rate': 2.6868485251718782e-05, 'epoch': 1.3885562208915503}\n",
            "Captured loss: 2.7291 at step 2087\n",
            "Logs at step 2088: {'loss': 2.5241, 'grad_norm': 2.377946138381958, 'learning_rate': 2.6857396318474167e-05, 'epoch': 1.3892215568862276}\n",
            "Captured loss: 2.5241 at step 2088\n",
            "Logs at step 2089: {'loss': 2.8066, 'grad_norm': 1.9850374460220337, 'learning_rate': 2.684630738522954e-05, 'epoch': 1.389886892880905}\n",
            "Captured loss: 2.8066 at step 2089\n",
            "Logs at step 2090: {'loss': 2.6246, 'grad_norm': 1.4343937635421753, 'learning_rate': 2.6835218451984922e-05, 'epoch': 1.390552228875582}\n",
            "Captured loss: 2.6246 at step 2090\n",
            "Logs at step 2091: {'loss': 2.344, 'grad_norm': 1.3199007511138916, 'learning_rate': 2.68241295187403e-05, 'epoch': 1.3912175648702596}\n",
            "Captured loss: 2.344 at step 2091\n",
            "Logs at step 2092: {'loss': 2.4685, 'grad_norm': 1.1011594533920288, 'learning_rate': 2.6813040585495673e-05, 'epoch': 1.3918829008649367}\n",
            "Captured loss: 2.4685 at step 2092\n",
            "Logs at step 2093: {'loss': 2.0831, 'grad_norm': 1.4073444604873657, 'learning_rate': 2.6801951652251058e-05, 'epoch': 1.392548236859614}\n",
            "Captured loss: 2.0831 at step 2093\n",
            "Logs at step 2094: {'loss': 2.5696, 'grad_norm': 1.2266162633895874, 'learning_rate': 2.6790862719006432e-05, 'epoch': 1.3932135728542914}\n",
            "Captured loss: 2.5696 at step 2094\n",
            "Logs at step 2095: {'loss': 2.4911, 'grad_norm': 1.024023175239563, 'learning_rate': 2.6779773785761813e-05, 'epoch': 1.3938789088489687}\n",
            "Captured loss: 2.4911 at step 2095\n",
            "Logs at step 2096: {'loss': 2.593, 'grad_norm': 1.0754722356796265, 'learning_rate': 2.676868485251719e-05, 'epoch': 1.394544244843646}\n",
            "Captured loss: 2.593 at step 2096\n",
            "Logs at step 2097: {'loss': 2.4849, 'grad_norm': 1.2648757696151733, 'learning_rate': 2.6757595919272565e-05, 'epoch': 1.3952095808383234}\n",
            "Captured loss: 2.4849 at step 2097\n",
            "Logs at step 2098: {'loss': 2.3989, 'grad_norm': 1.491999626159668, 'learning_rate': 2.6746506986027946e-05, 'epoch': 1.3958749168330007}\n",
            "Captured loss: 2.3989 at step 2098\n",
            "Logs at step 2099: {'loss': 2.4866, 'grad_norm': 1.042218804359436, 'learning_rate': 2.6735418052783323e-05, 'epoch': 1.396540252827678}\n",
            "Captured loss: 2.4866 at step 2099\n",
            "Logs at step 2100: {'loss': 2.442, 'grad_norm': 1.2944707870483398, 'learning_rate': 2.67243291195387e-05, 'epoch': 1.3972055888223553}\n",
            "Captured loss: 2.442 at step 2100\n",
            "Logs at step 2101: {'loss': 2.6262, 'grad_norm': 0.9674498438835144, 'learning_rate': 2.671324018629408e-05, 'epoch': 1.3978709248170327}\n",
            "Captured loss: 2.6262 at step 2101\n",
            "Logs at step 2102: {'loss': 2.5334, 'grad_norm': 1.3521615266799927, 'learning_rate': 2.6702151253049456e-05, 'epoch': 1.39853626081171}\n",
            "Captured loss: 2.5334 at step 2102\n",
            "Logs at step 2103: {'loss': 2.5133, 'grad_norm': 1.1634761095046997, 'learning_rate': 2.6691062319804837e-05, 'epoch': 1.3992015968063871}\n",
            "Captured loss: 2.5133 at step 2103\n",
            "Logs at step 2104: {'loss': 2.5959, 'grad_norm': 1.986161470413208, 'learning_rate': 2.6679973386560214e-05, 'epoch': 1.3998669328010647}\n",
            "Captured loss: 2.5959 at step 2104\n",
            "Logs at step 2105: {'loss': 2.3947, 'grad_norm': 0.9929263591766357, 'learning_rate': 2.6668884453315592e-05, 'epoch': 1.4005322687957418}\n",
            "Captured loss: 2.3947 at step 2105\n",
            "Logs at step 2106: {'loss': 2.5634, 'grad_norm': 1.4931083917617798, 'learning_rate': 2.6657795520070973e-05, 'epoch': 1.401197604790419}\n",
            "Captured loss: 2.5634 at step 2106\n",
            "Logs at step 2107: {'loss': 2.7696, 'grad_norm': 2.892707586288452, 'learning_rate': 2.6646706586826347e-05, 'epoch': 1.4018629407850964}\n",
            "Captured loss: 2.7696 at step 2107\n",
            "Logs at step 2108: {'loss': 2.5123, 'grad_norm': 1.825814127922058, 'learning_rate': 2.6635617653581728e-05, 'epoch': 1.4025282767797738}\n",
            "Captured loss: 2.5123 at step 2108\n",
            "Logs at step 2109: {'loss': 2.5793, 'grad_norm': 1.4863394498825073, 'learning_rate': 2.6624528720337105e-05, 'epoch': 1.403193612774451}\n",
            "Captured loss: 2.5793 at step 2109\n",
            "Logs at step 2110: {'loss': 2.5059, 'grad_norm': 1.0983250141143799, 'learning_rate': 2.661343978709248e-05, 'epoch': 1.4038589487691284}\n",
            "Captured loss: 2.5059 at step 2110\n",
            "Logs at step 2111: {'loss': 2.5927, 'grad_norm': 1.2396495342254639, 'learning_rate': 2.6602350853847864e-05, 'epoch': 1.4045242847638058}\n",
            "Captured loss: 2.5927 at step 2111\n",
            "Logs at step 2112: {'loss': 2.2398, 'grad_norm': 1.3969990015029907, 'learning_rate': 2.6591261920603238e-05, 'epoch': 1.405189620758483}\n",
            "Captured loss: 2.2398 at step 2112\n",
            "Logs at step 2113: {'loss': 2.6487, 'grad_norm': 1.6962718963623047, 'learning_rate': 2.6580172987358616e-05, 'epoch': 1.4058549567531604}\n",
            "Captured loss: 2.6487 at step 2113\n",
            "Logs at step 2114: {'loss': 2.4143, 'grad_norm': 1.2743552923202515, 'learning_rate': 2.6569084054113996e-05, 'epoch': 1.4065202927478375}\n",
            "Captured loss: 2.4143 at step 2114\n",
            "Logs at step 2115: {'loss': 2.5613, 'grad_norm': 1.1590137481689453, 'learning_rate': 2.655799512086937e-05, 'epoch': 1.407185628742515}\n",
            "Captured loss: 2.5613 at step 2115\n",
            "Logs at step 2116: {'loss': 2.478, 'grad_norm': 1.3449634313583374, 'learning_rate': 2.6546906187624755e-05, 'epoch': 1.4078509647371922}\n",
            "Captured loss: 2.478 at step 2116\n",
            "Logs at step 2117: {'loss': 2.5718, 'grad_norm': 1.206387996673584, 'learning_rate': 2.653581725438013e-05, 'epoch': 1.4085163007318697}\n",
            "Captured loss: 2.5718 at step 2117\n",
            "Logs at step 2118: {'loss': 2.4939, 'grad_norm': 1.4144777059555054, 'learning_rate': 2.6524728321135507e-05, 'epoch': 1.4091816367265468}\n",
            "Captured loss: 2.4939 at step 2118\n",
            "Logs at step 2119: {'loss': 2.5859, 'grad_norm': 1.1850330829620361, 'learning_rate': 2.6513639387890888e-05, 'epoch': 1.4098469727212242}\n",
            "Captured loss: 2.5859 at step 2119\n",
            "Logs at step 2120: {'loss': 2.333, 'grad_norm': 1.342633843421936, 'learning_rate': 2.6502550454646262e-05, 'epoch': 1.4105123087159015}\n",
            "Captured loss: 2.333 at step 2120\n",
            "Logs at step 2121: {'loss': 2.5697, 'grad_norm': 0.9868919253349304, 'learning_rate': 2.6491461521401646e-05, 'epoch': 1.4111776447105788}\n",
            "Captured loss: 2.5697 at step 2121\n",
            "Logs at step 2122: {'loss': 2.3289, 'grad_norm': 1.389818787574768, 'learning_rate': 2.648037258815702e-05, 'epoch': 1.4118429807052562}\n",
            "Captured loss: 2.3289 at step 2122\n",
            "Logs at step 2123: {'loss': 2.6959, 'grad_norm': 1.178755283355713, 'learning_rate': 2.6469283654912398e-05, 'epoch': 1.4125083166999335}\n",
            "Captured loss: 2.6959 at step 2123\n",
            "Logs at step 2124: {'loss': 2.5137, 'grad_norm': 1.1164313554763794, 'learning_rate': 2.645819472166778e-05, 'epoch': 1.4131736526946108}\n",
            "Captured loss: 2.5137 at step 2124\n",
            "Logs at step 2125: {'loss': 2.5887, 'grad_norm': 1.209152102470398, 'learning_rate': 2.6447105788423153e-05, 'epoch': 1.4138389886892881}\n",
            "Captured loss: 2.5887 at step 2125\n",
            "Logs at step 2126: {'loss': 2.4592, 'grad_norm': 1.0961700677871704, 'learning_rate': 2.643601685517853e-05, 'epoch': 1.4145043246839655}\n",
            "Captured loss: 2.4592 at step 2126\n",
            "Logs at step 2127: {'loss': 2.7549, 'grad_norm': 2.150017738342285, 'learning_rate': 2.642492792193391e-05, 'epoch': 1.4151696606786426}\n",
            "Captured loss: 2.7549 at step 2127\n",
            "Logs at step 2128: {'loss': 2.4065, 'grad_norm': 1.1717149019241333, 'learning_rate': 2.641383898868929e-05, 'epoch': 1.4158349966733201}\n",
            "Captured loss: 2.4065 at step 2128\n",
            "Logs at step 2129: {'loss': 2.4313, 'grad_norm': 1.3788715600967407, 'learning_rate': 2.640275005544467e-05, 'epoch': 1.4165003326679972}\n",
            "Captured loss: 2.4313 at step 2129\n",
            "Logs at step 2130: {'loss': 2.4995, 'grad_norm': 1.3152199983596802, 'learning_rate': 2.6391661122200044e-05, 'epoch': 1.4171656686626746}\n",
            "Captured loss: 2.4995 at step 2130\n",
            "Logs at step 2131: {'loss': 2.5366, 'grad_norm': 1.0072219371795654, 'learning_rate': 2.638057218895542e-05, 'epoch': 1.417831004657352}\n",
            "Captured loss: 2.5366 at step 2131\n",
            "Logs at step 2132: {'loss': 2.5717, 'grad_norm': 0.9874041080474854, 'learning_rate': 2.6369483255710803e-05, 'epoch': 1.4184963406520292}\n",
            "Captured loss: 2.5717 at step 2132\n",
            "Logs at step 2133: {'loss': 2.5664, 'grad_norm': 1.1560359001159668, 'learning_rate': 2.635839432246618e-05, 'epoch': 1.4191616766467066}\n",
            "Captured loss: 2.5664 at step 2133\n",
            "Logs at step 2134: {'loss': 2.4383, 'grad_norm': 1.20746648311615, 'learning_rate': 2.634730538922156e-05, 'epoch': 1.419827012641384}\n",
            "Captured loss: 2.4383 at step 2134\n",
            "Logs at step 2135: {'loss': 2.392, 'grad_norm': 1.4542216062545776, 'learning_rate': 2.6336216455976935e-05, 'epoch': 1.4204923486360612}\n",
            "Captured loss: 2.392 at step 2135\n",
            "Logs at step 2136: {'loss': 2.0251, 'grad_norm': 1.5705225467681885, 'learning_rate': 2.6325127522732313e-05, 'epoch': 1.4211576846307385}\n",
            "Captured loss: 2.0251 at step 2136\n",
            "Logs at step 2137: {'loss': 2.6141, 'grad_norm': 1.7855204343795776, 'learning_rate': 2.6314038589487694e-05, 'epoch': 1.4218230206254159}\n",
            "Captured loss: 2.6141 at step 2137\n",
            "Logs at step 2138: {'loss': 2.4941, 'grad_norm': 0.9732499718666077, 'learning_rate': 2.630294965624307e-05, 'epoch': 1.4224883566200932}\n",
            "Captured loss: 2.4941 at step 2138\n",
            "Logs at step 2139: {'loss': 2.2393, 'grad_norm': 1.4451775550842285, 'learning_rate': 2.6291860722998452e-05, 'epoch': 1.4231536926147705}\n",
            "Captured loss: 2.2393 at step 2139\n",
            "Logs at step 2140: {'loss': 2.6432, 'grad_norm': 1.561302661895752, 'learning_rate': 2.6280771789753826e-05, 'epoch': 1.4238190286094476}\n",
            "Captured loss: 2.6432 at step 2140\n",
            "Logs at step 2141: {'loss': 2.5237, 'grad_norm': 1.1582343578338623, 'learning_rate': 2.6269682856509204e-05, 'epoch': 1.4244843646041252}\n",
            "Captured loss: 2.5237 at step 2141\n",
            "Logs at step 2142: {'loss': 2.2923, 'grad_norm': 1.316424012184143, 'learning_rate': 2.6258593923264585e-05, 'epoch': 1.4251497005988023}\n",
            "Captured loss: 2.2923 at step 2142\n",
            "Logs at step 2143: {'loss': 2.3216, 'grad_norm': 1.12201726436615, 'learning_rate': 2.6247504990019962e-05, 'epoch': 1.4258150365934796}\n",
            "Captured loss: 2.3216 at step 2143\n",
            "Logs at step 2144: {'loss': 2.3148, 'grad_norm': 1.3120445013046265, 'learning_rate': 2.6236416056775337e-05, 'epoch': 1.426480372588157}\n",
            "Captured loss: 2.3148 at step 2144\n",
            "Logs at step 2145: {'loss': 2.6422, 'grad_norm': 1.334682583808899, 'learning_rate': 2.6225327123530717e-05, 'epoch': 1.4271457085828343}\n",
            "Captured loss: 2.6422 at step 2145\n",
            "Logs at step 2146: {'loss': 2.6247, 'grad_norm': 1.5260133743286133, 'learning_rate': 2.6214238190286095e-05, 'epoch': 1.4278110445775116}\n",
            "Captured loss: 2.6247 at step 2146\n",
            "Logs at step 2147: {'loss': 2.5675, 'grad_norm': 1.4708399772644043, 'learning_rate': 2.6203149257041476e-05, 'epoch': 1.428476380572189}\n",
            "Captured loss: 2.5675 at step 2147\n",
            "Logs at step 2148: {'loss': 2.4986, 'grad_norm': 1.2909051179885864, 'learning_rate': 2.6192060323796854e-05, 'epoch': 1.4291417165668663}\n",
            "Captured loss: 2.4986 at step 2148\n",
            "Logs at step 2149: {'loss': 2.4999, 'grad_norm': 1.3408441543579102, 'learning_rate': 2.6180971390552228e-05, 'epoch': 1.4298070525615436}\n",
            "Captured loss: 2.4999 at step 2149\n",
            "Logs at step 2150: {'loss': 2.4883, 'grad_norm': 1.4193603992462158, 'learning_rate': 2.616988245730761e-05, 'epoch': 1.430472388556221}\n",
            "Captured loss: 2.4883 at step 2150\n",
            "Logs at step 2151: {'loss': 2.3973, 'grad_norm': 1.5436536073684692, 'learning_rate': 2.6158793524062986e-05, 'epoch': 1.4311377245508983}\n",
            "Captured loss: 2.3973 at step 2151\n",
            "Logs at step 2152: {'loss': 2.8233, 'grad_norm': 1.4528933763504028, 'learning_rate': 2.6147704590818367e-05, 'epoch': 1.4318030605455756}\n",
            "Captured loss: 2.8233 at step 2152\n",
            "Logs at step 2153: {'loss': 2.5732, 'grad_norm': 1.0627237558364868, 'learning_rate': 2.6136615657573745e-05, 'epoch': 1.4324683965402527}\n",
            "Captured loss: 2.5732 at step 2153\n",
            "Logs at step 2154: {'loss': 2.595, 'grad_norm': 1.4575655460357666, 'learning_rate': 2.612552672432912e-05, 'epoch': 1.4331337325349303}\n",
            "Captured loss: 2.595 at step 2154\n",
            "Logs at step 2155: {'loss': 2.5677, 'grad_norm': 1.1821454763412476, 'learning_rate': 2.61144377910845e-05, 'epoch': 1.4337990685296074}\n",
            "Captured loss: 2.5677 at step 2155\n",
            "Logs at step 2156: {'loss': 2.4785, 'grad_norm': 1.9206339120864868, 'learning_rate': 2.6103348857839877e-05, 'epoch': 1.4344644045242847}\n",
            "Captured loss: 2.4785 at step 2156\n",
            "Logs at step 2157: {'loss': 2.501, 'grad_norm': 1.7647773027420044, 'learning_rate': 2.609225992459525e-05, 'epoch': 1.435129740518962}\n",
            "Captured loss: 2.501 at step 2157\n",
            "Logs at step 2158: {'loss': 2.6367, 'grad_norm': 1.8577301502227783, 'learning_rate': 2.6081170991350636e-05, 'epoch': 1.4357950765136394}\n",
            "Captured loss: 2.6367 at step 2158\n",
            "Logs at step 2159: {'loss': 2.5102, 'grad_norm': 1.2074352502822876, 'learning_rate': 2.607008205810601e-05, 'epoch': 1.4364604125083167}\n",
            "Captured loss: 2.5102 at step 2159\n",
            "Logs at step 2160: {'loss': 2.078, 'grad_norm': 1.5739059448242188, 'learning_rate': 2.605899312486139e-05, 'epoch': 1.437125748502994}\n",
            "Captured loss: 2.078 at step 2160\n",
            "Logs at step 2161: {'loss': 2.6983, 'grad_norm': 1.3431978225708008, 'learning_rate': 2.604790419161677e-05, 'epoch': 1.4377910844976713}\n",
            "Captured loss: 2.6983 at step 2161\n",
            "Logs at step 2162: {'loss': 2.5654, 'grad_norm': 1.239566445350647, 'learning_rate': 2.6036815258372143e-05, 'epoch': 1.4384564204923487}\n",
            "Captured loss: 2.5654 at step 2162\n",
            "Logs at step 2163: {'loss': 2.5715, 'grad_norm': 1.740930438041687, 'learning_rate': 2.6025726325127524e-05, 'epoch': 1.439121756487026}\n",
            "Captured loss: 2.5715 at step 2163\n",
            "Logs at step 2164: {'loss': 2.4154, 'grad_norm': 1.6699023246765137, 'learning_rate': 2.60146373918829e-05, 'epoch': 1.4397870924817033}\n",
            "Captured loss: 2.4154 at step 2164\n",
            "Logs at step 2165: {'loss': 2.4457, 'grad_norm': 1.4423332214355469, 'learning_rate': 2.6003548458638282e-05, 'epoch': 1.4404524284763807}\n",
            "Captured loss: 2.4457 at step 2165\n",
            "Logs at step 2166: {'loss': 2.1411, 'grad_norm': 2.0935513973236084, 'learning_rate': 2.599245952539366e-05, 'epoch': 1.4411177644710578}\n",
            "Captured loss: 2.1411 at step 2166\n",
            "Logs at step 2167: {'loss': 2.4325, 'grad_norm': 1.3101352453231812, 'learning_rate': 2.5981370592149034e-05, 'epoch': 1.4417831004657353}\n",
            "Captured loss: 2.4325 at step 2167\n",
            "Logs at step 2168: {'loss': 2.1678, 'grad_norm': 1.3293733596801758, 'learning_rate': 2.5970281658904415e-05, 'epoch': 1.4424484364604124}\n",
            "Captured loss: 2.1678 at step 2168\n",
            "Logs at step 2169: {'loss': 2.4973, 'grad_norm': 1.3506606817245483, 'learning_rate': 2.5959192725659792e-05, 'epoch': 1.4431137724550898}\n",
            "Captured loss: 2.4973 at step 2169\n",
            "Logs at step 2170: {'loss': 2.4261, 'grad_norm': 1.6873667240142822, 'learning_rate': 2.594810379241517e-05, 'epoch': 1.443779108449767}\n",
            "Captured loss: 2.4261 at step 2170\n",
            "Logs at step 2171: {'loss': 2.5894, 'grad_norm': 1.1765732765197754, 'learning_rate': 2.593701485917055e-05, 'epoch': 1.4444444444444444}\n",
            "Captured loss: 2.5894 at step 2171\n",
            "Logs at step 2172: {'loss': 2.2828, 'grad_norm': 1.710693120956421, 'learning_rate': 2.5925925925925925e-05, 'epoch': 1.4451097804391217}\n",
            "Captured loss: 2.2828 at step 2172\n",
            "Logs at step 2173: {'loss': 2.3226, 'grad_norm': 1.2540128231048584, 'learning_rate': 2.5914836992681306e-05, 'epoch': 1.445775116433799}\n",
            "Captured loss: 2.3226 at step 2173\n",
            "Logs at step 2174: {'loss': 2.2357, 'grad_norm': 1.1562098264694214, 'learning_rate': 2.5903748059436683e-05, 'epoch': 1.4464404524284764}\n",
            "Captured loss: 2.2357 at step 2174\n",
            "Logs at step 2175: {'loss': 2.5895, 'grad_norm': 1.2660105228424072, 'learning_rate': 2.5892659126192058e-05, 'epoch': 1.4471057884231537}\n",
            "Captured loss: 2.5895 at step 2175\n",
            "Logs at step 2176: {'loss': 2.6509, 'grad_norm': 1.658313512802124, 'learning_rate': 2.5881570192947442e-05, 'epoch': 1.447771124417831}\n",
            "Captured loss: 2.6509 at step 2176\n",
            "Logs at step 2177: {'loss': 2.591, 'grad_norm': 1.8023488521575928, 'learning_rate': 2.5870481259702816e-05, 'epoch': 1.4484364604125084}\n",
            "Captured loss: 2.591 at step 2177\n",
            "Logs at step 2178: {'loss': 2.4009, 'grad_norm': 0.9948176145553589, 'learning_rate': 2.5859392326458197e-05, 'epoch': 1.4491017964071857}\n",
            "Captured loss: 2.4009 at step 2178\n",
            "Logs at step 2179: {'loss': 2.4556, 'grad_norm': 1.2761093378067017, 'learning_rate': 2.5848303393213575e-05, 'epoch': 1.4497671324018628}\n",
            "Captured loss: 2.4556 at step 2179\n",
            "Logs at step 2180: {'loss': 2.5189, 'grad_norm': 1.0364465713500977, 'learning_rate': 2.583721445996895e-05, 'epoch': 1.4504324683965404}\n",
            "Captured loss: 2.5189 at step 2180\n",
            "Logs at step 2181: {'loss': 2.4166, 'grad_norm': 1.1078325510025024, 'learning_rate': 2.5826125526724333e-05, 'epoch': 1.4510978043912175}\n",
            "Captured loss: 2.4166 at step 2181\n",
            "Logs at step 2182: {'loss': 2.3536, 'grad_norm': 1.3920178413391113, 'learning_rate': 2.5815036593479707e-05, 'epoch': 1.4517631403858948}\n",
            "Captured loss: 2.3536 at step 2182\n",
            "Logs at step 2183: {'loss': 2.512, 'grad_norm': 1.1316806077957153, 'learning_rate': 2.5803947660235088e-05, 'epoch': 1.4524284763805722}\n",
            "Captured loss: 2.512 at step 2183\n",
            "Logs at step 2184: {'loss': 2.5952, 'grad_norm': 0.9970217943191528, 'learning_rate': 2.5792858726990466e-05, 'epoch': 1.4530938123752495}\n",
            "Captured loss: 2.5952 at step 2184\n",
            "Logs at step 2185: {'loss': 2.4323, 'grad_norm': 1.3782365322113037, 'learning_rate': 2.578176979374584e-05, 'epoch': 1.4537591483699268}\n",
            "Captured loss: 2.4323 at step 2185\n",
            "Logs at step 2186: {'loss': 2.3034, 'grad_norm': 1.3278273344039917, 'learning_rate': 2.5770680860501224e-05, 'epoch': 1.4544244843646041}\n",
            "Captured loss: 2.3034 at step 2186\n",
            "Logs at step 2187: {'loss': 2.1943, 'grad_norm': 1.22837495803833, 'learning_rate': 2.57595919272566e-05, 'epoch': 1.4550898203592815}\n",
            "Captured loss: 2.1943 at step 2187\n",
            "Logs at step 2188: {'loss': 2.3823, 'grad_norm': 1.1354146003723145, 'learning_rate': 2.5748502994011976e-05, 'epoch': 1.4557551563539588}\n",
            "Captured loss: 2.3823 at step 2188\n",
            "Logs at step 2189: {'loss': 2.3887, 'grad_norm': 1.1976759433746338, 'learning_rate': 2.5737414060767357e-05, 'epoch': 1.4564204923486361}\n",
            "Captured loss: 2.3887 at step 2189\n",
            "Logs at step 2190: {'loss': 2.5719, 'grad_norm': 2.1250417232513428, 'learning_rate': 2.572632512752273e-05, 'epoch': 1.4570858283433132}\n",
            "Captured loss: 2.5719 at step 2190\n",
            "Logs at step 2191: {'loss': 2.5133, 'grad_norm': 1.1771156787872314, 'learning_rate': 2.5715236194278115e-05, 'epoch': 1.4577511643379908}\n",
            "Captured loss: 2.5133 at step 2191\n",
            "Logs at step 2192: {'loss': 2.4406, 'grad_norm': 1.85757315158844, 'learning_rate': 2.570414726103349e-05, 'epoch': 1.458416500332668}\n",
            "Captured loss: 2.4406 at step 2192\n",
            "Logs at step 2193: {'loss': 2.6292, 'grad_norm': 1.1703457832336426, 'learning_rate': 2.5693058327788867e-05, 'epoch': 1.4590818363273452}\n",
            "Captured loss: 2.6292 at step 2193\n",
            "Logs at step 2194: {'loss': 2.5142, 'grad_norm': 1.0048130750656128, 'learning_rate': 2.5681969394544248e-05, 'epoch': 1.4597471723220226}\n",
            "Captured loss: 2.5142 at step 2194\n",
            "Logs at step 2195: {'loss': 2.5236, 'grad_norm': 1.2288329601287842, 'learning_rate': 2.5670880461299622e-05, 'epoch': 1.4604125083166999}\n",
            "Captured loss: 2.5236 at step 2195\n",
            "Logs at step 2196: {'loss': 2.6931, 'grad_norm': 1.343105435371399, 'learning_rate': 2.5659791528055006e-05, 'epoch': 1.4610778443113772}\n",
            "Captured loss: 2.6931 at step 2196\n",
            "Logs at step 2197: {'loss': 2.5402, 'grad_norm': 1.152540922164917, 'learning_rate': 2.564870259481038e-05, 'epoch': 1.4617431803060545}\n",
            "Captured loss: 2.5402 at step 2197\n",
            "Logs at step 2198: {'loss': 2.5985, 'grad_norm': 1.6495388746261597, 'learning_rate': 2.5637613661565758e-05, 'epoch': 1.4624085163007319}\n",
            "Captured loss: 2.5985 at step 2198\n",
            "Logs at step 2199: {'loss': 2.6638, 'grad_norm': 1.551988124847412, 'learning_rate': 2.562652472832114e-05, 'epoch': 1.4630738522954092}\n",
            "Captured loss: 2.6638 at step 2199\n",
            "Logs at step 2200: {'loss': 2.655, 'grad_norm': 1.3097155094146729, 'learning_rate': 2.5615435795076513e-05, 'epoch': 1.4637391882900865}\n",
            "Captured loss: 2.655 at step 2200\n",
            "Logs at step 2201: {'loss': 2.5139, 'grad_norm': 1.695527195930481, 'learning_rate': 2.560434686183189e-05, 'epoch': 1.4644045242847639}\n",
            "Captured loss: 2.5139 at step 2201\n",
            "Logs at step 2202: {'loss': 2.448, 'grad_norm': 1.2535821199417114, 'learning_rate': 2.5593257928587272e-05, 'epoch': 1.4650698602794412}\n",
            "Captured loss: 2.448 at step 2202\n",
            "Logs at step 2203: {'loss': 1.963, 'grad_norm': 2.1640982627868652, 'learning_rate': 2.558216899534265e-05, 'epoch': 1.4657351962741183}\n",
            "Captured loss: 1.963 at step 2203\n",
            "Logs at step 2204: {'loss': 2.4394, 'grad_norm': 1.2055307626724243, 'learning_rate': 2.557108006209803e-05, 'epoch': 1.4664005322687959}\n",
            "Captured loss: 2.4394 at step 2204\n",
            "Logs at step 2205: {'loss': 2.3532, 'grad_norm': 1.7525404691696167, 'learning_rate': 2.5559991128853404e-05, 'epoch': 1.467065868263473}\n",
            "Captured loss: 2.3532 at step 2205\n",
            "Logs at step 2206: {'loss': 2.479, 'grad_norm': 1.304578423500061, 'learning_rate': 2.5548902195608782e-05, 'epoch': 1.4677312042581503}\n",
            "Captured loss: 2.479 at step 2206\n",
            "Logs at step 2207: {'loss': 2.4027, 'grad_norm': 1.140459656715393, 'learning_rate': 2.5537813262364163e-05, 'epoch': 1.4683965402528276}\n",
            "Captured loss: 2.4027 at step 2207\n",
            "Logs at step 2208: {'loss': 2.5568, 'grad_norm': 1.080633521080017, 'learning_rate': 2.552672432911954e-05, 'epoch': 1.469061876247505}\n",
            "Captured loss: 2.5568 at step 2208\n",
            "Logs at step 2209: {'loss': 2.6499, 'grad_norm': 1.3109642267227173, 'learning_rate': 2.551563539587492e-05, 'epoch': 1.4697272122421823}\n",
            "Captured loss: 2.6499 at step 2209\n",
            "Logs at step 2210: {'loss': 2.1437, 'grad_norm': 1.3558305501937866, 'learning_rate': 2.5504546462630296e-05, 'epoch': 1.4703925482368596}\n",
            "Captured loss: 2.1437 at step 2210\n",
            "Logs at step 2211: {'loss': 2.1709, 'grad_norm': 1.9759089946746826, 'learning_rate': 2.5493457529385673e-05, 'epoch': 1.471057884231537}\n",
            "Captured loss: 2.1709 at step 2211\n",
            "Logs at step 2212: {'loss': 2.5327, 'grad_norm': 1.222790002822876, 'learning_rate': 2.5482368596141054e-05, 'epoch': 1.4717232202262143}\n",
            "Captured loss: 2.5327 at step 2212\n",
            "Logs at step 2213: {'loss': 2.5036, 'grad_norm': 2.166743755340576, 'learning_rate': 2.547127966289643e-05, 'epoch': 1.4723885562208916}\n",
            "Captured loss: 2.5036 at step 2213\n",
            "Logs at step 2214: {'loss': 2.4967, 'grad_norm': 1.7544149160385132, 'learning_rate': 2.5460190729651806e-05, 'epoch': 1.473053892215569}\n",
            "Captured loss: 2.4967 at step 2214\n",
            "Logs at step 2215: {'loss': 2.6391, 'grad_norm': 1.3333333730697632, 'learning_rate': 2.5449101796407187e-05, 'epoch': 1.4737192282102463}\n",
            "Captured loss: 2.6391 at step 2215\n",
            "Logs at step 2216: {'loss': 2.3119, 'grad_norm': 1.2096892595291138, 'learning_rate': 2.5438012863162564e-05, 'epoch': 1.4743845642049234}\n",
            "Captured loss: 2.3119 at step 2216\n",
            "Logs at step 2217: {'loss': 2.5469, 'grad_norm': 1.081222653388977, 'learning_rate': 2.5426923929917945e-05, 'epoch': 1.475049900199601}\n",
            "Captured loss: 2.5469 at step 2217\n",
            "Logs at step 2218: {'loss': 2.6116, 'grad_norm': 2.01971697807312, 'learning_rate': 2.5415834996673323e-05, 'epoch': 1.475715236194278}\n",
            "Captured loss: 2.6116 at step 2218\n",
            "Logs at step 2219: {'loss': 2.4941, 'grad_norm': 1.4844197034835815, 'learning_rate': 2.5404746063428697e-05, 'epoch': 1.4763805721889554}\n",
            "Captured loss: 2.4941 at step 2219\n",
            "Logs at step 2220: {'loss': 2.4603, 'grad_norm': 1.4126814603805542, 'learning_rate': 2.5393657130184078e-05, 'epoch': 1.4770459081836327}\n",
            "Captured loss: 2.4603 at step 2220\n",
            "Logs at step 2221: {'loss': 2.6164, 'grad_norm': 1.8702377080917358, 'learning_rate': 2.5382568196939455e-05, 'epoch': 1.47771124417831}\n",
            "Captured loss: 2.6164 at step 2221\n",
            "Logs at step 2222: {'loss': 2.6574, 'grad_norm': 1.2445027828216553, 'learning_rate': 2.5371479263694836e-05, 'epoch': 1.4783765801729873}\n",
            "Captured loss: 2.6574 at step 2222\n",
            "Logs at step 2223: {'loss': 2.3234, 'grad_norm': 1.5355994701385498, 'learning_rate': 2.5360390330450214e-05, 'epoch': 1.4790419161676647}\n",
            "Captured loss: 2.3234 at step 2223\n",
            "Logs at step 2224: {'loss': 2.3851, 'grad_norm': 1.1928448677062988, 'learning_rate': 2.5349301397205588e-05, 'epoch': 1.479707252162342}\n",
            "Captured loss: 2.3851 at step 2224\n",
            "Logs at step 2225: {'loss': 2.4973, 'grad_norm': 1.0054627656936646, 'learning_rate': 2.533821246396097e-05, 'epoch': 1.4803725881570193}\n",
            "Captured loss: 2.4973 at step 2225\n",
            "Logs at step 2226: {'loss': 2.5295, 'grad_norm': 1.0716396570205688, 'learning_rate': 2.5327123530716347e-05, 'epoch': 1.4810379241516967}\n",
            "Captured loss: 2.5295 at step 2226\n",
            "Logs at step 2227: {'loss': 2.5232, 'grad_norm': 1.6223256587982178, 'learning_rate': 2.531603459747172e-05, 'epoch': 1.481703260146374}\n",
            "Captured loss: 2.5232 at step 2227\n",
            "Logs at step 2228: {'loss': 2.4624, 'grad_norm': 1.7508221864700317, 'learning_rate': 2.53049456642271e-05, 'epoch': 1.4823685961410513}\n",
            "Captured loss: 2.4624 at step 2228\n",
            "Logs at step 2229: {'loss': 2.3838, 'grad_norm': 2.0640716552734375, 'learning_rate': 2.529385673098248e-05, 'epoch': 1.4830339321357284}\n",
            "Captured loss: 2.3838 at step 2229\n",
            "Logs at step 2230: {'loss': 2.6382, 'grad_norm': 1.0138996839523315, 'learning_rate': 2.528276779773786e-05, 'epoch': 1.483699268130406}\n",
            "Captured loss: 2.6382 at step 2230\n",
            "Logs at step 2231: {'loss': 2.5915, 'grad_norm': 1.3324145078659058, 'learning_rate': 2.5271678864493238e-05, 'epoch': 1.484364604125083}\n",
            "Captured loss: 2.5915 at step 2231\n",
            "Logs at step 2232: {'loss': 2.2431, 'grad_norm': 1.8212202787399292, 'learning_rate': 2.5260589931248612e-05, 'epoch': 1.4850299401197604}\n",
            "Captured loss: 2.2431 at step 2232\n",
            "Logs at step 2233: {'loss': 2.6096, 'grad_norm': 1.5296940803527832, 'learning_rate': 2.5249500998003993e-05, 'epoch': 1.4856952761144377}\n",
            "Captured loss: 2.6096 at step 2233\n",
            "Logs at step 2234: {'loss': 2.5717, 'grad_norm': 1.4142876863479614, 'learning_rate': 2.523841206475937e-05, 'epoch': 1.486360612109115}\n",
            "Captured loss: 2.5717 at step 2234\n",
            "Logs at step 2235: {'loss': 2.3776, 'grad_norm': 1.6649316549301147, 'learning_rate': 2.522732313151475e-05, 'epoch': 1.4870259481037924}\n",
            "Captured loss: 2.3776 at step 2235\n",
            "Logs at step 2236: {'loss': 2.6577, 'grad_norm': 1.2681041955947876, 'learning_rate': 2.521623419827013e-05, 'epoch': 1.4876912840984697}\n",
            "Captured loss: 2.6577 at step 2236\n",
            "Logs at step 2237: {'loss': 2.5366, 'grad_norm': 1.9819461107254028, 'learning_rate': 2.5205145265025503e-05, 'epoch': 1.488356620093147}\n",
            "Captured loss: 2.5366 at step 2237\n",
            "Logs at step 2238: {'loss': 2.4781, 'grad_norm': 1.0311269760131836, 'learning_rate': 2.5194056331780884e-05, 'epoch': 1.4890219560878244}\n",
            "Captured loss: 2.4781 at step 2238\n",
            "Logs at step 2239: {'loss': 2.6174, 'grad_norm': 1.252429723739624, 'learning_rate': 2.518296739853626e-05, 'epoch': 1.4896872920825017}\n",
            "Captured loss: 2.6174 at step 2239\n",
            "Logs at step 2240: {'loss': 2.2488, 'grad_norm': 1.0825588703155518, 'learning_rate': 2.5171878465291642e-05, 'epoch': 1.490352628077179}\n",
            "Captured loss: 2.2488 at step 2240\n",
            "Logs at step 2241: {'loss': 2.4842, 'grad_norm': 1.582281231880188, 'learning_rate': 2.516078953204702e-05, 'epoch': 1.4910179640718564}\n",
            "Captured loss: 2.4842 at step 2241\n",
            "Logs at step 2242: {'loss': 2.6651, 'grad_norm': 1.6869189739227295, 'learning_rate': 2.5149700598802394e-05, 'epoch': 1.4916833000665335}\n",
            "Captured loss: 2.6651 at step 2242\n",
            "Logs at step 2243: {'loss': 2.5336, 'grad_norm': 1.0549838542938232, 'learning_rate': 2.5138611665557775e-05, 'epoch': 1.492348636061211}\n",
            "Captured loss: 2.5336 at step 2243\n",
            "Logs at step 2244: {'loss': 2.4774, 'grad_norm': 1.3867486715316772, 'learning_rate': 2.5127522732313153e-05, 'epoch': 1.4930139720558881}\n",
            "Captured loss: 2.4774 at step 2244\n",
            "Logs at step 2245: {'loss': 2.4087, 'grad_norm': 1.5302919149398804, 'learning_rate': 2.5116433799068527e-05, 'epoch': 1.4936793080505655}\n",
            "Captured loss: 2.4087 at step 2245\n",
            "Logs at step 2246: {'loss': 2.5681, 'grad_norm': 1.757666826248169, 'learning_rate': 2.510534486582391e-05, 'epoch': 1.4943446440452428}\n",
            "Captured loss: 2.5681 at step 2246\n",
            "Logs at step 2247: {'loss': 2.4884, 'grad_norm': 1.1083927154541016, 'learning_rate': 2.5094255932579285e-05, 'epoch': 1.4950099800399201}\n",
            "Captured loss: 2.4884 at step 2247\n",
            "Logs at step 2248: {'loss': 2.5635, 'grad_norm': 1.3944404125213623, 'learning_rate': 2.5083166999334666e-05, 'epoch': 1.4956753160345975}\n",
            "Captured loss: 2.5635 at step 2248\n",
            "Logs at step 2249: {'loss': 2.2676, 'grad_norm': 1.226966142654419, 'learning_rate': 2.5072078066090044e-05, 'epoch': 1.4963406520292748}\n",
            "Captured loss: 2.2676 at step 2249\n",
            "Logs at step 2250: {'loss': 2.4172, 'grad_norm': 1.5580732822418213, 'learning_rate': 2.5060989132845418e-05, 'epoch': 1.4970059880239521}\n",
            "Captured loss: 2.4172 at step 2250\n",
            "Logs at step 2251: {'loss': 2.617, 'grad_norm': 0.964500367641449, 'learning_rate': 2.5049900199600802e-05, 'epoch': 1.4976713240186295}\n",
            "Captured loss: 2.617 at step 2251\n",
            "Logs at step 2252: {'loss': 2.4511, 'grad_norm': 1.3547557592391968, 'learning_rate': 2.5038811266356176e-05, 'epoch': 1.4983366600133068}\n",
            "Captured loss: 2.4511 at step 2252\n",
            "Logs at step 2253: {'loss': 2.3347, 'grad_norm': 1.712722659111023, 'learning_rate': 2.5027722333111557e-05, 'epoch': 1.499001996007984}\n",
            "Captured loss: 2.3347 at step 2253\n",
            "Logs at step 2254: {'loss': 2.4553, 'grad_norm': 1.4087798595428467, 'learning_rate': 2.5016633399866935e-05, 'epoch': 1.4996673320026614}\n",
            "Captured loss: 2.4553 at step 2254\n",
            "Logs at step 2255: {'loss': 2.5292, 'grad_norm': 1.313065767288208, 'learning_rate': 2.500554446662231e-05, 'epoch': 1.5003326679973386}\n",
            "Captured loss: 2.5292 at step 2255\n",
            "Logs at step 2256: {'loss': 2.1888, 'grad_norm': 1.3314388990402222, 'learning_rate': 2.499445553337769e-05, 'epoch': 1.500998003992016}\n",
            "Captured loss: 2.1888 at step 2256\n",
            "Logs at step 2257: {'loss': 2.4539, 'grad_norm': 1.3035837411880493, 'learning_rate': 2.4983366600133068e-05, 'epoch': 1.5016633399866932}\n",
            "Captured loss: 2.4539 at step 2257\n",
            "Logs at step 2258: {'loss': 2.4626, 'grad_norm': 1.0008363723754883, 'learning_rate': 2.4972277666888445e-05, 'epoch': 1.5023286759813705}\n",
            "Captured loss: 2.4626 at step 2258\n",
            "Logs at step 2259: {'loss': 2.3226, 'grad_norm': 1.525248408317566, 'learning_rate': 2.4961188733643826e-05, 'epoch': 1.5029940119760479}\n",
            "Captured loss: 2.3226 at step 2259\n",
            "Logs at step 2260: {'loss': 2.6159, 'grad_norm': 1.5788724422454834, 'learning_rate': 2.49500998003992e-05, 'epoch': 1.5036593479707252}\n",
            "Captured loss: 2.6159 at step 2260\n",
            "Logs at step 2261: {'loss': 2.6572, 'grad_norm': 1.2026176452636719, 'learning_rate': 2.493901086715458e-05, 'epoch': 1.5043246839654025}\n",
            "Captured loss: 2.6572 at step 2261\n",
            "Logs at step 2262: {'loss': 2.5579, 'grad_norm': 1.2588948011398315, 'learning_rate': 2.492792193390996e-05, 'epoch': 1.5049900199600799}\n",
            "Captured loss: 2.5579 at step 2262\n",
            "Logs at step 2263: {'loss': 2.5522, 'grad_norm': 1.1478540897369385, 'learning_rate': 2.4916833000665336e-05, 'epoch': 1.5056553559547572}\n",
            "Captured loss: 2.5522 at step 2263\n",
            "Logs at step 2264: {'loss': 2.6609, 'grad_norm': 1.2692052125930786, 'learning_rate': 2.4905744067420717e-05, 'epoch': 1.5063206919494343}\n",
            "Captured loss: 2.6609 at step 2264\n",
            "Logs at step 2265: {'loss': 2.534, 'grad_norm': 1.154489517211914, 'learning_rate': 2.489465513417609e-05, 'epoch': 1.5069860279441119}\n",
            "Captured loss: 2.534 at step 2265\n",
            "Logs at step 2266: {'loss': 2.5624, 'grad_norm': 1.1572237014770508, 'learning_rate': 2.4883566200931472e-05, 'epoch': 1.507651363938789}\n",
            "Captured loss: 2.5624 at step 2266\n",
            "Logs at step 2267: {'loss': 2.4815, 'grad_norm': 0.9820506572723389, 'learning_rate': 2.487247726768685e-05, 'epoch': 1.5083166999334665}\n",
            "Captured loss: 2.4815 at step 2267\n",
            "Logs at step 2268: {'loss': 2.5403, 'grad_norm': 1.3501391410827637, 'learning_rate': 2.4861388334442227e-05, 'epoch': 1.5089820359281436}\n",
            "Captured loss: 2.5403 at step 2268\n",
            "Logs at step 2269: {'loss': 2.6086, 'grad_norm': 1.3969069719314575, 'learning_rate': 2.4850299401197605e-05, 'epoch': 1.5096473719228212}\n",
            "Captured loss: 2.6086 at step 2269\n",
            "Logs at step 2270: {'loss': 2.5692, 'grad_norm': 1.3647973537445068, 'learning_rate': 2.4839210467952982e-05, 'epoch': 1.5103127079174983}\n",
            "Captured loss: 2.5692 at step 2270\n",
            "Logs at step 2271: {'loss': 2.6119, 'grad_norm': 1.0728046894073486, 'learning_rate': 2.4828121534708363e-05, 'epoch': 1.5109780439121756}\n",
            "Captured loss: 2.6119 at step 2271\n",
            "Logs at step 2272: {'loss': 2.4746, 'grad_norm': 1.5701050758361816, 'learning_rate': 2.481703260146374e-05, 'epoch': 1.511643379906853}\n",
            "Captured loss: 2.4746 at step 2272\n",
            "Logs at step 2273: {'loss': 2.5556, 'grad_norm': 1.0762369632720947, 'learning_rate': 2.480594366821912e-05, 'epoch': 1.5123087159015303}\n",
            "Captured loss: 2.5556 at step 2273\n",
            "Logs at step 2274: {'loss': 2.4293, 'grad_norm': 1.2952163219451904, 'learning_rate': 2.4794854734974496e-05, 'epoch': 1.5129740518962076}\n",
            "Captured loss: 2.4293 at step 2274\n",
            "Logs at step 2275: {'loss': 2.5776, 'grad_norm': 1.089585781097412, 'learning_rate': 2.4783765801729874e-05, 'epoch': 1.513639387890885}\n",
            "Captured loss: 2.5776 at step 2275\n",
            "Logs at step 2276: {'loss': 2.3781, 'grad_norm': 1.4700480699539185, 'learning_rate': 2.4772676868485255e-05, 'epoch': 1.5143047238855623}\n",
            "Captured loss: 2.3781 at step 2276\n",
            "Logs at step 2277: {'loss': 2.5081, 'grad_norm': 1.4926199913024902, 'learning_rate': 2.4761587935240632e-05, 'epoch': 1.5149700598802394}\n",
            "Captured loss: 2.5081 at step 2277\n",
            "Logs at step 2278: {'loss': 2.2999, 'grad_norm': 1.793442726135254, 'learning_rate': 2.475049900199601e-05, 'epoch': 1.515635395874917}\n",
            "Captured loss: 2.2999 at step 2278\n",
            "Logs at step 2279: {'loss': 2.5968, 'grad_norm': 1.064849853515625, 'learning_rate': 2.4739410068751387e-05, 'epoch': 1.516300731869594}\n",
            "Captured loss: 2.5968 at step 2279\n",
            "Logs at step 2280: {'loss': 2.4754, 'grad_norm': 1.4749019145965576, 'learning_rate': 2.4728321135506765e-05, 'epoch': 1.5169660678642716}\n",
            "Captured loss: 2.4754 at step 2280\n",
            "Logs at step 2281: {'loss': 2.494, 'grad_norm': 1.4274214506149292, 'learning_rate': 2.4717232202262146e-05, 'epoch': 1.5176314038589487}\n",
            "Captured loss: 2.494 at step 2281\n",
            "Logs at step 2282: {'loss': 2.6443, 'grad_norm': 1.1271129846572876, 'learning_rate': 2.470614326901752e-05, 'epoch': 1.5182967398536262}\n",
            "Captured loss: 2.6443 at step 2282\n",
            "Logs at step 2283: {'loss': 2.2908, 'grad_norm': 1.3548099994659424, 'learning_rate': 2.46950543357729e-05, 'epoch': 1.5189620758483033}\n",
            "Captured loss: 2.2908 at step 2283\n",
            "Logs at step 2284: {'loss': 2.5063, 'grad_norm': 1.3034710884094238, 'learning_rate': 2.468396540252828e-05, 'epoch': 1.5196274118429807}\n",
            "Captured loss: 2.5063 at step 2284\n",
            "Logs at step 2285: {'loss': 2.616, 'grad_norm': 1.1665809154510498, 'learning_rate': 2.4672876469283656e-05, 'epoch': 1.520292747837658}\n",
            "Captured loss: 2.616 at step 2285\n",
            "Logs at step 2286: {'loss': 2.5089, 'grad_norm': 1.2681736946105957, 'learning_rate': 2.4661787536039037e-05, 'epoch': 1.5209580838323353}\n",
            "Captured loss: 2.5089 at step 2286\n",
            "Logs at step 2287: {'loss': 2.5594, 'grad_norm': 1.3601068258285522, 'learning_rate': 2.465069860279441e-05, 'epoch': 1.5216234198270127}\n",
            "Captured loss: 2.5594 at step 2287\n",
            "Logs at step 2288: {'loss': 2.4821, 'grad_norm': 1.130635142326355, 'learning_rate': 2.463960966954979e-05, 'epoch': 1.52228875582169}\n",
            "Captured loss: 2.4821 at step 2288\n",
            "Logs at step 2289: {'loss': 2.4931, 'grad_norm': 1.0543428659439087, 'learning_rate': 2.462852073630517e-05, 'epoch': 1.5229540918163673}\n",
            "Captured loss: 2.4931 at step 2289\n",
            "Logs at step 2290: {'loss': 2.557, 'grad_norm': 1.4102717638015747, 'learning_rate': 2.4617431803060547e-05, 'epoch': 1.5236194278110444}\n",
            "Captured loss: 2.557 at step 2290\n",
            "Logs at step 2291: {'loss': 2.5453, 'grad_norm': 1.1124660968780518, 'learning_rate': 2.4606342869815925e-05, 'epoch': 1.524284763805722}\n",
            "Captured loss: 2.5453 at step 2291\n",
            "Logs at step 2292: {'loss': 2.3704, 'grad_norm': 1.2417470216751099, 'learning_rate': 2.4595253936571302e-05, 'epoch': 1.524950099800399}\n",
            "Captured loss: 2.3704 at step 2292\n",
            "Logs at step 2293: {'loss': 2.3789, 'grad_norm': 1.5782287120819092, 'learning_rate': 2.458416500332668e-05, 'epoch': 1.5256154357950766}\n",
            "Captured loss: 2.3789 at step 2293\n",
            "Logs at step 2294: {'loss': 2.2436, 'grad_norm': 1.301297903060913, 'learning_rate': 2.457307607008206e-05, 'epoch': 1.5262807717897537}\n",
            "Captured loss: 2.2436 at step 2294\n",
            "Logs at step 2295: {'loss': 2.545, 'grad_norm': 1.3262877464294434, 'learning_rate': 2.4561987136837438e-05, 'epoch': 1.5269461077844313}\n",
            "Captured loss: 2.545 at step 2295\n",
            "Logs at step 2296: {'loss': 2.6795, 'grad_norm': 1.5943019390106201, 'learning_rate': 2.4550898203592816e-05, 'epoch': 1.5276114437791084}\n",
            "Captured loss: 2.6795 at step 2296\n",
            "Logs at step 2297: {'loss': 2.275, 'grad_norm': 1.313723087310791, 'learning_rate': 2.4539809270348193e-05, 'epoch': 1.5282767797737857}\n",
            "Captured loss: 2.275 at step 2297\n",
            "Logs at step 2298: {'loss': 2.5191, 'grad_norm': 1.2287713289260864, 'learning_rate': 2.452872033710357e-05, 'epoch': 1.528942115768463}\n",
            "Captured loss: 2.5191 at step 2298\n",
            "Logs at step 2299: {'loss': 2.5952, 'grad_norm': 1.5942621231079102, 'learning_rate': 2.4517631403858952e-05, 'epoch': 1.5296074517631404}\n",
            "Captured loss: 2.5952 at step 2299\n",
            "Logs at step 2300: {'loss': 2.6638, 'grad_norm': 1.2546049356460571, 'learning_rate': 2.4506542470614326e-05, 'epoch': 1.5302727877578177}\n",
            "Captured loss: 2.6638 at step 2300\n",
            "Logs at step 2301: {'loss': 2.456, 'grad_norm': 1.0831031799316406, 'learning_rate': 2.4495453537369707e-05, 'epoch': 1.530938123752495}\n",
            "Captured loss: 2.456 at step 2301\n",
            "Logs at step 2302: {'loss': 2.3543, 'grad_norm': 1.7991523742675781, 'learning_rate': 2.4484364604125084e-05, 'epoch': 1.5316034597471724}\n",
            "Captured loss: 2.3543 at step 2302\n",
            "Logs at step 2303: {'loss': 2.4889, 'grad_norm': 1.38920259475708, 'learning_rate': 2.4473275670880462e-05, 'epoch': 1.5322687957418495}\n",
            "Captured loss: 2.4889 at step 2303\n",
            "Logs at step 2304: {'loss': 2.4125, 'grad_norm': 1.2693451642990112, 'learning_rate': 2.446218673763584e-05, 'epoch': 1.532934131736527}\n",
            "Captured loss: 2.4125 at step 2304\n",
            "Logs at step 2305: {'loss': 2.4965, 'grad_norm': 1.1172899007797241, 'learning_rate': 2.4451097804391217e-05, 'epoch': 1.5335994677312041}\n",
            "Captured loss: 2.4965 at step 2305\n",
            "Logs at step 2306: {'loss': 2.6508, 'grad_norm': 1.2581208944320679, 'learning_rate': 2.4440008871146598e-05, 'epoch': 1.5342648037258817}\n",
            "Captured loss: 2.6508 at step 2306\n",
            "Logs at step 2307: {'loss': 2.5033, 'grad_norm': 0.936021089553833, 'learning_rate': 2.4428919937901976e-05, 'epoch': 1.5349301397205588}\n",
            "Captured loss: 2.5033 at step 2307\n",
            "Logs at step 2308: {'loss': 2.5333, 'grad_norm': 1.2454684972763062, 'learning_rate': 2.4417831004657353e-05, 'epoch': 1.5355954757152364}\n",
            "Captured loss: 2.5333 at step 2308\n",
            "Logs at step 2309: {'loss': 2.5216, 'grad_norm': 1.2288635969161987, 'learning_rate': 2.440674207141273e-05, 'epoch': 1.5362608117099135}\n",
            "Captured loss: 2.5216 at step 2309\n",
            "Logs at step 2310: {'loss': 2.5289, 'grad_norm': 1.5116149187088013, 'learning_rate': 2.4395653138168108e-05, 'epoch': 1.5369261477045908}\n",
            "Captured loss: 2.5289 at step 2310\n",
            "Logs at step 2311: {'loss': 2.621, 'grad_norm': 1.457662582397461, 'learning_rate': 2.438456420492349e-05, 'epoch': 1.5375914836992681}\n",
            "Captured loss: 2.621 at step 2311\n",
            "Logs at step 2312: {'loss': 2.3221, 'grad_norm': 1.3476203680038452, 'learning_rate': 2.4373475271678867e-05, 'epoch': 1.5382568196939455}\n",
            "Captured loss: 2.3221 at step 2312\n",
            "Logs at step 2313: {'loss': 2.5786, 'grad_norm': 1.0834318399429321, 'learning_rate': 2.4362386338434244e-05, 'epoch': 1.5389221556886228}\n",
            "Captured loss: 2.5786 at step 2313\n",
            "Logs at step 2314: {'loss': 2.7257, 'grad_norm': 2.516868829727173, 'learning_rate': 2.4351297405189622e-05, 'epoch': 1.5395874916833001}\n",
            "Captured loss: 2.7257 at step 2314\n",
            "Logs at step 2315: {'loss': 2.3559, 'grad_norm': 1.2720348834991455, 'learning_rate': 2.4340208471945e-05, 'epoch': 1.5402528276779774}\n",
            "Captured loss: 2.3559 at step 2315\n",
            "Logs at step 2316: {'loss': 2.6084, 'grad_norm': 1.3413903713226318, 'learning_rate': 2.432911953870038e-05, 'epoch': 1.5409181636726546}\n",
            "Captured loss: 2.6084 at step 2316\n",
            "Logs at step 2317: {'loss': 2.5858, 'grad_norm': 1.0080657005310059, 'learning_rate': 2.4318030605455754e-05, 'epoch': 1.541583499667332}\n",
            "Captured loss: 2.5858 at step 2317\n",
            "Logs at step 2318: {'loss': 2.3208, 'grad_norm': 1.3688596487045288, 'learning_rate': 2.4306941672211135e-05, 'epoch': 1.5422488356620092}\n",
            "Captured loss: 2.3208 at step 2318\n",
            "Logs at step 2319: {'loss': 2.578, 'grad_norm': 2.160064935684204, 'learning_rate': 2.4295852738966513e-05, 'epoch': 1.5429141716566868}\n",
            "Captured loss: 2.578 at step 2319\n",
            "Logs at step 2320: {'loss': 2.6393, 'grad_norm': 1.864221215248108, 'learning_rate': 2.428476380572189e-05, 'epoch': 1.5435795076513639}\n",
            "Captured loss: 2.6393 at step 2320\n",
            "Logs at step 2321: {'loss': 2.4636, 'grad_norm': 1.5793218612670898, 'learning_rate': 2.427367487247727e-05, 'epoch': 1.5442448436460412}\n",
            "Captured loss: 2.4636 at step 2321\n",
            "Logs at step 2322: {'loss': 2.3664, 'grad_norm': 1.269386887550354, 'learning_rate': 2.4262585939232646e-05, 'epoch': 1.5449101796407185}\n",
            "Captured loss: 2.3664 at step 2322\n",
            "Logs at step 2323: {'loss': 2.5909, 'grad_norm': 1.2918310165405273, 'learning_rate': 2.4251497005988023e-05, 'epoch': 1.5455755156353959}\n",
            "Captured loss: 2.5909 at step 2323\n",
            "Logs at step 2324: {'loss': 2.3827, 'grad_norm': 1.227895975112915, 'learning_rate': 2.4240408072743404e-05, 'epoch': 1.5462408516300732}\n",
            "Captured loss: 2.3827 at step 2324\n",
            "Logs at step 2325: {'loss': 2.6401, 'grad_norm': 1.2723886966705322, 'learning_rate': 2.422931913949878e-05, 'epoch': 1.5469061876247505}\n",
            "Captured loss: 2.6401 at step 2325\n",
            "Logs at step 2326: {'loss': 2.514, 'grad_norm': 1.0681567192077637, 'learning_rate': 2.421823020625416e-05, 'epoch': 1.5475715236194278}\n",
            "Captured loss: 2.514 at step 2326\n",
            "Logs at step 2327: {'loss': 2.244, 'grad_norm': 1.8576072454452515, 'learning_rate': 2.4207141273009537e-05, 'epoch': 1.5482368596141052}\n",
            "Captured loss: 2.244 at step 2327\n",
            "Logs at step 2328: {'loss': 2.5304, 'grad_norm': 1.0555752515792847, 'learning_rate': 2.4196052339764914e-05, 'epoch': 1.5489021956087825}\n",
            "Captured loss: 2.5304 at step 2328\n",
            "Logs at step 2329: {'loss': 2.5614, 'grad_norm': 1.362355351448059, 'learning_rate': 2.4184963406520295e-05, 'epoch': 1.5495675316034596}\n",
            "Captured loss: 2.5614 at step 2329\n",
            "Logs at step 2330: {'loss': 2.8409, 'grad_norm': 1.5309938192367554, 'learning_rate': 2.4173874473275673e-05, 'epoch': 1.5502328675981372}\n",
            "Captured loss: 2.8409 at step 2330\n",
            "Logs at step 2331: {'loss': 2.6758, 'grad_norm': 1.5040647983551025, 'learning_rate': 2.416278554003105e-05, 'epoch': 1.5508982035928143}\n",
            "Captured loss: 2.6758 at step 2331\n",
            "Logs at step 2332: {'loss': 2.5875, 'grad_norm': 1.672633409500122, 'learning_rate': 2.4151696606786428e-05, 'epoch': 1.5515635395874918}\n",
            "Captured loss: 2.5875 at step 2332\n",
            "Logs at step 2333: {'loss': 2.3756, 'grad_norm': 1.416473150253296, 'learning_rate': 2.4140607673541805e-05, 'epoch': 1.552228875582169}\n",
            "Captured loss: 2.3756 at step 2333\n",
            "Logs at step 2334: {'loss': 2.2515, 'grad_norm': 1.4135717153549194, 'learning_rate': 2.4129518740297186e-05, 'epoch': 1.5528942115768463}\n",
            "Captured loss: 2.2515 at step 2334\n",
            "Logs at step 2335: {'loss': 2.4683, 'grad_norm': 1.155139684677124, 'learning_rate': 2.411842980705256e-05, 'epoch': 1.5535595475715236}\n",
            "Captured loss: 2.4683 at step 2335\n",
            "Logs at step 2336: {'loss': 2.2776, 'grad_norm': 1.1912182569503784, 'learning_rate': 2.410734087380794e-05, 'epoch': 1.554224883566201}\n",
            "Captured loss: 2.2776 at step 2336\n",
            "Logs at step 2337: {'loss': 2.7833, 'grad_norm': 1.674281120300293, 'learning_rate': 2.409625194056332e-05, 'epoch': 1.5548902195608783}\n",
            "Captured loss: 2.7833 at step 2337\n",
            "Logs at step 2338: {'loss': 2.4143, 'grad_norm': 1.3289973735809326, 'learning_rate': 2.4085163007318697e-05, 'epoch': 1.5555555555555556}\n",
            "Captured loss: 2.4143 at step 2338\n",
            "Logs at step 2339: {'loss': 2.3519, 'grad_norm': 1.359592318534851, 'learning_rate': 2.4074074074074074e-05, 'epoch': 1.556220891550233}\n",
            "Captured loss: 2.3519 at step 2339\n",
            "Logs at step 2340: {'loss': 2.5533, 'grad_norm': 1.4093844890594482, 'learning_rate': 2.406298514082945e-05, 'epoch': 1.55688622754491}\n",
            "Captured loss: 2.5533 at step 2340\n",
            "Logs at step 2341: {'loss': 2.6316, 'grad_norm': 1.5951017141342163, 'learning_rate': 2.4051896207584833e-05, 'epoch': 1.5575515635395876}\n",
            "Captured loss: 2.6316 at step 2341\n",
            "Logs at step 2342: {'loss': 2.5242, 'grad_norm': 1.8842260837554932, 'learning_rate': 2.404080727434021e-05, 'epoch': 1.5582168995342647}\n",
            "Captured loss: 2.5242 at step 2342\n",
            "Logs at step 2343: {'loss': 2.6196, 'grad_norm': 1.6694444417953491, 'learning_rate': 2.4029718341095588e-05, 'epoch': 1.5588822355289422}\n",
            "Captured loss: 2.6196 at step 2343\n",
            "Logs at step 2344: {'loss': 2.5279, 'grad_norm': 1.0336817502975464, 'learning_rate': 2.4018629407850965e-05, 'epoch': 1.5595475715236193}\n",
            "Captured loss: 2.5279 at step 2344\n",
            "Logs at step 2345: {'loss': 2.5321, 'grad_norm': 0.930427610874176, 'learning_rate': 2.4007540474606343e-05, 'epoch': 1.5602129075182969}\n",
            "Captured loss: 2.5321 at step 2345\n",
            "Logs at step 2346: {'loss': 2.5063, 'grad_norm': 1.246117353439331, 'learning_rate': 2.3996451541361724e-05, 'epoch': 1.560878243512974}\n",
            "Captured loss: 2.5063 at step 2346\n",
            "Logs at step 2347: {'loss': 2.538, 'grad_norm': 2.231320858001709, 'learning_rate': 2.39853626081171e-05, 'epoch': 1.5615435795076513}\n",
            "Captured loss: 2.538 at step 2347\n",
            "Logs at step 2348: {'loss': 2.4456, 'grad_norm': 1.1831239461898804, 'learning_rate': 2.397427367487248e-05, 'epoch': 1.5622089155023287}\n",
            "Captured loss: 2.4456 at step 2348\n",
            "Logs at step 2349: {'loss': 2.2695, 'grad_norm': 1.4032261371612549, 'learning_rate': 2.3963184741627856e-05, 'epoch': 1.562874251497006}\n",
            "Captured loss: 2.2695 at step 2349\n",
            "Logs at step 2350: {'loss': 2.4807, 'grad_norm': 1.6682307720184326, 'learning_rate': 2.3952095808383234e-05, 'epoch': 1.5635395874916833}\n",
            "Captured loss: 2.4807 at step 2350\n",
            "Logs at step 2351: {'loss': 2.6049, 'grad_norm': 1.2480577230453491, 'learning_rate': 2.3941006875138615e-05, 'epoch': 1.5642049234863606}\n",
            "Captured loss: 2.6049 at step 2351\n",
            "Logs at step 2352: {'loss': 2.3845, 'grad_norm': 2.1044178009033203, 'learning_rate': 2.3929917941893992e-05, 'epoch': 1.564870259481038}\n",
            "Captured loss: 2.3845 at step 2352\n",
            "Logs at step 2353: {'loss': 2.5779, 'grad_norm': 1.041412353515625, 'learning_rate': 2.3918829008649367e-05, 'epoch': 1.565535595475715}\n",
            "Captured loss: 2.5779 at step 2353\n",
            "Logs at step 2354: {'loss': 2.4795, 'grad_norm': 1.3286288976669312, 'learning_rate': 2.3907740075404748e-05, 'epoch': 1.5662009314703926}\n",
            "Captured loss: 2.4795 at step 2354\n",
            "Logs at step 2355: {'loss': 2.2149, 'grad_norm': 1.6776375770568848, 'learning_rate': 2.3896651142160125e-05, 'epoch': 1.5668662674650697}\n",
            "Captured loss: 2.2149 at step 2355\n",
            "Logs at step 2356: {'loss': 2.342, 'grad_norm': 1.2957333326339722, 'learning_rate': 2.3885562208915506e-05, 'epoch': 1.5675316034597473}\n",
            "Captured loss: 2.342 at step 2356\n",
            "Logs at step 2357: {'loss': 1.9859, 'grad_norm': 1.5368722677230835, 'learning_rate': 2.387447327567088e-05, 'epoch': 1.5681969394544244}\n",
            "Captured loss: 1.9859 at step 2357\n",
            "Logs at step 2358: {'loss': 2.1213, 'grad_norm': 1.6791993379592896, 'learning_rate': 2.3863384342426258e-05, 'epoch': 1.568862275449102}\n",
            "Captured loss: 2.1213 at step 2358\n",
            "Logs at step 2359: {'loss': 2.5307, 'grad_norm': 1.079049825668335, 'learning_rate': 2.385229540918164e-05, 'epoch': 1.569527611443779}\n",
            "Captured loss: 2.5307 at step 2359\n",
            "Logs at step 2360: {'loss': 2.4655, 'grad_norm': 1.5398393869400024, 'learning_rate': 2.3841206475937016e-05, 'epoch': 1.5701929474384564}\n",
            "Captured loss: 2.4655 at step 2360\n",
            "Logs at step 2361: {'loss': 2.5619, 'grad_norm': 1.5776902437210083, 'learning_rate': 2.3830117542692394e-05, 'epoch': 1.5708582834331337}\n",
            "Captured loss: 2.5619 at step 2361\n",
            "Logs at step 2362: {'loss': 2.5314, 'grad_norm': 1.6342741250991821, 'learning_rate': 2.381902860944777e-05, 'epoch': 1.571523619427811}\n",
            "Captured loss: 2.5314 at step 2362\n",
            "Logs at step 2363: {'loss': 2.5465, 'grad_norm': 1.3694968223571777, 'learning_rate': 2.380793967620315e-05, 'epoch': 1.5721889554224884}\n",
            "Captured loss: 2.5465 at step 2363\n",
            "Logs at step 2364: {'loss': 2.5684, 'grad_norm': 1.2361843585968018, 'learning_rate': 2.379685074295853e-05, 'epoch': 1.5728542914171657}\n",
            "Captured loss: 2.5684 at step 2364\n",
            "Logs at step 2365: {'loss': 2.6107, 'grad_norm': 1.0135568380355835, 'learning_rate': 2.3785761809713907e-05, 'epoch': 1.573519627411843}\n",
            "Captured loss: 2.6107 at step 2365\n",
            "Logs at step 2366: {'loss': 2.5141, 'grad_norm': 1.0628067255020142, 'learning_rate': 2.3774672876469285e-05, 'epoch': 1.5741849634065201}\n",
            "Captured loss: 2.5141 at step 2366\n",
            "Logs at step 2367: {'loss': 2.5018, 'grad_norm': 1.4004755020141602, 'learning_rate': 2.3763583943224662e-05, 'epoch': 1.5748502994011977}\n",
            "Captured loss: 2.5018 at step 2367\n",
            "Logs at step 2368: {'loss': 2.5918, 'grad_norm': 1.1272435188293457, 'learning_rate': 2.375249500998004e-05, 'epoch': 1.5755156353958748}\n",
            "Captured loss: 2.5918 at step 2368\n",
            "Logs at step 2369: {'loss': 2.2679, 'grad_norm': 1.1982921361923218, 'learning_rate': 2.374140607673542e-05, 'epoch': 1.5761809713905524}\n",
            "Captured loss: 2.2679 at step 2369\n",
            "Logs at step 2370: {'loss': 2.532, 'grad_norm': 1.1968485116958618, 'learning_rate': 2.3730317143490795e-05, 'epoch': 1.5768463073852295}\n",
            "Captured loss: 2.532 at step 2370\n",
            "Logs at step 2371: {'loss': 2.5223, 'grad_norm': 2.1485612392425537, 'learning_rate': 2.3719228210246176e-05, 'epoch': 1.577511643379907}\n",
            "Captured loss: 2.5223 at step 2371\n",
            "Logs at step 2372: {'loss': 2.5294, 'grad_norm': 1.1016682386398315, 'learning_rate': 2.3708139277001554e-05, 'epoch': 1.5781769793745841}\n",
            "Captured loss: 2.5294 at step 2372\n",
            "Logs at step 2373: {'loss': 2.4212, 'grad_norm': 1.2646076679229736, 'learning_rate': 2.369705034375693e-05, 'epoch': 1.5788423153692615}\n",
            "Captured loss: 2.4212 at step 2373\n",
            "Logs at step 2374: {'loss': 2.6056, 'grad_norm': 1.8119548559188843, 'learning_rate': 2.3685961410512312e-05, 'epoch': 1.5795076513639388}\n",
            "Captured loss: 2.6056 at step 2374\n",
            "Logs at step 2375: {'loss': 2.4292, 'grad_norm': 1.7161245346069336, 'learning_rate': 2.3674872477267686e-05, 'epoch': 1.580172987358616}\n",
            "Captured loss: 2.4292 at step 2375\n",
            "Logs at step 2376: {'loss': 2.5264, 'grad_norm': 1.151486873626709, 'learning_rate': 2.3663783544023067e-05, 'epoch': 1.5808383233532934}\n",
            "Captured loss: 2.5264 at step 2376\n",
            "Logs at step 2377: {'loss': 2.4647, 'grad_norm': 1.0959358215332031, 'learning_rate': 2.3652694610778445e-05, 'epoch': 1.5815036593479708}\n",
            "Captured loss: 2.4647 at step 2377\n",
            "Logs at step 2378: {'loss': 2.0267, 'grad_norm': 1.8923660516738892, 'learning_rate': 2.3641605677533822e-05, 'epoch': 1.582168995342648}\n",
            "Captured loss: 2.0267 at step 2378\n",
            "Logs at step 2379: {'loss': 2.0943, 'grad_norm': 1.2723548412322998, 'learning_rate': 2.36305167442892e-05, 'epoch': 1.5828343313373252}\n",
            "Captured loss: 2.0943 at step 2379\n",
            "Logs at step 2380: {'loss': 2.723, 'grad_norm': 1.415239691734314, 'learning_rate': 2.3619427811044577e-05, 'epoch': 1.5834996673320028}\n",
            "Captured loss: 2.723 at step 2380\n",
            "Logs at step 2381: {'loss': 2.6537, 'grad_norm': 1.705952763557434, 'learning_rate': 2.360833887779996e-05, 'epoch': 1.5841650033266799}\n",
            "Captured loss: 2.6537 at step 2381\n",
            "Logs at step 2382: {'loss': 2.5144, 'grad_norm': 1.104011058807373, 'learning_rate': 2.3597249944555336e-05, 'epoch': 1.5848303393213574}\n",
            "Captured loss: 2.5144 at step 2382\n",
            "Logs at step 2383: {'loss': 2.3964, 'grad_norm': 1.248957633972168, 'learning_rate': 2.3586161011310713e-05, 'epoch': 1.5854956753160345}\n",
            "Captured loss: 2.3964 at step 2383\n",
            "Logs at step 2384: {'loss': 2.7768, 'grad_norm': 1.3873538970947266, 'learning_rate': 2.357507207806609e-05, 'epoch': 1.5861610113107119}\n",
            "Captured loss: 2.7768 at step 2384\n",
            "Logs at step 2385: {'loss': 2.4548, 'grad_norm': 1.2520455121994019, 'learning_rate': 2.356398314482147e-05, 'epoch': 1.5868263473053892}\n",
            "Captured loss: 2.4548 at step 2385\n",
            "Logs at step 2386: {'loss': 2.5171, 'grad_norm': 1.08965003490448, 'learning_rate': 2.355289421157685e-05, 'epoch': 1.5874916833000665}\n",
            "Captured loss: 2.5171 at step 2386\n",
            "Logs at step 2387: {'loss': 2.6392, 'grad_norm': 1.20939302444458, 'learning_rate': 2.3541805278332227e-05, 'epoch': 1.5881570192947438}\n",
            "Captured loss: 2.6392 at step 2387\n",
            "Logs at step 2388: {'loss': 2.5496, 'grad_norm': 1.076995611190796, 'learning_rate': 2.35307163450876e-05, 'epoch': 1.5888223552894212}\n",
            "Captured loss: 2.5496 at step 2388\n",
            "Logs at step 2389: {'loss': 2.2712, 'grad_norm': 2.099062204360962, 'learning_rate': 2.3519627411842982e-05, 'epoch': 1.5894876912840985}\n",
            "Captured loss: 2.2712 at step 2389\n",
            "Logs at step 2390: {'loss': 2.6423, 'grad_norm': 1.130142331123352, 'learning_rate': 2.350853847859836e-05, 'epoch': 1.5901530272787758}\n",
            "Captured loss: 2.6423 at step 2390\n",
            "Logs at step 2391: {'loss': 2.3107, 'grad_norm': 1.5858112573623657, 'learning_rate': 2.349744954535374e-05, 'epoch': 1.5908183632734532}\n",
            "Captured loss: 2.3107 at step 2391\n",
            "Logs at step 2392: {'loss': 2.5602, 'grad_norm': 1.2792584896087646, 'learning_rate': 2.3486360612109115e-05, 'epoch': 1.5914836992681303}\n",
            "Captured loss: 2.5602 at step 2392\n",
            "Logs at step 2393: {'loss': 2.6611, 'grad_norm': 1.8105013370513916, 'learning_rate': 2.3475271678864492e-05, 'epoch': 1.5921490352628078}\n",
            "Captured loss: 2.6611 at step 2393\n",
            "Logs at step 2394: {'loss': 2.6689, 'grad_norm': 1.2514557838439941, 'learning_rate': 2.3464182745619873e-05, 'epoch': 1.592814371257485}\n",
            "Captured loss: 2.6689 at step 2394\n",
            "Logs at step 2395: {'loss': 2.2726, 'grad_norm': 1.2765856981277466, 'learning_rate': 2.345309381237525e-05, 'epoch': 1.5934797072521625}\n",
            "Captured loss: 2.2726 at step 2395\n",
            "Logs at step 2396: {'loss': 2.3037, 'grad_norm': 1.51962149143219, 'learning_rate': 2.344200487913063e-05, 'epoch': 1.5941450432468396}\n",
            "Captured loss: 2.3037 at step 2396\n",
            "Logs at step 2397: {'loss': 2.6101, 'grad_norm': 1.4682672023773193, 'learning_rate': 2.3430915945886006e-05, 'epoch': 1.594810379241517}\n",
            "Captured loss: 2.6101 at step 2397\n",
            "Logs at step 2398: {'loss': 2.6065, 'grad_norm': 1.0232152938842773, 'learning_rate': 2.3419827012641383e-05, 'epoch': 1.5954757152361942}\n",
            "Captured loss: 2.6065 at step 2398\n",
            "Logs at step 2399: {'loss': 2.6499, 'grad_norm': 1.2088342905044556, 'learning_rate': 2.3408738079396764e-05, 'epoch': 1.5961410512308716}\n",
            "Captured loss: 2.6499 at step 2399\n",
            "Logs at step 2400: {'loss': 2.3815, 'grad_norm': 1.4624412059783936, 'learning_rate': 2.3397649146152142e-05, 'epoch': 1.596806387225549}\n",
            "Captured loss: 2.3815 at step 2400\n",
            "Logs at step 2401: {'loss': 2.4726, 'grad_norm': 1.706788420677185, 'learning_rate': 2.338656021290752e-05, 'epoch': 1.5974717232202262}\n",
            "Captured loss: 2.4726 at step 2401\n",
            "Logs at step 2402: {'loss': 2.5607, 'grad_norm': 1.9946213960647583, 'learning_rate': 2.3375471279662897e-05, 'epoch': 1.5981370592149036}\n",
            "Captured loss: 2.5607 at step 2402\n",
            "Logs at step 2403: {'loss': 2.0164, 'grad_norm': 1.4188083410263062, 'learning_rate': 2.3364382346418275e-05, 'epoch': 1.5988023952095807}\n",
            "Captured loss: 2.0164 at step 2403\n",
            "Logs at step 2404: {'loss': 2.5035, 'grad_norm': 1.1800494194030762, 'learning_rate': 2.3353293413173656e-05, 'epoch': 1.5994677312042582}\n",
            "Captured loss: 2.5035 at step 2404\n",
            "Logs at step 2405: {'loss': 2.6765, 'grad_norm': 1.1385552883148193, 'learning_rate': 2.334220447992903e-05, 'epoch': 1.6001330671989353}\n",
            "Captured loss: 2.6765 at step 2405\n",
            "Logs at step 2406: {'loss': 2.3056, 'grad_norm': 1.2879890203475952, 'learning_rate': 2.333111554668441e-05, 'epoch': 1.6007984031936129}\n",
            "Captured loss: 2.3056 at step 2406\n",
            "Logs at step 2407: {'loss': 2.2216, 'grad_norm': 1.3267678022384644, 'learning_rate': 2.3320026613439788e-05, 'epoch': 1.60146373918829}\n",
            "Captured loss: 2.2216 at step 2407\n",
            "Logs at step 2408: {'loss': 2.5275, 'grad_norm': 1.1555918455123901, 'learning_rate': 2.3308937680195166e-05, 'epoch': 1.6021290751829675}\n",
            "Captured loss: 2.5275 at step 2408\n",
            "Logs at step 2409: {'loss': 2.5576, 'grad_norm': 1.0972720384597778, 'learning_rate': 2.3297848746950547e-05, 'epoch': 1.6027944111776447}\n",
            "Captured loss: 2.5576 at step 2409\n",
            "Logs at step 2410: {'loss': 2.2536, 'grad_norm': 1.9249303340911865, 'learning_rate': 2.328675981370592e-05, 'epoch': 1.603459747172322}\n",
            "Captured loss: 2.2536 at step 2410\n",
            "Logs at step 2411: {'loss': 2.5275, 'grad_norm': 1.3049317598342896, 'learning_rate': 2.3275670880461302e-05, 'epoch': 1.6041250831669993}\n",
            "Captured loss: 2.5275 at step 2411\n",
            "Logs at step 2412: {'loss': 2.3291, 'grad_norm': 1.6383311748504639, 'learning_rate': 2.326458194721668e-05, 'epoch': 1.6047904191616766}\n",
            "Captured loss: 2.3291 at step 2412\n",
            "Logs at step 2413: {'loss': 2.526, 'grad_norm': 1.4512362480163574, 'learning_rate': 2.3253493013972057e-05, 'epoch': 1.605455755156354}\n",
            "Captured loss: 2.526 at step 2413\n",
            "Logs at step 2414: {'loss': 2.4229, 'grad_norm': 1.2193052768707275, 'learning_rate': 2.3242404080727434e-05, 'epoch': 1.6061210911510313}\n",
            "Captured loss: 2.4229 at step 2414\n",
            "Logs at step 2415: {'loss': 2.5523, 'grad_norm': 1.2145792245864868, 'learning_rate': 2.3231315147482812e-05, 'epoch': 1.6067864271457086}\n",
            "Captured loss: 2.5523 at step 2415\n",
            "Logs at step 2416: {'loss': 2.4371, 'grad_norm': 1.0811914205551147, 'learning_rate': 2.3220226214238193e-05, 'epoch': 1.6074517631403857}\n",
            "Captured loss: 2.4371 at step 2416\n",
            "Logs at step 2417: {'loss': 2.4904, 'grad_norm': 1.3519799709320068, 'learning_rate': 2.320913728099357e-05, 'epoch': 1.6081170991350633}\n",
            "Captured loss: 2.4904 at step 2417\n",
            "Logs at step 2418: {'loss': 2.3445, 'grad_norm': 1.6020069122314453, 'learning_rate': 2.3198048347748945e-05, 'epoch': 1.6087824351297404}\n",
            "Captured loss: 2.3445 at step 2418\n",
            "Logs at step 2419: {'loss': 2.5673, 'grad_norm': 1.1436145305633545, 'learning_rate': 2.3186959414504326e-05, 'epoch': 1.609447771124418}\n",
            "Captured loss: 2.5673 at step 2419\n",
            "Logs at step 2420: {'loss': 2.4909, 'grad_norm': 1.1836304664611816, 'learning_rate': 2.3175870481259703e-05, 'epoch': 1.610113107119095}\n",
            "Captured loss: 2.4909 at step 2420\n",
            "Logs at step 2421: {'loss': 2.6747, 'grad_norm': 1.4202483892440796, 'learning_rate': 2.3164781548015084e-05, 'epoch': 1.6107784431137726}\n",
            "Captured loss: 2.6747 at step 2421\n",
            "Logs at step 2422: {'loss': 2.6523, 'grad_norm': 1.9212874174118042, 'learning_rate': 2.315369261477046e-05, 'epoch': 1.6114437791084497}\n",
            "Captured loss: 2.6523 at step 2422\n",
            "Logs at step 2423: {'loss': 2.5508, 'grad_norm': 1.0211833715438843, 'learning_rate': 2.3142603681525836e-05, 'epoch': 1.612109115103127}\n",
            "Captured loss: 2.5508 at step 2423\n",
            "Logs at step 2424: {'loss': 2.1829, 'grad_norm': 1.5700830221176147, 'learning_rate': 2.3131514748281217e-05, 'epoch': 1.6127744510978044}\n",
            "Captured loss: 2.1829 at step 2424\n",
            "Logs at step 2425: {'loss': 2.534, 'grad_norm': 1.2215996980667114, 'learning_rate': 2.3120425815036594e-05, 'epoch': 1.6134397870924817}\n",
            "Captured loss: 2.534 at step 2425\n",
            "Logs at step 2426: {'loss': 2.4671, 'grad_norm': 1.4367352724075317, 'learning_rate': 2.3109336881791975e-05, 'epoch': 1.614105123087159}\n",
            "Captured loss: 2.4671 at step 2426\n",
            "Logs at step 2427: {'loss': 2.4072, 'grad_norm': 1.1976566314697266, 'learning_rate': 2.309824794854735e-05, 'epoch': 1.6147704590818364}\n",
            "Captured loss: 2.4072 at step 2427\n",
            "Logs at step 2428: {'loss': 2.598, 'grad_norm': 1.9285454750061035, 'learning_rate': 2.3087159015302727e-05, 'epoch': 1.6154357950765137}\n",
            "Captured loss: 2.598 at step 2428\n",
            "Logs at step 2429: {'loss': 2.4752, 'grad_norm': 1.3348078727722168, 'learning_rate': 2.3076070082058108e-05, 'epoch': 1.6161011310711908}\n",
            "Captured loss: 2.4752 at step 2429\n",
            "Logs at step 2430: {'loss': 2.68, 'grad_norm': 1.534966230392456, 'learning_rate': 2.3064981148813485e-05, 'epoch': 1.6167664670658684}\n",
            "Captured loss: 2.68 at step 2430\n",
            "Logs at step 2431: {'loss': 1.9765, 'grad_norm': 1.7232155799865723, 'learning_rate': 2.3053892215568866e-05, 'epoch': 1.6174318030605455}\n",
            "Captured loss: 1.9765 at step 2431\n",
            "Logs at step 2432: {'loss': 2.6011, 'grad_norm': 1.1628104448318481, 'learning_rate': 2.304280328232424e-05, 'epoch': 1.618097139055223}\n",
            "Captured loss: 2.6011 at step 2432\n",
            "Logs at step 2433: {'loss': 1.9217, 'grad_norm': 2.0292601585388184, 'learning_rate': 2.3031714349079618e-05, 'epoch': 1.6187624750499001}\n",
            "Captured loss: 1.9217 at step 2433\n",
            "Logs at step 2434: {'loss': 2.1614, 'grad_norm': 1.2755182981491089, 'learning_rate': 2.3020625415835e-05, 'epoch': 1.6194278110445777}\n",
            "Captured loss: 2.1614 at step 2434\n",
            "Logs at step 2435: {'loss': 2.4339, 'grad_norm': 1.4392802715301514, 'learning_rate': 2.3009536482590377e-05, 'epoch': 1.6200931470392548}\n",
            "Captured loss: 2.4339 at step 2435\n",
            "Logs at step 2436: {'loss': 2.628, 'grad_norm': 1.433552861213684, 'learning_rate': 2.2998447549345754e-05, 'epoch': 1.620758483033932}\n",
            "Captured loss: 2.628 at step 2436\n",
            "Logs at step 2437: {'loss': 2.2908, 'grad_norm': 1.3917561769485474, 'learning_rate': 2.298735861610113e-05, 'epoch': 1.6214238190286094}\n",
            "Captured loss: 2.2908 at step 2437\n",
            "Logs at step 2438: {'loss': 2.3461, 'grad_norm': 1.8119860887527466, 'learning_rate': 2.297626968285651e-05, 'epoch': 1.6220891550232868}\n",
            "Captured loss: 2.3461 at step 2438\n",
            "Logs at step 2439: {'loss': 2.3419, 'grad_norm': 1.3888916969299316, 'learning_rate': 2.296518074961189e-05, 'epoch': 1.622754491017964}\n",
            "Captured loss: 2.3419 at step 2439\n",
            "Logs at step 2440: {'loss': 2.4729, 'grad_norm': 1.0778475999832153, 'learning_rate': 2.2954091816367264e-05, 'epoch': 1.6234198270126414}\n",
            "Captured loss: 2.4729 at step 2440\n",
            "Logs at step 2441: {'loss': 2.4694, 'grad_norm': 1.1425739526748657, 'learning_rate': 2.2943002883122645e-05, 'epoch': 1.6240851630073188}\n",
            "Captured loss: 2.4694 at step 2441\n",
            "Logs at step 2442: {'loss': 2.4926, 'grad_norm': 1.2644623517990112, 'learning_rate': 2.2931913949878023e-05, 'epoch': 1.6247504990019959}\n",
            "Captured loss: 2.4926 at step 2442\n",
            "Logs at step 2443: {'loss': 2.3717, 'grad_norm': 1.7283523082733154, 'learning_rate': 2.29208250166334e-05, 'epoch': 1.6254158349966734}\n",
            "Captured loss: 2.3717 at step 2443\n",
            "Logs at step 2444: {'loss': 2.6499, 'grad_norm': 1.4868950843811035, 'learning_rate': 2.290973608338878e-05, 'epoch': 1.6260811709913505}\n",
            "Captured loss: 2.6499 at step 2444\n",
            "Logs at step 2445: {'loss': 2.4964, 'grad_norm': 1.4628909826278687, 'learning_rate': 2.2898647150144155e-05, 'epoch': 1.626746506986028}\n",
            "Captured loss: 2.4964 at step 2445\n",
            "Logs at step 2446: {'loss': 2.5634, 'grad_norm': 1.222088098526001, 'learning_rate': 2.2887558216899536e-05, 'epoch': 1.6274118429807052}\n",
            "Captured loss: 2.5634 at step 2446\n",
            "Logs at step 2447: {'loss': 2.4852, 'grad_norm': 1.281734824180603, 'learning_rate': 2.2876469283654914e-05, 'epoch': 1.6280771789753827}\n",
            "Captured loss: 2.4852 at step 2447\n",
            "Logs at step 2448: {'loss': 2.4048, 'grad_norm': 1.2999346256256104, 'learning_rate': 2.286538035041029e-05, 'epoch': 1.6287425149700598}\n",
            "Captured loss: 2.4048 at step 2448\n",
            "Logs at step 2449: {'loss': 2.4749, 'grad_norm': 1.9794243574142456, 'learning_rate': 2.285429141716567e-05, 'epoch': 1.6294078509647372}\n",
            "Captured loss: 2.4749 at step 2449\n",
            "Logs at step 2450: {'loss': 2.4039, 'grad_norm': 1.3919715881347656, 'learning_rate': 2.2843202483921047e-05, 'epoch': 1.6300731869594145}\n",
            "Captured loss: 2.4039 at step 2450\n",
            "Logs at step 2451: {'loss': 2.5544, 'grad_norm': 0.9799842238426208, 'learning_rate': 2.2832113550676428e-05, 'epoch': 1.6307385229540918}\n",
            "Captured loss: 2.5544 at step 2451\n",
            "Logs at step 2452: {'loss': 2.864, 'grad_norm': 2.024671792984009, 'learning_rate': 2.2821024617431805e-05, 'epoch': 1.6314038589487692}\n",
            "Captured loss: 2.864 at step 2452\n",
            "Logs at step 2453: {'loss': 2.3813, 'grad_norm': 1.2913539409637451, 'learning_rate': 2.2809935684187183e-05, 'epoch': 1.6320691949434465}\n",
            "Captured loss: 2.3813 at step 2453\n",
            "Logs at step 2454: {'loss': 2.2365, 'grad_norm': 1.5351622104644775, 'learning_rate': 2.279884675094256e-05, 'epoch': 1.6327345309381238}\n",
            "Captured loss: 2.2365 at step 2454\n",
            "Logs at step 2455: {'loss': 2.4315, 'grad_norm': 1.17604398727417, 'learning_rate': 2.2787757817697938e-05, 'epoch': 1.633399866932801}\n",
            "Captured loss: 2.4315 at step 2455\n",
            "Logs at step 2456: {'loss': 2.2963, 'grad_norm': 1.615264892578125, 'learning_rate': 2.277666888445332e-05, 'epoch': 1.6340652029274785}\n",
            "Captured loss: 2.2963 at step 2456\n",
            "Logs at step 2457: {'loss': 2.3043, 'grad_norm': 1.4017493724822998, 'learning_rate': 2.2765579951208696e-05, 'epoch': 1.6347305389221556}\n",
            "Captured loss: 2.3043 at step 2457\n",
            "Logs at step 2458: {'loss': 2.4088, 'grad_norm': 1.2465604543685913, 'learning_rate': 2.275449101796407e-05, 'epoch': 1.6353958749168331}\n",
            "Captured loss: 2.4088 at step 2458\n",
            "Logs at step 2459: {'loss': 2.2104, 'grad_norm': 1.5519905090332031, 'learning_rate': 2.274340208471945e-05, 'epoch': 1.6360612109115102}\n",
            "Captured loss: 2.2104 at step 2459\n",
            "Logs at step 2460: {'loss': 2.5571, 'grad_norm': 1.8015923500061035, 'learning_rate': 2.273231315147483e-05, 'epoch': 1.6367265469061876}\n",
            "Captured loss: 2.5571 at step 2460\n",
            "Logs at step 2461: {'loss': 2.5595, 'grad_norm': 1.0840578079223633, 'learning_rate': 2.272122421823021e-05, 'epoch': 1.637391882900865}\n",
            "Captured loss: 2.5595 at step 2461\n",
            "Logs at step 2462: {'loss': 2.2048, 'grad_norm': 1.4798901081085205, 'learning_rate': 2.2710135284985584e-05, 'epoch': 1.6380572188955422}\n",
            "Captured loss: 2.2048 at step 2462\n",
            "Logs at step 2463: {'loss': 2.6136, 'grad_norm': 1.2372817993164062, 'learning_rate': 2.269904635174096e-05, 'epoch': 1.6387225548902196}\n",
            "Captured loss: 2.6136 at step 2463\n",
            "Logs at step 2464: {'loss': 2.5336, 'grad_norm': 1.2304608821868896, 'learning_rate': 2.2687957418496342e-05, 'epoch': 1.639387890884897}\n",
            "Captured loss: 2.5336 at step 2464\n",
            "Logs at step 2465: {'loss': 2.4777, 'grad_norm': 1.096182942390442, 'learning_rate': 2.267686848525172e-05, 'epoch': 1.6400532268795742}\n",
            "Captured loss: 2.4777 at step 2465\n",
            "Logs at step 2466: {'loss': 2.1796, 'grad_norm': 1.3713066577911377, 'learning_rate': 2.2665779552007098e-05, 'epoch': 1.6407185628742516}\n",
            "Captured loss: 2.1796 at step 2466\n",
            "Logs at step 2467: {'loss': 2.2993, 'grad_norm': 1.3368375301361084, 'learning_rate': 2.2654690618762475e-05, 'epoch': 1.6413838988689289}\n",
            "Captured loss: 2.2993 at step 2467\n",
            "Logs at step 2468: {'loss': 2.0919, 'grad_norm': 1.5626071691513062, 'learning_rate': 2.2643601685517853e-05, 'epoch': 1.642049234863606}\n",
            "Captured loss: 2.0919 at step 2468\n",
            "Logs at step 2469: {'loss': 2.3293, 'grad_norm': 1.168932318687439, 'learning_rate': 2.2632512752273234e-05, 'epoch': 1.6427145708582835}\n",
            "Captured loss: 2.3293 at step 2469\n",
            "Logs at step 2470: {'loss': 2.5717, 'grad_norm': 1.3855236768722534, 'learning_rate': 2.262142381902861e-05, 'epoch': 1.6433799068529606}\n",
            "Captured loss: 2.5717 at step 2470\n",
            "Logs at step 2471: {'loss': 2.0527, 'grad_norm': 1.5115967988967896, 'learning_rate': 2.261033488578399e-05, 'epoch': 1.6440452428476382}\n",
            "Captured loss: 2.0527 at step 2471\n",
            "Logs at step 2472: {'loss': 2.6371, 'grad_norm': 1.1486161947250366, 'learning_rate': 2.2599245952539366e-05, 'epoch': 1.6447105788423153}\n",
            "Captured loss: 2.6371 at step 2472\n",
            "Logs at step 2473: {'loss': 2.3852, 'grad_norm': 1.660673975944519, 'learning_rate': 2.2588157019294744e-05, 'epoch': 1.6453759148369926}\n",
            "Captured loss: 2.3852 at step 2473\n",
            "Logs at step 2474: {'loss': 2.2197, 'grad_norm': 1.6394295692443848, 'learning_rate': 2.2577068086050125e-05, 'epoch': 1.64604125083167}\n",
            "Captured loss: 2.2197 at step 2474\n",
            "Logs at step 2475: {'loss': 2.5053, 'grad_norm': 1.5735769271850586, 'learning_rate': 2.2565979152805502e-05, 'epoch': 1.6467065868263473}\n",
            "Captured loss: 2.5053 at step 2475\n",
            "Logs at step 2476: {'loss': 2.5211, 'grad_norm': 1.146199345588684, 'learning_rate': 2.255489021956088e-05, 'epoch': 1.6473719228210246}\n",
            "Captured loss: 2.5211 at step 2476\n",
            "Logs at step 2477: {'loss': 2.369, 'grad_norm': 1.2056804895401, 'learning_rate': 2.2543801286316257e-05, 'epoch': 1.648037258815702}\n",
            "Captured loss: 2.369 at step 2477\n",
            "Logs at step 2478: {'loss': 2.5704, 'grad_norm': 1.054120659828186, 'learning_rate': 2.2532712353071635e-05, 'epoch': 1.6487025948103793}\n",
            "Captured loss: 2.5704 at step 2478\n",
            "Logs at step 2479: {'loss': 2.4153, 'grad_norm': 1.5184228420257568, 'learning_rate': 2.2521623419827016e-05, 'epoch': 1.6493679308050564}\n",
            "Captured loss: 2.4153 at step 2479\n",
            "Logs at step 2480: {'loss': 2.5934, 'grad_norm': 1.099690556526184, 'learning_rate': 2.251053448658239e-05, 'epoch': 1.650033266799734}\n",
            "Captured loss: 2.5934 at step 2480\n",
            "Logs at step 2481: {'loss': 2.6965, 'grad_norm': 1.3670583963394165, 'learning_rate': 2.249944555333777e-05, 'epoch': 1.650698602794411}\n",
            "Captured loss: 2.6965 at step 2481\n",
            "Logs at step 2482: {'loss': 2.5104, 'grad_norm': 1.2719708681106567, 'learning_rate': 2.248835662009315e-05, 'epoch': 1.6513639387890886}\n",
            "Captured loss: 2.5104 at step 2482\n",
            "Logs at step 2483: {'loss': 2.5681, 'grad_norm': 1.0758682489395142, 'learning_rate': 2.2477267686848526e-05, 'epoch': 1.6520292747837657}\n",
            "Captured loss: 2.5681 at step 2483\n",
            "Logs at step 2484: {'loss': 2.3713, 'grad_norm': 1.15549635887146, 'learning_rate': 2.2466178753603904e-05, 'epoch': 1.6526946107784433}\n",
            "Captured loss: 2.3713 at step 2484\n",
            "Logs at step 2485: {'loss': 2.6365, 'grad_norm': 1.0250272750854492, 'learning_rate': 2.245508982035928e-05, 'epoch': 1.6533599467731204}\n",
            "Captured loss: 2.6365 at step 2485\n",
            "Logs at step 2486: {'loss': 2.5637, 'grad_norm': 1.565531849861145, 'learning_rate': 2.2444000887114662e-05, 'epoch': 1.6540252827677977}\n",
            "Captured loss: 2.5637 at step 2486\n",
            "Logs at step 2487: {'loss': 2.5362, 'grad_norm': 1.1285063028335571, 'learning_rate': 2.243291195387004e-05, 'epoch': 1.654690618762475}\n",
            "Captured loss: 2.5362 at step 2487\n",
            "Logs at step 2488: {'loss': 2.0923, 'grad_norm': 1.3887319564819336, 'learning_rate': 2.2421823020625417e-05, 'epoch': 1.6553559547571524}\n",
            "Captured loss: 2.0923 at step 2488\n",
            "Logs at step 2489: {'loss': 2.576, 'grad_norm': 1.3539782762527466, 'learning_rate': 2.2410734087380795e-05, 'epoch': 1.6560212907518297}\n",
            "Captured loss: 2.576 at step 2489\n",
            "Logs at step 2490: {'loss': 2.6959, 'grad_norm': 1.3076863288879395, 'learning_rate': 2.2399645154136172e-05, 'epoch': 1.656686626746507}\n",
            "Captured loss: 2.6959 at step 2490\n",
            "Logs at step 2491: {'loss': 2.5674, 'grad_norm': 1.1365690231323242, 'learning_rate': 2.2388556220891553e-05, 'epoch': 1.6573519627411843}\n",
            "Captured loss: 2.5674 at step 2491\n",
            "Logs at step 2492: {'loss': 2.5448, 'grad_norm': 1.112345814704895, 'learning_rate': 2.237746728764693e-05, 'epoch': 1.6580172987358615}\n",
            "Captured loss: 2.5448 at step 2492\n",
            "Logs at step 2493: {'loss': 2.3795, 'grad_norm': 1.3945375680923462, 'learning_rate': 2.2366378354402305e-05, 'epoch': 1.658682634730539}\n",
            "Captured loss: 2.3795 at step 2493\n",
            "Logs at step 2494: {'loss': 2.5926, 'grad_norm': 1.271294116973877, 'learning_rate': 2.2355289421157686e-05, 'epoch': 1.6593479707252161}\n",
            "Captured loss: 2.5926 at step 2494\n",
            "Logs at step 2495: {'loss': 2.641, 'grad_norm': 1.2907648086547852, 'learning_rate': 2.2344200487913063e-05, 'epoch': 1.6600133067198937}\n",
            "Captured loss: 2.641 at step 2495\n",
            "Logs at step 2496: {'loss': 2.2007, 'grad_norm': 1.6174407005310059, 'learning_rate': 2.2333111554668444e-05, 'epoch': 1.6606786427145708}\n",
            "Captured loss: 2.2007 at step 2496\n",
            "Logs at step 2497: {'loss': 2.279, 'grad_norm': 1.34465491771698, 'learning_rate': 2.232202262142382e-05, 'epoch': 1.6613439787092483}\n",
            "Captured loss: 2.279 at step 2497\n",
            "Logs at step 2498: {'loss': 2.3488, 'grad_norm': 1.3016868829727173, 'learning_rate': 2.2310933688179196e-05, 'epoch': 1.6620093147039254}\n",
            "Captured loss: 2.3488 at step 2498\n",
            "Logs at step 2499: {'loss': 2.5879, 'grad_norm': 1.0012818574905396, 'learning_rate': 2.2299844754934577e-05, 'epoch': 1.6626746506986028}\n",
            "Captured loss: 2.5879 at step 2499\n",
            "Logs at step 2500: {'loss': 2.4869, 'grad_norm': 1.7439887523651123, 'learning_rate': 2.2288755821689955e-05, 'epoch': 1.66333998669328}\n",
            "Captured loss: 2.4869 at step 2500\n",
            "Logs at step 2501: {'loss': 2.5043, 'grad_norm': 1.1061807870864868, 'learning_rate': 2.2277666888445332e-05, 'epoch': 1.6640053226879574}\n",
            "Captured loss: 2.5043 at step 2501\n",
            "Logs at step 2502: {'loss': 2.508, 'grad_norm': 1.0011957883834839, 'learning_rate': 2.226657795520071e-05, 'epoch': 1.6646706586826348}\n",
            "Captured loss: 2.508 at step 2502\n",
            "Logs at step 2503: {'loss': 2.19, 'grad_norm': 1.5371005535125732, 'learning_rate': 2.2255489021956087e-05, 'epoch': 1.665335994677312}\n",
            "Captured loss: 2.19 at step 2503\n",
            "Logs at step 2504: {'loss': 2.6117, 'grad_norm': 1.1542283296585083, 'learning_rate': 2.2244400088711468e-05, 'epoch': 1.6660013306719894}\n",
            "Captured loss: 2.6117 at step 2504\n",
            "Logs at step 2505: {'loss': 2.4355, 'grad_norm': 1.060057520866394, 'learning_rate': 2.2233311155466846e-05, 'epoch': 1.6666666666666665}\n",
            "Captured loss: 2.4355 at step 2505\n",
            "Logs at step 2506: {'loss': 2.6309, 'grad_norm': 1.2430416345596313, 'learning_rate': 2.2222222222222223e-05, 'epoch': 1.667332002661344}\n",
            "Captured loss: 2.6309 at step 2506\n",
            "Logs at step 2507: {'loss': 2.2137, 'grad_norm': 1.2105209827423096, 'learning_rate': 2.22111332889776e-05, 'epoch': 1.6679973386560212}\n",
            "Captured loss: 2.2137 at step 2507\n",
            "Logs at step 2508: {'loss': 2.5939, 'grad_norm': 1.2457817792892456, 'learning_rate': 2.220004435573298e-05, 'epoch': 1.6686626746506987}\n",
            "Captured loss: 2.5939 at step 2508\n",
            "Logs at step 2509: {'loss': 2.5585, 'grad_norm': 2.671196937561035, 'learning_rate': 2.218895542248836e-05, 'epoch': 1.6693280106453758}\n",
            "Captured loss: 2.5585 at step 2509\n",
            "Logs at step 2510: {'loss': 2.3293, 'grad_norm': 1.4659911394119263, 'learning_rate': 2.2177866489243737e-05, 'epoch': 1.6699933466400534}\n",
            "Captured loss: 2.3293 at step 2510\n",
            "Logs at step 2511: {'loss': 2.2886, 'grad_norm': 1.297264575958252, 'learning_rate': 2.2166777555999114e-05, 'epoch': 1.6706586826347305}\n",
            "Captured loss: 2.2886 at step 2511\n",
            "Logs at step 2512: {'loss': 2.6254, 'grad_norm': 1.2236448526382446, 'learning_rate': 2.2155688622754492e-05, 'epoch': 1.6713240186294078}\n",
            "Captured loss: 2.6254 at step 2512\n",
            "Logs at step 2513: {'loss': 2.5303, 'grad_norm': 1.5006929636001587, 'learning_rate': 2.214459968950987e-05, 'epoch': 1.6719893546240852}\n",
            "Captured loss: 2.5303 at step 2513\n",
            "Logs at step 2514: {'loss': 2.5373, 'grad_norm': 1.0043396949768066, 'learning_rate': 2.213351075626525e-05, 'epoch': 1.6726546906187625}\n",
            "Captured loss: 2.5373 at step 2514\n",
            "Logs at step 2515: {'loss': 2.3383, 'grad_norm': 1.4491517543792725, 'learning_rate': 2.2122421823020625e-05, 'epoch': 1.6733200266134398}\n",
            "Captured loss: 2.3383 at step 2515\n",
            "Logs at step 2516: {'loss': 2.6162, 'grad_norm': 1.3857837915420532, 'learning_rate': 2.2111332889776006e-05, 'epoch': 1.6739853626081171}\n",
            "Captured loss: 2.6162 at step 2516\n",
            "Logs at step 2517: {'loss': 2.5615, 'grad_norm': 1.5854579210281372, 'learning_rate': 2.2100243956531383e-05, 'epoch': 1.6746506986027945}\n",
            "Captured loss: 2.5615 at step 2517\n",
            "Logs at step 2518: {'loss': 2.3969, 'grad_norm': 1.6706527471542358, 'learning_rate': 2.208915502328676e-05, 'epoch': 1.6753160345974716}\n",
            "Captured loss: 2.3969 at step 2518\n",
            "Logs at step 2519: {'loss': 2.2585, 'grad_norm': 1.45639169216156, 'learning_rate': 2.2078066090042138e-05, 'epoch': 1.6759813705921491}\n",
            "Captured loss: 2.2585 at step 2519\n",
            "Logs at step 2520: {'loss': 2.6241, 'grad_norm': 2.1170296669006348, 'learning_rate': 2.2066977156797516e-05, 'epoch': 1.6766467065868262}\n",
            "Captured loss: 2.6241 at step 2520\n",
            "Logs at step 2521: {'loss': 2.4636, 'grad_norm': 1.4738820791244507, 'learning_rate': 2.2055888223552897e-05, 'epoch': 1.6773120425815038}\n",
            "Captured loss: 2.4636 at step 2521\n",
            "Logs at step 2522: {'loss': 2.6978, 'grad_norm': 1.4879416227340698, 'learning_rate': 2.2044799290308274e-05, 'epoch': 1.677977378576181}\n",
            "Captured loss: 2.6978 at step 2522\n",
            "Logs at step 2523: {'loss': 2.598, 'grad_norm': 1.518301010131836, 'learning_rate': 2.2033710357063652e-05, 'epoch': 1.6786427145708582}\n",
            "Captured loss: 2.598 at step 2523\n",
            "Logs at step 2524: {'loss': 2.2086, 'grad_norm': 1.698577880859375, 'learning_rate': 2.202262142381903e-05, 'epoch': 1.6793080505655356}\n",
            "Captured loss: 2.2086 at step 2524\n",
            "Logs at step 2525: {'loss': 2.6182, 'grad_norm': 0.9730871319770813, 'learning_rate': 2.2011532490574407e-05, 'epoch': 1.679973386560213}\n",
            "Captured loss: 2.6182 at step 2525\n",
            "Logs at step 2526: {'loss': 2.3004, 'grad_norm': 1.7246242761611938, 'learning_rate': 2.2000443557329788e-05, 'epoch': 1.6806387225548902}\n",
            "Captured loss: 2.3004 at step 2526\n",
            "Logs at step 2527: {'loss': 1.9793, 'grad_norm': 1.5469598770141602, 'learning_rate': 2.1989354624085165e-05, 'epoch': 1.6813040585495675}\n",
            "Captured loss: 1.9793 at step 2527\n",
            "Logs at step 2528: {'loss': 2.6396, 'grad_norm': 1.1279940605163574, 'learning_rate': 2.197826569084054e-05, 'epoch': 1.6819693945442449}\n",
            "Captured loss: 2.6396 at step 2528\n",
            "Logs at step 2529: {'loss': 2.4335, 'grad_norm': 1.0118739604949951, 'learning_rate': 2.196717675759592e-05, 'epoch': 1.6826347305389222}\n",
            "Captured loss: 2.4335 at step 2529\n",
            "Logs at step 2530: {'loss': 2.1665, 'grad_norm': 1.6910876035690308, 'learning_rate': 2.1956087824351298e-05, 'epoch': 1.6833000665335995}\n",
            "Captured loss: 2.1665 at step 2530\n",
            "Logs at step 2531: {'loss': 2.8198, 'grad_norm': 1.6736313104629517, 'learning_rate': 2.1944998891106676e-05, 'epoch': 1.6839654025282766}\n",
            "Captured loss: 2.8198 at step 2531\n",
            "Logs at step 2532: {'loss': 2.4864, 'grad_norm': 1.1751456260681152, 'learning_rate': 2.1933909957862057e-05, 'epoch': 1.6846307385229542}\n",
            "Captured loss: 2.4864 at step 2532\n",
            "Logs at step 2533: {'loss': 2.6512, 'grad_norm': 1.207261562347412, 'learning_rate': 2.192282102461743e-05, 'epoch': 1.6852960745176313}\n",
            "Captured loss: 2.6512 at step 2533\n",
            "Logs at step 2534: {'loss': 2.357, 'grad_norm': 1.2540794610977173, 'learning_rate': 2.191173209137281e-05, 'epoch': 1.6859614105123089}\n",
            "Captured loss: 2.357 at step 2534\n",
            "Logs at step 2535: {'loss': 2.5103, 'grad_norm': 1.6373332738876343, 'learning_rate': 2.190064315812819e-05, 'epoch': 1.686626746506986}\n",
            "Captured loss: 2.5103 at step 2535\n",
            "Logs at step 2536: {'loss': 2.4734, 'grad_norm': 1.123716115951538, 'learning_rate': 2.1889554224883567e-05, 'epoch': 1.6872920825016633}\n",
            "Captured loss: 2.4734 at step 2536\n",
            "Logs at step 2537: {'loss': 2.3952, 'grad_norm': 1.266038417816162, 'learning_rate': 2.1878465291638944e-05, 'epoch': 1.6879574184963406}\n",
            "Captured loss: 2.3952 at step 2537\n",
            "Logs at step 2538: {'loss': 2.4064, 'grad_norm': 1.556107521057129, 'learning_rate': 2.1867376358394322e-05, 'epoch': 1.688622754491018}\n",
            "Captured loss: 2.4064 at step 2538\n",
            "Logs at step 2539: {'loss': 2.544, 'grad_norm': 1.1714388132095337, 'learning_rate': 2.1856287425149703e-05, 'epoch': 1.6892880904856953}\n",
            "Captured loss: 2.544 at step 2539\n",
            "Logs at step 2540: {'loss': 2.4424, 'grad_norm': 1.7443948984146118, 'learning_rate': 2.184519849190508e-05, 'epoch': 1.6899534264803726}\n",
            "Captured loss: 2.4424 at step 2540\n",
            "Logs at step 2541: {'loss': 2.2394, 'grad_norm': 1.9278510808944702, 'learning_rate': 2.1834109558660458e-05, 'epoch': 1.69061876247505}\n",
            "Captured loss: 2.2394 at step 2541\n",
            "Logs at step 2542: {'loss': 2.6307, 'grad_norm': 1.3117460012435913, 'learning_rate': 2.1823020625415835e-05, 'epoch': 1.691284098469727}\n",
            "Captured loss: 2.6307 at step 2542\n",
            "Logs at step 2543: {'loss': 2.5873, 'grad_norm': 1.328263521194458, 'learning_rate': 2.1811931692171213e-05, 'epoch': 1.6919494344644046}\n",
            "Captured loss: 2.5873 at step 2543\n",
            "Logs at step 2544: {'loss': 2.3466, 'grad_norm': 1.9566445350646973, 'learning_rate': 2.1800842758926594e-05, 'epoch': 1.6926147704590817}\n",
            "Captured loss: 2.3466 at step 2544\n",
            "Logs at step 2545: {'loss': 2.5462, 'grad_norm': 1.090803861618042, 'learning_rate': 2.178975382568197e-05, 'epoch': 1.6932801064537593}\n",
            "Captured loss: 2.5462 at step 2545\n",
            "Logs at step 2546: {'loss': 2.5552, 'grad_norm': 1.2796776294708252, 'learning_rate': 2.177866489243735e-05, 'epoch': 1.6939454424484364}\n",
            "Captured loss: 2.5552 at step 2546\n",
            "Logs at step 2547: {'loss': 2.438, 'grad_norm': 1.1554784774780273, 'learning_rate': 2.1767575959192727e-05, 'epoch': 1.694610778443114}\n",
            "Captured loss: 2.438 at step 2547\n",
            "Logs at step 2548: {'loss': 2.603, 'grad_norm': 1.2066789865493774, 'learning_rate': 2.1756487025948104e-05, 'epoch': 1.695276114437791}\n",
            "Captured loss: 2.603 at step 2548\n",
            "Logs at step 2549: {'loss': 2.5041, 'grad_norm': 1.0877972841262817, 'learning_rate': 2.1745398092703485e-05, 'epoch': 1.6959414504324684}\n",
            "Captured loss: 2.5041 at step 2549\n",
            "Logs at step 2550: {'loss': 2.2561, 'grad_norm': 1.9491380453109741, 'learning_rate': 2.173430915945886e-05, 'epoch': 1.6966067864271457}\n",
            "Captured loss: 2.2561 at step 2550\n",
            "Logs at step 2551: {'loss': 2.6195, 'grad_norm': 1.5622508525848389, 'learning_rate': 2.172322022621424e-05, 'epoch': 1.697272122421823}\n",
            "Captured loss: 2.6195 at step 2551\n",
            "Logs at step 2552: {'loss': 2.5643, 'grad_norm': 1.2761592864990234, 'learning_rate': 2.1712131292969618e-05, 'epoch': 1.6979374584165003}\n",
            "Captured loss: 2.5643 at step 2552\n",
            "Logs at step 2553: {'loss': 2.5668, 'grad_norm': 0.9825534224510193, 'learning_rate': 2.1701042359724995e-05, 'epoch': 1.6986027944111777}\n",
            "Captured loss: 2.5668 at step 2553\n",
            "Logs at step 2554: {'loss': 2.5048, 'grad_norm': 1.0770206451416016, 'learning_rate': 2.1689953426480376e-05, 'epoch': 1.699268130405855}\n",
            "Captured loss: 2.5048 at step 2554\n",
            "Logs at step 2555: {'loss': 2.4234, 'grad_norm': 1.3112255334854126, 'learning_rate': 2.167886449323575e-05, 'epoch': 1.6999334664005321}\n",
            "Captured loss: 2.4234 at step 2555\n",
            "Logs at step 2556: {'loss': 2.6427, 'grad_norm': 1.3231780529022217, 'learning_rate': 2.166777555999113e-05, 'epoch': 1.7005988023952097}\n",
            "Captured loss: 2.6427 at step 2556\n",
            "Logs at step 2557: {'loss': 2.5742, 'grad_norm': 1.41974937915802, 'learning_rate': 2.165668662674651e-05, 'epoch': 1.7012641383898868}\n",
            "Captured loss: 2.5742 at step 2557\n",
            "Logs at step 2558: {'loss': 2.3285, 'grad_norm': 1.883995532989502, 'learning_rate': 2.1645597693501886e-05, 'epoch': 1.7019294743845643}\n",
            "Captured loss: 2.3285 at step 2558\n",
            "Logs at step 2559: {'loss': 2.567, 'grad_norm': 1.0601211786270142, 'learning_rate': 2.1634508760257264e-05, 'epoch': 1.7025948103792414}\n",
            "Captured loss: 2.567 at step 2559\n",
            "Logs at step 2560: {'loss': 2.8675, 'grad_norm': 3.4834110736846924, 'learning_rate': 2.162341982701264e-05, 'epoch': 1.703260146373919}\n",
            "Captured loss: 2.8675 at step 2560\n",
            "Logs at step 2561: {'loss': 2.257, 'grad_norm': 1.5153688192367554, 'learning_rate': 2.1612330893768022e-05, 'epoch': 1.703925482368596}\n",
            "Captured loss: 2.257 at step 2561\n",
            "Logs at step 2562: {'loss': 2.5267, 'grad_norm': 1.059189796447754, 'learning_rate': 2.16012419605234e-05, 'epoch': 1.7045908183632734}\n",
            "Captured loss: 2.5267 at step 2562\n",
            "Logs at step 2563: {'loss': 2.375, 'grad_norm': 1.465355396270752, 'learning_rate': 2.1590153027278774e-05, 'epoch': 1.7052561543579507}\n",
            "Captured loss: 2.375 at step 2563\n",
            "Logs at step 2564: {'loss': 2.2382, 'grad_norm': 1.141968846321106, 'learning_rate': 2.1579064094034155e-05, 'epoch': 1.705921490352628}\n",
            "Captured loss: 2.2382 at step 2564\n",
            "Logs at step 2565: {'loss': 2.3876, 'grad_norm': 1.6433005332946777, 'learning_rate': 2.1567975160789533e-05, 'epoch': 1.7065868263473054}\n",
            "Captured loss: 2.3876 at step 2565\n",
            "Logs at step 2566: {'loss': 2.2724, 'grad_norm': 1.3624190092086792, 'learning_rate': 2.155688622754491e-05, 'epoch': 1.7072521623419827}\n",
            "Captured loss: 2.2724 at step 2566\n",
            "Logs at step 2567: {'loss': 2.4843, 'grad_norm': 1.4328091144561768, 'learning_rate': 2.154579729430029e-05, 'epoch': 1.70791749833666}\n",
            "Captured loss: 2.4843 at step 2567\n",
            "Logs at step 2568: {'loss': 2.2224, 'grad_norm': 1.2567747831344604, 'learning_rate': 2.1534708361055665e-05, 'epoch': 1.7085828343313372}\n",
            "Captured loss: 2.2224 at step 2568\n",
            "Logs at step 2569: {'loss': 2.4655, 'grad_norm': 1.1518900394439697, 'learning_rate': 2.1523619427811046e-05, 'epoch': 1.7092481703260147}\n",
            "Captured loss: 2.4655 at step 2569\n",
            "Logs at step 2570: {'loss': 2.6179, 'grad_norm': 1.2630215883255005, 'learning_rate': 2.1512530494566424e-05, 'epoch': 1.7099135063206918}\n",
            "Captured loss: 2.6179 at step 2570\n",
            "Logs at step 2571: {'loss': 2.2042, 'grad_norm': 1.1550159454345703, 'learning_rate': 2.15014415613218e-05, 'epoch': 1.7105788423153694}\n",
            "Captured loss: 2.2042 at step 2571\n",
            "Logs at step 2572: {'loss': 2.5734, 'grad_norm': 1.6638343334197998, 'learning_rate': 2.149035262807718e-05, 'epoch': 1.7112441783100465}\n",
            "Captured loss: 2.5734 at step 2572\n",
            "Logs at step 2573: {'loss': 2.2394, 'grad_norm': 1.305907964706421, 'learning_rate': 2.1479263694832556e-05, 'epoch': 1.711909514304724}\n",
            "Captured loss: 2.2394 at step 2573\n",
            "Logs at step 2574: {'loss': 2.0899, 'grad_norm': 2.061952590942383, 'learning_rate': 2.1468174761587937e-05, 'epoch': 1.7125748502994012}\n",
            "Captured loss: 2.0899 at step 2574\n",
            "Logs at step 2575: {'loss': 2.3918, 'grad_norm': 1.3340134620666504, 'learning_rate': 2.1457085828343315e-05, 'epoch': 1.7132401862940785}\n",
            "Captured loss: 2.3918 at step 2575\n",
            "Logs at step 2576: {'loss': 2.6095, 'grad_norm': 1.0302191972732544, 'learning_rate': 2.1445996895098692e-05, 'epoch': 1.7139055222887558}\n",
            "Captured loss: 2.6095 at step 2576\n",
            "Logs at step 2577: {'loss': 2.5927, 'grad_norm': 1.4425877332687378, 'learning_rate': 2.143490796185407e-05, 'epoch': 1.7145708582834331}\n",
            "Captured loss: 2.5927 at step 2577\n",
            "Logs at step 2578: {'loss': 2.333, 'grad_norm': 1.210121750831604, 'learning_rate': 2.1423819028609448e-05, 'epoch': 1.7152361942781105}\n",
            "Captured loss: 2.333 at step 2578\n",
            "Logs at step 2579: {'loss': 2.5766, 'grad_norm': 1.1729891300201416, 'learning_rate': 2.141273009536483e-05, 'epoch': 1.7159015302727878}\n",
            "Captured loss: 2.5766 at step 2579\n",
            "Logs at step 2580: {'loss': 2.2574, 'grad_norm': 1.7178694009780884, 'learning_rate': 2.1401641162120206e-05, 'epoch': 1.7165668662674651}\n",
            "Captured loss: 2.2574 at step 2580\n",
            "Logs at step 2581: {'loss': 2.5519, 'grad_norm': 0.9726698994636536, 'learning_rate': 2.1390552228875584e-05, 'epoch': 1.7172322022621422}\n",
            "Captured loss: 2.5519 at step 2581\n",
            "Logs at step 2582: {'loss': 2.6283, 'grad_norm': 1.4314595460891724, 'learning_rate': 2.137946329563096e-05, 'epoch': 1.7178975382568198}\n",
            "Captured loss: 2.6283 at step 2582\n",
            "Logs at step 2583: {'loss': 2.1906, 'grad_norm': 1.1939188241958618, 'learning_rate': 2.136837436238634e-05, 'epoch': 1.718562874251497}\n",
            "Captured loss: 2.1906 at step 2583\n",
            "Logs at step 2584: {'loss': 2.2576, 'grad_norm': 1.246504783630371, 'learning_rate': 2.135728542914172e-05, 'epoch': 1.7192282102461744}\n",
            "Captured loss: 2.2576 at step 2584\n",
            "Logs at step 2585: {'loss': 2.4654, 'grad_norm': 1.7388485670089722, 'learning_rate': 2.1346196495897094e-05, 'epoch': 1.7198935462408516}\n",
            "Captured loss: 2.4654 at step 2585\n",
            "Logs at step 2586: {'loss': 2.2565, 'grad_norm': 1.2781898975372314, 'learning_rate': 2.1335107562652475e-05, 'epoch': 1.720558882235529}\n",
            "Captured loss: 2.2565 at step 2586\n",
            "Logs at step 2587: {'loss': 2.2638, 'grad_norm': 1.3776048421859741, 'learning_rate': 2.1324018629407852e-05, 'epoch': 1.7212242182302062}\n",
            "Captured loss: 2.2638 at step 2587\n",
            "Logs at step 2588: {'loss': 2.3462, 'grad_norm': 1.493403434753418, 'learning_rate': 2.131292969616323e-05, 'epoch': 1.7218895542248835}\n",
            "Captured loss: 2.3462 at step 2588\n",
            "Logs at step 2589: {'loss': 2.5765, 'grad_norm': 1.9329683780670166, 'learning_rate': 2.130184076291861e-05, 'epoch': 1.7225548902195609}\n",
            "Captured loss: 2.5765 at step 2589\n",
            "Logs at step 2590: {'loss': 2.5822, 'grad_norm': 1.35470449924469, 'learning_rate': 2.1290751829673985e-05, 'epoch': 1.7232202262142382}\n",
            "Captured loss: 2.5822 at step 2590\n",
            "Logs at step 2591: {'loss': 2.4658, 'grad_norm': 1.0489866733551025, 'learning_rate': 2.1279662896429366e-05, 'epoch': 1.7238855622089155}\n",
            "Captured loss: 2.4658 at step 2591\n",
            "Logs at step 2592: {'loss': 2.2048, 'grad_norm': 1.5587652921676636, 'learning_rate': 2.1268573963184743e-05, 'epoch': 1.7245508982035929}\n",
            "Captured loss: 2.2048 at step 2592\n",
            "Logs at step 2593: {'loss': 2.5169, 'grad_norm': 1.0437664985656738, 'learning_rate': 2.125748502994012e-05, 'epoch': 1.7252162341982702}\n",
            "Captured loss: 2.5169 at step 2593\n",
            "Logs at step 2594: {'loss': 2.5476, 'grad_norm': 1.4966485500335693, 'learning_rate': 2.12463960966955e-05, 'epoch': 1.7258815701929473}\n",
            "Captured loss: 2.5476 at step 2594\n",
            "Logs at step 2595: {'loss': 2.1513, 'grad_norm': 1.2348756790161133, 'learning_rate': 2.1235307163450876e-05, 'epoch': 1.7265469061876249}\n",
            "Captured loss: 2.1513 at step 2595\n",
            "Logs at step 2596: {'loss': 2.5681, 'grad_norm': 0.9641000628471375, 'learning_rate': 2.1224218230206254e-05, 'epoch': 1.727212242182302}\n",
            "Captured loss: 2.5681 at step 2596\n",
            "Logs at step 2597: {'loss': 2.6847, 'grad_norm': 2.198082685470581, 'learning_rate': 2.1213129296961635e-05, 'epoch': 1.7278775781769795}\n",
            "Captured loss: 2.6847 at step 2597\n",
            "Logs at step 2598: {'loss': 2.8674, 'grad_norm': 1.6092582941055298, 'learning_rate': 2.120204036371701e-05, 'epoch': 1.7285429141716566}\n",
            "Captured loss: 2.8674 at step 2598\n",
            "Logs at step 2599: {'loss': 2.1713, 'grad_norm': 1.6700561046600342, 'learning_rate': 2.119095143047239e-05, 'epoch': 1.729208250166334}\n",
            "Captured loss: 2.1713 at step 2599\n",
            "Logs at step 2600: {'loss': 2.3065, 'grad_norm': 1.2472493648529053, 'learning_rate': 2.1179862497227767e-05, 'epoch': 1.7298735861610113}\n",
            "Captured loss: 2.3065 at step 2600\n",
            "Logs at step 2601: {'loss': 2.2987, 'grad_norm': 1.4567577838897705, 'learning_rate': 2.1168773563983145e-05, 'epoch': 1.7305389221556886}\n",
            "Captured loss: 2.2987 at step 2601\n",
            "Logs at step 2602: {'loss': 2.6525, 'grad_norm': 1.1074559688568115, 'learning_rate': 2.1157684630738526e-05, 'epoch': 1.731204258150366}\n",
            "Captured loss: 2.6525 at step 2602\n",
            "Logs at step 2603: {'loss': 2.6344, 'grad_norm': 1.3847124576568604, 'learning_rate': 2.11465956974939e-05, 'epoch': 1.7318695941450433}\n",
            "Captured loss: 2.6344 at step 2603\n",
            "Logs at step 2604: {'loss': 2.495, 'grad_norm': 1.5770761966705322, 'learning_rate': 2.113550676424928e-05, 'epoch': 1.7325349301397206}\n",
            "Captured loss: 2.495 at step 2604\n",
            "Logs at step 2605: {'loss': 2.5828, 'grad_norm': 1.2559930086135864, 'learning_rate': 2.112441783100466e-05, 'epoch': 1.7332002661343977}\n",
            "Captured loss: 2.5828 at step 2605\n",
            "Logs at step 2606: {'loss': 2.3725, 'grad_norm': 1.734142780303955, 'learning_rate': 2.1113328897760036e-05, 'epoch': 1.7338656021290753}\n",
            "Captured loss: 2.3725 at step 2606\n",
            "Logs at step 2607: {'loss': 2.4211, 'grad_norm': 1.2158607244491577, 'learning_rate': 2.1102239964515413e-05, 'epoch': 1.7345309381237524}\n",
            "Captured loss: 2.4211 at step 2607\n",
            "Logs at step 2608: {'loss': 2.5664, 'grad_norm': 1.0956816673278809, 'learning_rate': 2.109115103127079e-05, 'epoch': 1.73519627411843}\n",
            "Captured loss: 2.5664 at step 2608\n",
            "Logs at step 2609: {'loss': 2.6068, 'grad_norm': 1.0179696083068848, 'learning_rate': 2.1080062098026172e-05, 'epoch': 1.735861610113107}\n",
            "Captured loss: 2.6068 at step 2609\n",
            "Logs at step 2610: {'loss': 2.5464, 'grad_norm': 0.9737669825553894, 'learning_rate': 2.106897316478155e-05, 'epoch': 1.7365269461077846}\n",
            "Captured loss: 2.5464 at step 2610\n",
            "Logs at step 2611: {'loss': 2.6168, 'grad_norm': 1.7798863649368286, 'learning_rate': 2.1057884231536927e-05, 'epoch': 1.7371922821024617}\n",
            "Captured loss: 2.6168 at step 2611\n",
            "Logs at step 2612: {'loss': 2.6394, 'grad_norm': 1.8862673044204712, 'learning_rate': 2.1046795298292305e-05, 'epoch': 1.737857618097139}\n",
            "Captured loss: 2.6394 at step 2612\n",
            "Logs at step 2613: {'loss': 2.5998, 'grad_norm': 1.240222454071045, 'learning_rate': 2.1035706365047682e-05, 'epoch': 1.7385229540918163}\n",
            "Captured loss: 2.5998 at step 2613\n",
            "Logs at step 2614: {'loss': 2.5241, 'grad_norm': 0.8838265538215637, 'learning_rate': 2.1024617431803063e-05, 'epoch': 1.7391882900864937}\n",
            "Captured loss: 2.5241 at step 2614\n",
            "Logs at step 2615: {'loss': 2.461, 'grad_norm': 1.4240854978561401, 'learning_rate': 2.101352849855844e-05, 'epoch': 1.739853626081171}\n",
            "Captured loss: 2.461 at step 2615\n",
            "Logs at step 2616: {'loss': 2.6355, 'grad_norm': 1.8030637502670288, 'learning_rate': 2.1002439565313818e-05, 'epoch': 1.7405189620758483}\n",
            "Captured loss: 2.6355 at step 2616\n",
            "Logs at step 2617: {'loss': 2.4945, 'grad_norm': 1.361742615699768, 'learning_rate': 2.0991350632069196e-05, 'epoch': 1.7411842980705257}\n",
            "Captured loss: 2.4945 at step 2617\n",
            "Logs at step 2618: {'loss': 2.447, 'grad_norm': 1.2278149127960205, 'learning_rate': 2.0980261698824573e-05, 'epoch': 1.7418496340652028}\n",
            "Captured loss: 2.447 at step 2618\n",
            "Logs at step 2619: {'loss': 2.5155, 'grad_norm': 1.0369139909744263, 'learning_rate': 2.0969172765579954e-05, 'epoch': 1.7425149700598803}\n",
            "Captured loss: 2.5155 at step 2619\n",
            "Logs at step 2620: {'loss': 2.5771, 'grad_norm': 1.321960210800171, 'learning_rate': 2.095808383233533e-05, 'epoch': 1.7431803060545574}\n",
            "Captured loss: 2.5771 at step 2620\n",
            "Logs at step 2621: {'loss': 2.6057, 'grad_norm': 2.69246768951416, 'learning_rate': 2.094699489909071e-05, 'epoch': 1.743845642049235}\n",
            "Captured loss: 2.6057 at step 2621\n",
            "Logs at step 2622: {'loss': 2.588, 'grad_norm': 1.3764042854309082, 'learning_rate': 2.0935905965846087e-05, 'epoch': 1.744510978043912}\n",
            "Captured loss: 2.588 at step 2622\n",
            "Logs at step 2623: {'loss': 2.6021, 'grad_norm': 1.2836894989013672, 'learning_rate': 2.0924817032601464e-05, 'epoch': 1.7451763140385896}\n",
            "Captured loss: 2.6021 at step 2623\n",
            "Logs at step 2624: {'loss': 2.5718, 'grad_norm': 1.422110676765442, 'learning_rate': 2.0913728099356845e-05, 'epoch': 1.7458416500332667}\n",
            "Captured loss: 2.5718 at step 2624\n",
            "Logs at step 2625: {'loss': 2.4436, 'grad_norm': 1.0290216207504272, 'learning_rate': 2.090263916611222e-05, 'epoch': 1.746506986027944}\n",
            "Captured loss: 2.4436 at step 2625\n",
            "Logs at step 2626: {'loss': 2.8307, 'grad_norm': 2.3781304359436035, 'learning_rate': 2.08915502328676e-05, 'epoch': 1.7471723220226214}\n",
            "Captured loss: 2.8307 at step 2626\n",
            "Logs at step 2627: {'loss': 2.665, 'grad_norm': 1.2006787061691284, 'learning_rate': 2.0880461299622978e-05, 'epoch': 1.7478376580172987}\n",
            "Captured loss: 2.665 at step 2627\n",
            "Logs at step 2628: {'loss': 2.532, 'grad_norm': 1.3090165853500366, 'learning_rate': 2.0869372366378356e-05, 'epoch': 1.748502994011976}\n",
            "Captured loss: 2.532 at step 2628\n",
            "Logs at step 2629: {'loss': 2.126, 'grad_norm': 1.5560791492462158, 'learning_rate': 2.0858283433133733e-05, 'epoch': 1.7491683300066534}\n",
            "Captured loss: 2.126 at step 2629\n",
            "Logs at step 2630: {'loss': 2.5561, 'grad_norm': 1.1294969320297241, 'learning_rate': 2.084719449988911e-05, 'epoch': 1.7498336660013307}\n",
            "Captured loss: 2.5561 at step 2630\n",
            "Logs at step 2631: {'loss': 2.2791, 'grad_norm': 1.4364080429077148, 'learning_rate': 2.0836105566644488e-05, 'epoch': 1.7504990019960078}\n",
            "Captured loss: 2.2791 at step 2631\n",
            "Logs at step 2632: {'loss': 2.5696, 'grad_norm': 1.0074659585952759, 'learning_rate': 2.082501663339987e-05, 'epoch': 1.7511643379906854}\n",
            "Captured loss: 2.5696 at step 2632\n",
            "Logs at step 2633: {'loss': 2.5919, 'grad_norm': 1.2603508234024048, 'learning_rate': 2.0813927700155247e-05, 'epoch': 1.7518296739853625}\n",
            "Captured loss: 2.5919 at step 2633\n",
            "Logs at step 2634: {'loss': 2.634, 'grad_norm': 1.1704330444335938, 'learning_rate': 2.0802838766910624e-05, 'epoch': 1.75249500998004}\n",
            "Captured loss: 2.634 at step 2634\n",
            "Logs at step 2635: {'loss': 2.2902, 'grad_norm': 2.103783130645752, 'learning_rate': 2.0791749833666002e-05, 'epoch': 1.7531603459747171}\n",
            "Captured loss: 2.2902 at step 2635\n",
            "Logs at step 2636: {'loss': 2.409, 'grad_norm': 1.2618963718414307, 'learning_rate': 2.078066090042138e-05, 'epoch': 1.7538256819693947}\n",
            "Captured loss: 2.409 at step 2636\n",
            "Logs at step 2637: {'loss': 2.3709, 'grad_norm': 1.2630425691604614, 'learning_rate': 2.076957196717676e-05, 'epoch': 1.7544910179640718}\n",
            "Captured loss: 2.3709 at step 2637\n",
            "Logs at step 2638: {'loss': 2.332, 'grad_norm': 1.1152958869934082, 'learning_rate': 2.0758483033932134e-05, 'epoch': 1.7551563539587491}\n",
            "Captured loss: 2.332 at step 2638\n",
            "Logs at step 2639: {'loss': 2.4798, 'grad_norm': 1.5592066049575806, 'learning_rate': 2.0747394100687515e-05, 'epoch': 1.7558216899534265}\n",
            "Captured loss: 2.4798 at step 2639\n",
            "Logs at step 2640: {'loss': 2.4266, 'grad_norm': 1.1173638105392456, 'learning_rate': 2.0736305167442893e-05, 'epoch': 1.7564870259481038}\n",
            "Captured loss: 2.4266 at step 2640\n",
            "Logs at step 2641: {'loss': 2.5249, 'grad_norm': 1.1721817255020142, 'learning_rate': 2.072521623419827e-05, 'epoch': 1.7571523619427811}\n",
            "Captured loss: 2.5249 at step 2641\n",
            "Logs at step 2642: {'loss': 2.5701, 'grad_norm': 1.352644443511963, 'learning_rate': 2.0714127300953648e-05, 'epoch': 1.7578176979374585}\n",
            "Captured loss: 2.5701 at step 2642\n",
            "Logs at step 2643: {'loss': 2.2942, 'grad_norm': 1.582933783531189, 'learning_rate': 2.0703038367709026e-05, 'epoch': 1.7584830339321358}\n",
            "Captured loss: 2.2942 at step 2643\n",
            "Logs at step 2644: {'loss': 2.2534, 'grad_norm': 1.2093526124954224, 'learning_rate': 2.0691949434464407e-05, 'epoch': 1.759148369926813}\n",
            "Captured loss: 2.2534 at step 2644\n",
            "Logs at step 2645: {'loss': 2.5521, 'grad_norm': 1.3041541576385498, 'learning_rate': 2.0680860501219784e-05, 'epoch': 1.7598137059214904}\n",
            "Captured loss: 2.5521 at step 2645\n",
            "Logs at step 2646: {'loss': 2.1779, 'grad_norm': 1.4194194078445435, 'learning_rate': 2.066977156797516e-05, 'epoch': 1.7604790419161676}\n",
            "Captured loss: 2.1779 at step 2646\n",
            "Logs at step 2647: {'loss': 2.1577, 'grad_norm': 1.417267084121704, 'learning_rate': 2.065868263473054e-05, 'epoch': 1.761144377910845}\n",
            "Captured loss: 2.1577 at step 2647\n",
            "Logs at step 2648: {'loss': 2.1849, 'grad_norm': 1.3692824840545654, 'learning_rate': 2.0647593701485917e-05, 'epoch': 1.7618097139055222}\n",
            "Captured loss: 2.1849 at step 2648\n",
            "Logs at step 2649: {'loss': 2.5215, 'grad_norm': 1.5912712812423706, 'learning_rate': 2.0636504768241298e-05, 'epoch': 1.7624750499001998}\n",
            "Captured loss: 2.5215 at step 2649\n",
            "Logs at step 2650: {'loss': 2.5996, 'grad_norm': 1.282508134841919, 'learning_rate': 2.0625415834996675e-05, 'epoch': 1.7631403858948769}\n",
            "Captured loss: 2.5996 at step 2650\n",
            "Logs at step 2651: {'loss': 2.5576, 'grad_norm': 1.4992226362228394, 'learning_rate': 2.0614326901752053e-05, 'epoch': 1.7638057218895542}\n",
            "Captured loss: 2.5576 at step 2651\n",
            "Logs at step 2652: {'loss': 2.6587, 'grad_norm': 1.8542308807373047, 'learning_rate': 2.060323796850743e-05, 'epoch': 1.7644710578842315}\n",
            "Captured loss: 2.6587 at step 2652\n",
            "Logs at step 2653: {'loss': 2.5482, 'grad_norm': 0.9670027494430542, 'learning_rate': 2.0592149035262808e-05, 'epoch': 1.7651363938789089}\n",
            "Captured loss: 2.5482 at step 2653\n",
            "Logs at step 2654: {'loss': 2.539, 'grad_norm': 1.8484444618225098, 'learning_rate': 2.058106010201819e-05, 'epoch': 1.7658017298735862}\n",
            "Captured loss: 2.539 at step 2654\n",
            "Logs at step 2655: {'loss': 2.5649, 'grad_norm': 1.160699486732483, 'learning_rate': 2.0569971168773566e-05, 'epoch': 1.7664670658682635}\n",
            "Captured loss: 2.5649 at step 2655\n",
            "Logs at step 2656: {'loss': 2.2792, 'grad_norm': 1.6784296035766602, 'learning_rate': 2.0558882235528944e-05, 'epoch': 1.7671324018629408}\n",
            "Captured loss: 2.2792 at step 2656\n",
            "Logs at step 2657: {'loss': 2.272, 'grad_norm': 1.6120896339416504, 'learning_rate': 2.054779330228432e-05, 'epoch': 1.767797737857618}\n",
            "Captured loss: 2.272 at step 2657\n",
            "Logs at step 2658: {'loss': 2.5121, 'grad_norm': 1.0927759408950806, 'learning_rate': 2.05367043690397e-05, 'epoch': 1.7684630738522955}\n",
            "Captured loss: 2.5121 at step 2658\n",
            "Logs at step 2659: {'loss': 2.1542, 'grad_norm': 1.635723352432251, 'learning_rate': 2.052561543579508e-05, 'epoch': 1.7691284098469726}\n",
            "Captured loss: 2.1542 at step 2659\n",
            "Logs at step 2660: {'loss': 2.6226, 'grad_norm': 2.0859179496765137, 'learning_rate': 2.0514526502550454e-05, 'epoch': 1.7697937458416502}\n",
            "Captured loss: 2.6226 at step 2660\n",
            "Logs at step 2661: {'loss': 2.53, 'grad_norm': 1.674294352531433, 'learning_rate': 2.0503437569305832e-05, 'epoch': 1.7704590818363273}\n",
            "Captured loss: 2.53 at step 2661\n",
            "Logs at step 2662: {'loss': 1.9562, 'grad_norm': 1.808660864830017, 'learning_rate': 2.0492348636061213e-05, 'epoch': 1.7711244178310046}\n",
            "Captured loss: 1.9562 at step 2662\n",
            "Logs at step 2663: {'loss': 2.5879, 'grad_norm': 1.5495866537094116, 'learning_rate': 2.048125970281659e-05, 'epoch': 1.771789753825682}\n",
            "Captured loss: 2.5879 at step 2663\n",
            "Logs at step 2664: {'loss': 2.6071, 'grad_norm': 1.2800378799438477, 'learning_rate': 2.0470170769571968e-05, 'epoch': 1.7724550898203593}\n",
            "Captured loss: 2.6071 at step 2664\n",
            "Logs at step 2665: {'loss': 2.5101, 'grad_norm': 0.9990904927253723, 'learning_rate': 2.0459081836327345e-05, 'epoch': 1.7731204258150366}\n",
            "Captured loss: 2.5101 at step 2665\n",
            "Logs at step 2666: {'loss': 2.3586, 'grad_norm': 1.595262050628662, 'learning_rate': 2.0447992903082723e-05, 'epoch': 1.773785761809714}\n",
            "Captured loss: 2.3586 at step 2666\n",
            "Logs at step 2667: {'loss': 2.4612, 'grad_norm': 1.3003039360046387, 'learning_rate': 2.0436903969838104e-05, 'epoch': 1.7744510978043913}\n",
            "Captured loss: 2.4612 at step 2667\n",
            "Logs at step 2668: {'loss': 2.1999, 'grad_norm': 1.4736803770065308, 'learning_rate': 2.042581503659348e-05, 'epoch': 1.7751164337990686}\n",
            "Captured loss: 2.1999 at step 2668\n",
            "Logs at step 2669: {'loss': 2.361, 'grad_norm': 1.8748904466629028, 'learning_rate': 2.041472610334886e-05, 'epoch': 1.775781769793746}\n",
            "Captured loss: 2.361 at step 2669\n",
            "Logs at step 2670: {'loss': 2.6864, 'grad_norm': 1.1680123805999756, 'learning_rate': 2.0403637170104236e-05, 'epoch': 1.776447105788423}\n",
            "Captured loss: 2.6864 at step 2670\n",
            "Logs at step 2671: {'loss': 2.3787, 'grad_norm': 1.6206775903701782, 'learning_rate': 2.0392548236859614e-05, 'epoch': 1.7771124417831006}\n",
            "Captured loss: 2.3787 at step 2671\n",
            "Logs at step 2672: {'loss': 2.4566, 'grad_norm': 1.4286432266235352, 'learning_rate': 2.0381459303614995e-05, 'epoch': 1.7777777777777777}\n",
            "Captured loss: 2.4566 at step 2672\n",
            "Logs at step 2673: {'loss': 2.2862, 'grad_norm': 1.413358449935913, 'learning_rate': 2.037037037037037e-05, 'epoch': 1.7784431137724552}\n",
            "Captured loss: 2.2862 at step 2673\n",
            "Logs at step 2674: {'loss': 2.7889, 'grad_norm': 1.527637004852295, 'learning_rate': 2.035928143712575e-05, 'epoch': 1.7791084497671323}\n",
            "Captured loss: 2.7889 at step 2674\n",
            "Logs at step 2675: {'loss': 2.2749, 'grad_norm': 1.2878831624984741, 'learning_rate': 2.0348192503881128e-05, 'epoch': 1.7797737857618097}\n",
            "Captured loss: 2.2749 at step 2675\n",
            "Logs at step 2676: {'loss': 2.6372, 'grad_norm': 1.871313452720642, 'learning_rate': 2.0337103570636505e-05, 'epoch': 1.780439121756487}\n",
            "Captured loss: 2.6372 at step 2676\n",
            "Logs at step 2677: {'loss': 2.4842, 'grad_norm': 1.341543197631836, 'learning_rate': 2.0326014637391883e-05, 'epoch': 1.7811044577511643}\n",
            "Captured loss: 2.4842 at step 2677\n",
            "Logs at step 2678: {'loss': 2.3338, 'grad_norm': 1.5102554559707642, 'learning_rate': 2.031492570414726e-05, 'epoch': 1.7817697937458417}\n",
            "Captured loss: 2.3338 at step 2678\n",
            "Logs at step 2679: {'loss': 2.5549, 'grad_norm': 1.552615761756897, 'learning_rate': 2.030383677090264e-05, 'epoch': 1.782435129740519}\n",
            "Captured loss: 2.5549 at step 2679\n",
            "Logs at step 2680: {'loss': 2.4923, 'grad_norm': 1.5167055130004883, 'learning_rate': 2.029274783765802e-05, 'epoch': 1.7831004657351963}\n",
            "Captured loss: 2.4923 at step 2680\n",
            "Logs at step 2681: {'loss': 2.4753, 'grad_norm': 1.4489463567733765, 'learning_rate': 2.0281658904413396e-05, 'epoch': 1.7837658017298734}\n",
            "Captured loss: 2.4753 at step 2681\n",
            "Logs at step 2682: {'loss': 2.6334, 'grad_norm': 1.612431287765503, 'learning_rate': 2.0270569971168774e-05, 'epoch': 1.784431137724551}\n",
            "Captured loss: 2.6334 at step 2682\n",
            "Logs at step 2683: {'loss': 2.57, 'grad_norm': 1.3946985006332397, 'learning_rate': 2.025948103792415e-05, 'epoch': 1.785096473719228}\n",
            "Captured loss: 2.57 at step 2683\n",
            "Logs at step 2684: {'loss': 2.4302, 'grad_norm': 1.464461326599121, 'learning_rate': 2.0248392104679532e-05, 'epoch': 1.7857618097139056}\n",
            "Captured loss: 2.4302 at step 2684\n",
            "Logs at step 2685: {'loss': 2.518, 'grad_norm': 1.2725248336791992, 'learning_rate': 2.023730317143491e-05, 'epoch': 1.7864271457085827}\n",
            "Captured loss: 2.518 at step 2685\n",
            "Logs at step 2686: {'loss': 2.5512, 'grad_norm': 1.8566854000091553, 'learning_rate': 2.0226214238190287e-05, 'epoch': 1.7870924817032603}\n",
            "Captured loss: 2.5512 at step 2686\n",
            "Logs at step 2687: {'loss': 2.217, 'grad_norm': 1.3707586526870728, 'learning_rate': 2.0215125304945665e-05, 'epoch': 1.7877578176979374}\n",
            "Captured loss: 2.217 at step 2687\n",
            "Logs at step 2688: {'loss': 2.4842, 'grad_norm': 1.2761656045913696, 'learning_rate': 2.0204036371701043e-05, 'epoch': 1.7884231536926147}\n",
            "Captured loss: 2.4842 at step 2688\n",
            "Logs at step 2689: {'loss': 2.5226, 'grad_norm': 1.2329027652740479, 'learning_rate': 2.0192947438456423e-05, 'epoch': 1.789088489687292}\n",
            "Captured loss: 2.5226 at step 2689\n",
            "Logs at step 2690: {'loss': 2.3003, 'grad_norm': 2.354766368865967, 'learning_rate': 2.01818585052118e-05, 'epoch': 1.7897538256819694}\n",
            "Captured loss: 2.3003 at step 2690\n",
            "Logs at step 2691: {'loss': 2.5434, 'grad_norm': 1.132806658744812, 'learning_rate': 2.017076957196718e-05, 'epoch': 1.7904191616766467}\n",
            "Captured loss: 2.5434 at step 2691\n",
            "Logs at step 2692: {'loss': 2.5905, 'grad_norm': 1.325190782546997, 'learning_rate': 2.0159680638722556e-05, 'epoch': 1.791084497671324}\n",
            "Captured loss: 2.5905 at step 2692\n",
            "Logs at step 2693: {'loss': 2.673, 'grad_norm': 1.25132155418396, 'learning_rate': 2.0148591705477934e-05, 'epoch': 1.7917498336660014}\n",
            "Captured loss: 2.673 at step 2693\n",
            "Logs at step 2694: {'loss': 2.3057, 'grad_norm': 1.2987477779388428, 'learning_rate': 2.0137502772233315e-05, 'epoch': 1.7924151696606785}\n",
            "Captured loss: 2.3057 at step 2694\n",
            "Logs at step 2695: {'loss': 2.6008, 'grad_norm': 0.9041915535926819, 'learning_rate': 2.012641383898869e-05, 'epoch': 1.793080505655356}\n",
            "Captured loss: 2.6008 at step 2695\n",
            "Logs at step 2696: {'loss': 2.1833, 'grad_norm': 1.8884515762329102, 'learning_rate': 2.0115324905744066e-05, 'epoch': 1.7937458416500331}\n",
            "Captured loss: 2.1833 at step 2696\n",
            "Logs at step 2697: {'loss': 2.6443, 'grad_norm': 1.3593769073486328, 'learning_rate': 2.0104235972499447e-05, 'epoch': 1.7944111776447107}\n",
            "Captured loss: 2.6443 at step 2697\n",
            "Logs at step 2698: {'loss': 2.611, 'grad_norm': 1.990859031677246, 'learning_rate': 2.0093147039254825e-05, 'epoch': 1.7950765136393878}\n",
            "Captured loss: 2.611 at step 2698\n",
            "Logs at step 2699: {'loss': 2.4932, 'grad_norm': 1.209172248840332, 'learning_rate': 2.0082058106010202e-05, 'epoch': 1.7957418496340654}\n",
            "Captured loss: 2.4932 at step 2699\n",
            "Logs at step 2700: {'loss': 2.4712, 'grad_norm': 1.6775392293930054, 'learning_rate': 2.007096917276558e-05, 'epoch': 1.7964071856287425}\n",
            "Captured loss: 2.4712 at step 2700\n",
            "Logs at step 2701: {'loss': 2.517, 'grad_norm': 1.2143677473068237, 'learning_rate': 2.0059880239520957e-05, 'epoch': 1.7970725216234198}\n",
            "Captured loss: 2.517 at step 2701\n",
            "Logs at step 2702: {'loss': 1.9365, 'grad_norm': 2.099243640899658, 'learning_rate': 2.004879130627634e-05, 'epoch': 1.7977378576180971}\n",
            "Captured loss: 1.9365 at step 2702\n",
            "Logs at step 2703: {'loss': 2.4472, 'grad_norm': 1.579260230064392, 'learning_rate': 2.0037702373031716e-05, 'epoch': 1.7984031936127745}\n",
            "Captured loss: 2.4472 at step 2703\n",
            "Logs at step 2704: {'loss': 2.5011, 'grad_norm': 1.5945494174957275, 'learning_rate': 2.0026613439787093e-05, 'epoch': 1.7990685296074518}\n",
            "Captured loss: 2.5011 at step 2704\n",
            "Logs at step 2705: {'loss': 2.5584, 'grad_norm': 1.3026206493377686, 'learning_rate': 2.001552450654247e-05, 'epoch': 1.7997338656021291}\n",
            "Captured loss: 2.5584 at step 2705\n",
            "Logs at step 2706: {'loss': 2.5596, 'grad_norm': 1.2277512550354004, 'learning_rate': 2.000443557329785e-05, 'epoch': 1.8003992015968064}\n",
            "Captured loss: 2.5596 at step 2706\n",
            "Logs at step 2707: {'loss': 2.6648, 'grad_norm': 1.3294346332550049, 'learning_rate': 1.999334664005323e-05, 'epoch': 1.8010645375914835}\n",
            "Captured loss: 2.6648 at step 2707\n",
            "Logs at step 2708: {'loss': 2.422, 'grad_norm': 1.2697136402130127, 'learning_rate': 1.9982257706808604e-05, 'epoch': 1.801729873586161}\n",
            "Captured loss: 2.422 at step 2708\n",
            "Logs at step 2709: {'loss': 2.1673, 'grad_norm': 1.608177661895752, 'learning_rate': 1.9971168773563985e-05, 'epoch': 1.8023952095808382}\n",
            "Captured loss: 2.1673 at step 2709\n",
            "Logs at step 2710: {'loss': 2.5953, 'grad_norm': 1.3298242092132568, 'learning_rate': 1.9960079840319362e-05, 'epoch': 1.8030605455755158}\n",
            "Captured loss: 2.5953 at step 2710\n",
            "Logs at step 2711: {'loss': 2.4894, 'grad_norm': 1.0519828796386719, 'learning_rate': 1.994899090707474e-05, 'epoch': 1.8037258815701929}\n",
            "Captured loss: 2.4894 at step 2711\n",
            "Logs at step 2712: {'loss': 2.151, 'grad_norm': 2.313406467437744, 'learning_rate': 1.993790197383012e-05, 'epoch': 1.8043912175648704}\n",
            "Captured loss: 2.151 at step 2712\n",
            "Logs at step 2713: {'loss': 2.4228, 'grad_norm': 1.6715316772460938, 'learning_rate': 1.9926813040585495e-05, 'epoch': 1.8050565535595475}\n",
            "Captured loss: 2.4228 at step 2713\n",
            "Logs at step 2714: {'loss': 2.5015, 'grad_norm': 1.5237504243850708, 'learning_rate': 1.9915724107340876e-05, 'epoch': 1.8057218895542249}\n",
            "Captured loss: 2.5015 at step 2714\n",
            "Logs at step 2715: {'loss': 2.6023, 'grad_norm': 1.195757508277893, 'learning_rate': 1.9904635174096253e-05, 'epoch': 1.8063872255489022}\n",
            "Captured loss: 2.6023 at step 2715\n",
            "Logs at step 2716: {'loss': 2.5725, 'grad_norm': 1.481877088546753, 'learning_rate': 1.989354624085163e-05, 'epoch': 1.8070525615435795}\n",
            "Captured loss: 2.5725 at step 2716\n",
            "Logs at step 2717: {'loss': 2.3157, 'grad_norm': 1.279008150100708, 'learning_rate': 1.988245730760701e-05, 'epoch': 1.8077178975382568}\n",
            "Captured loss: 2.3157 at step 2717\n",
            "Logs at step 2718: {'loss': 2.5768, 'grad_norm': 1.535930871963501, 'learning_rate': 1.9871368374362386e-05, 'epoch': 1.8083832335329342}\n",
            "Captured loss: 2.5768 at step 2718\n",
            "Logs at step 2719: {'loss': 2.502, 'grad_norm': 1.0504695177078247, 'learning_rate': 1.9860279441117767e-05, 'epoch': 1.8090485695276115}\n",
            "Captured loss: 2.502 at step 2719\n",
            "Logs at step 2720: {'loss': 2.4492, 'grad_norm': 1.148406982421875, 'learning_rate': 1.9849190507873144e-05, 'epoch': 1.8097139055222886}\n",
            "Captured loss: 2.4492 at step 2720\n",
            "Logs at step 2721: {'loss': 2.5556, 'grad_norm': 1.5322977304458618, 'learning_rate': 1.9838101574628522e-05, 'epoch': 1.8103792415169662}\n",
            "Captured loss: 2.5556 at step 2721\n",
            "Logs at step 2722: {'loss': 2.5189, 'grad_norm': 1.8400688171386719, 'learning_rate': 1.98270126413839e-05, 'epoch': 1.8110445775116433}\n",
            "Captured loss: 2.5189 at step 2722\n",
            "Logs at step 2723: {'loss': 2.3, 'grad_norm': 1.430746078491211, 'learning_rate': 1.9815923708139277e-05, 'epoch': 1.8117099135063208}\n",
            "Captured loss: 2.3 at step 2723\n",
            "Logs at step 2724: {'loss': 2.4861, 'grad_norm': 1.0866138935089111, 'learning_rate': 1.9804834774894658e-05, 'epoch': 1.812375249500998}\n",
            "Captured loss: 2.4861 at step 2724\n",
            "Logs at step 2725: {'loss': 2.4338, 'grad_norm': 1.8367103338241577, 'learning_rate': 1.9793745841650036e-05, 'epoch': 1.8130405854956753}\n",
            "Captured loss: 2.4338 at step 2725\n",
            "Logs at step 2726: {'loss': 2.6603, 'grad_norm': 1.3828333616256714, 'learning_rate': 1.978265690840541e-05, 'epoch': 1.8137059214903526}\n",
            "Captured loss: 2.6603 at step 2726\n",
            "Logs at step 2727: {'loss': 2.3338, 'grad_norm': 1.4939732551574707, 'learning_rate': 1.977156797516079e-05, 'epoch': 1.81437125748503}\n",
            "Captured loss: 2.3338 at step 2727\n",
            "Logs at step 2728: {'loss': 2.0384, 'grad_norm': 1.4359811544418335, 'learning_rate': 1.9760479041916168e-05, 'epoch': 1.8150365934797072}\n",
            "Captured loss: 2.0384 at step 2728\n",
            "Logs at step 2729: {'loss': 2.5437, 'grad_norm': 1.1991229057312012, 'learning_rate': 1.974939010867155e-05, 'epoch': 1.8157019294743846}\n",
            "Captured loss: 2.5437 at step 2729\n",
            "Logs at step 2730: {'loss': 2.3958, 'grad_norm': 1.5198462009429932, 'learning_rate': 1.9738301175426923e-05, 'epoch': 1.816367265469062}\n",
            "Captured loss: 2.3958 at step 2730\n",
            "Logs at step 2731: {'loss': 2.3166, 'grad_norm': 1.4252207279205322, 'learning_rate': 1.97272122421823e-05, 'epoch': 1.8170326014637392}\n",
            "Captured loss: 2.3166 at step 2731\n",
            "Logs at step 2732: {'loss': 2.5349, 'grad_norm': 1.596213459968567, 'learning_rate': 1.9716123308937682e-05, 'epoch': 1.8176979374584166}\n",
            "Captured loss: 2.5349 at step 2732\n",
            "Logs at step 2733: {'loss': 2.2547, 'grad_norm': 1.7365669012069702, 'learning_rate': 1.970503437569306e-05, 'epoch': 1.8183632734530937}\n",
            "Captured loss: 2.2547 at step 2733\n",
            "Logs at step 2734: {'loss': 2.5298, 'grad_norm': 1.3075957298278809, 'learning_rate': 1.969394544244844e-05, 'epoch': 1.8190286094477712}\n",
            "Captured loss: 2.5298 at step 2734\n",
            "Logs at step 2735: {'loss': 2.2058, 'grad_norm': 1.7904762029647827, 'learning_rate': 1.9682856509203814e-05, 'epoch': 1.8196939454424483}\n",
            "Captured loss: 2.2058 at step 2735\n",
            "Logs at step 2736: {'loss': 2.2257, 'grad_norm': 1.2562731504440308, 'learning_rate': 1.9671767575959192e-05, 'epoch': 1.8203592814371259}\n",
            "Captured loss: 2.2257 at step 2736\n",
            "Logs at step 2737: {'loss': 2.5597, 'grad_norm': 1.1676483154296875, 'learning_rate': 1.9660678642714573e-05, 'epoch': 1.821024617431803}\n",
            "Captured loss: 2.5597 at step 2737\n",
            "Logs at step 2738: {'loss': 2.2103, 'grad_norm': 1.424517035484314, 'learning_rate': 1.964958970946995e-05, 'epoch': 1.8216899534264803}\n",
            "Captured loss: 2.2103 at step 2738\n",
            "Logs at step 2739: {'loss': 2.7674, 'grad_norm': 1.51301109790802, 'learning_rate': 1.9638500776225328e-05, 'epoch': 1.8223552894211577}\n",
            "Captured loss: 2.7674 at step 2739\n",
            "Logs at step 2740: {'loss': 2.5259, 'grad_norm': 0.90988689661026, 'learning_rate': 1.9627411842980706e-05, 'epoch': 1.823020625415835}\n",
            "Captured loss: 2.5259 at step 2740\n",
            "Logs at step 2741: {'loss': 2.5785, 'grad_norm': 1.2879506349563599, 'learning_rate': 1.9616322909736083e-05, 'epoch': 1.8236859614105123}\n",
            "Captured loss: 2.5785 at step 2741\n",
            "Logs at step 2742: {'loss': 2.2004, 'grad_norm': 1.4862470626831055, 'learning_rate': 1.9605233976491464e-05, 'epoch': 1.8243512974051896}\n",
            "Captured loss: 2.2004 at step 2742\n",
            "Logs at step 2743: {'loss': 2.5336, 'grad_norm': 1.010435700416565, 'learning_rate': 1.9594145043246838e-05, 'epoch': 1.825016633399867}\n",
            "Captured loss: 2.5336 at step 2743\n",
            "Logs at step 2744: {'loss': 2.5867, 'grad_norm': 2.147921085357666, 'learning_rate': 1.958305611000222e-05, 'epoch': 1.825681969394544}\n",
            "Captured loss: 2.5867 at step 2744\n",
            "Logs at step 2745: {'loss': 2.6004, 'grad_norm': 1.4910587072372437, 'learning_rate': 1.9571967176757597e-05, 'epoch': 1.8263473053892216}\n",
            "Captured loss: 2.6004 at step 2745\n",
            "Logs at step 2746: {'loss': 2.6071, 'grad_norm': 1.0795228481292725, 'learning_rate': 1.9560878243512974e-05, 'epoch': 1.8270126413838987}\n",
            "Captured loss: 2.6071 at step 2746\n",
            "Logs at step 2747: {'loss': 2.5441, 'grad_norm': 1.3729194402694702, 'learning_rate': 1.9549789310268355e-05, 'epoch': 1.8276779773785763}\n",
            "Captured loss: 2.5441 at step 2747\n",
            "Logs at step 2748: {'loss': 2.5082, 'grad_norm': 1.5562198162078857, 'learning_rate': 1.953870037702373e-05, 'epoch': 1.8283433133732534}\n",
            "Captured loss: 2.5082 at step 2748\n",
            "Logs at step 2749: {'loss': 2.5784, 'grad_norm': 1.0265337228775024, 'learning_rate': 1.952761144377911e-05, 'epoch': 1.829008649367931}\n",
            "Captured loss: 2.5784 at step 2749\n",
            "Logs at step 2750: {'loss': 2.4696, 'grad_norm': 1.4580316543579102, 'learning_rate': 1.9516522510534488e-05, 'epoch': 1.829673985362608}\n",
            "Captured loss: 2.4696 at step 2750\n",
            "Logs at step 2751: {'loss': 2.6398, 'grad_norm': 1.1307921409606934, 'learning_rate': 1.9505433577289865e-05, 'epoch': 1.8303393213572854}\n",
            "Captured loss: 2.6398 at step 2751\n",
            "Logs at step 2752: {'loss': 2.5812, 'grad_norm': 1.5289485454559326, 'learning_rate': 1.9494344644045243e-05, 'epoch': 1.8310046573519627}\n",
            "Captured loss: 2.5812 at step 2752\n",
            "Logs at step 2753: {'loss': 2.2431, 'grad_norm': 1.3549901247024536, 'learning_rate': 1.948325571080062e-05, 'epoch': 1.83166999334664}\n",
            "Captured loss: 2.2431 at step 2753\n",
            "Logs at step 2754: {'loss': 2.52, 'grad_norm': 1.0797673463821411, 'learning_rate': 1.9472166777556e-05, 'epoch': 1.8323353293413174}\n",
            "Captured loss: 2.52 at step 2754\n",
            "Logs at step 2755: {'loss': 2.6425, 'grad_norm': 1.316174030303955, 'learning_rate': 1.946107784431138e-05, 'epoch': 1.8330006653359947}\n",
            "Captured loss: 2.6425 at step 2755\n",
            "Logs at step 2756: {'loss': 2.5493, 'grad_norm': 1.075544834136963, 'learning_rate': 1.9449988911066757e-05, 'epoch': 1.833666001330672}\n",
            "Captured loss: 2.5493 at step 2756\n",
            "Logs at step 2757: {'loss': 2.5913, 'grad_norm': 1.266352891921997, 'learning_rate': 1.9438899977822134e-05, 'epoch': 1.8343313373253491}\n",
            "Captured loss: 2.5913 at step 2757\n",
            "Logs at step 2758: {'loss': 2.3933, 'grad_norm': 1.0861977338790894, 'learning_rate': 1.9427811044577512e-05, 'epoch': 1.8349966733200267}\n",
            "Captured loss: 2.3933 at step 2758\n",
            "Logs at step 2759: {'loss': 2.5232, 'grad_norm': 1.9784636497497559, 'learning_rate': 1.9416722111332893e-05, 'epoch': 1.8356620093147038}\n",
            "Captured loss: 2.5232 at step 2759\n",
            "Logs at step 2760: {'loss': 2.4057, 'grad_norm': 1.458892583847046, 'learning_rate': 1.940563317808827e-05, 'epoch': 1.8363273453093814}\n",
            "Captured loss: 2.4057 at step 2760\n",
            "Logs at step 2761: {'loss': 2.4733, 'grad_norm': 1.1468067169189453, 'learning_rate': 1.9394544244843644e-05, 'epoch': 1.8369926813040585}\n",
            "Captured loss: 2.4733 at step 2761\n",
            "Logs at step 2762: {'loss': 2.5824, 'grad_norm': 2.1532256603240967, 'learning_rate': 1.9383455311599025e-05, 'epoch': 1.837658017298736}\n",
            "Captured loss: 2.5824 at step 2762\n",
            "Logs at step 2763: {'loss': 2.5212, 'grad_norm': 1.0902254581451416, 'learning_rate': 1.9372366378354403e-05, 'epoch': 1.8383233532934131}\n",
            "Captured loss: 2.5212 at step 2763\n",
            "Logs at step 2764: {'loss': 2.1305, 'grad_norm': 1.5194058418273926, 'learning_rate': 1.9361277445109784e-05, 'epoch': 1.8389886892880905}\n",
            "Captured loss: 2.1305 at step 2764\n",
            "Logs at step 2765: {'loss': 2.2197, 'grad_norm': 1.2969239950180054, 'learning_rate': 1.9350188511865158e-05, 'epoch': 1.8396540252827678}\n",
            "Captured loss: 2.2197 at step 2765\n",
            "Logs at step 2766: {'loss': 2.1316, 'grad_norm': 1.8958617448806763, 'learning_rate': 1.9339099578620535e-05, 'epoch': 1.840319361277445}\n",
            "Captured loss: 2.1316 at step 2766\n",
            "Logs at step 2767: {'loss': 2.4757, 'grad_norm': 1.1711006164550781, 'learning_rate': 1.9328010645375916e-05, 'epoch': 1.8409846972721224}\n",
            "Captured loss: 2.4757 at step 2767\n",
            "Logs at step 2768: {'loss': 2.6119, 'grad_norm': 1.1075994968414307, 'learning_rate': 1.9316921712131294e-05, 'epoch': 1.8416500332667998}\n",
            "Captured loss: 2.6119 at step 2768\n",
            "Logs at step 2769: {'loss': 2.3971, 'grad_norm': 1.3829915523529053, 'learning_rate': 1.9305832778886675e-05, 'epoch': 1.842315369261477}\n",
            "Captured loss: 2.3971 at step 2769\n",
            "Logs at step 2770: {'loss': 2.4252, 'grad_norm': 1.4646258354187012, 'learning_rate': 1.929474384564205e-05, 'epoch': 1.8429807052561542}\n",
            "Captured loss: 2.4252 at step 2770\n",
            "Logs at step 2771: {'loss': 2.5002, 'grad_norm': 1.1306352615356445, 'learning_rate': 1.9283654912397427e-05, 'epoch': 1.8436460412508318}\n",
            "Captured loss: 2.5002 at step 2771\n",
            "Logs at step 2772: {'loss': 2.6587, 'grad_norm': 2.14841365814209, 'learning_rate': 1.9272565979152808e-05, 'epoch': 1.8443113772455089}\n",
            "Captured loss: 2.6587 at step 2772\n",
            "Logs at step 2773: {'loss': 2.6044, 'grad_norm': 1.3659056425094604, 'learning_rate': 1.9261477045908185e-05, 'epoch': 1.8449767132401864}\n",
            "Captured loss: 2.6044 at step 2773\n",
            "Logs at step 2774: {'loss': 2.3196, 'grad_norm': 1.6743719577789307, 'learning_rate': 1.9250388112663563e-05, 'epoch': 1.8456420492348635}\n",
            "Captured loss: 2.3196 at step 2774\n",
            "Logs at step 2775: {'loss': 2.4691, 'grad_norm': 1.0172492265701294, 'learning_rate': 1.923929917941894e-05, 'epoch': 1.846307385229541}\n",
            "Captured loss: 2.4691 at step 2775\n",
            "Logs at step 2776: {'loss': 2.4908, 'grad_norm': 0.9794119000434875, 'learning_rate': 1.9228210246174318e-05, 'epoch': 1.8469727212242182}\n",
            "Captured loss: 2.4908 at step 2776\n",
            "Logs at step 2777: {'loss': 2.553, 'grad_norm': 1.511422872543335, 'learning_rate': 1.92171213129297e-05, 'epoch': 1.8476380572188955}\n",
            "Captured loss: 2.553 at step 2777\n",
            "Logs at step 2778: {'loss': 2.3125, 'grad_norm': 1.849448800086975, 'learning_rate': 1.9206032379685073e-05, 'epoch': 1.8483033932135728}\n",
            "Captured loss: 2.3125 at step 2778\n",
            "Logs at step 2779: {'loss': 2.5354, 'grad_norm': 1.1903849840164185, 'learning_rate': 1.9194943446440454e-05, 'epoch': 1.8489687292082502}\n",
            "Captured loss: 2.5354 at step 2779\n",
            "Logs at step 2780: {'loss': 2.5483, 'grad_norm': 1.160536289215088, 'learning_rate': 1.918385451319583e-05, 'epoch': 1.8496340652029275}\n",
            "Captured loss: 2.5483 at step 2780\n",
            "Logs at step 2781: {'loss': 2.4741, 'grad_norm': 0.9860550761222839, 'learning_rate': 1.917276557995121e-05, 'epoch': 1.8502994011976048}\n",
            "Captured loss: 2.4741 at step 2781\n",
            "Logs at step 2782: {'loss': 2.6387, 'grad_norm': 1.8457149267196655, 'learning_rate': 1.916167664670659e-05, 'epoch': 1.8509647371922822}\n",
            "Captured loss: 2.6387 at step 2782\n",
            "Logs at step 2783: {'loss': 2.4524, 'grad_norm': 1.268518090248108, 'learning_rate': 1.9150587713461964e-05, 'epoch': 1.8516300731869593}\n",
            "Captured loss: 2.4524 at step 2783\n",
            "Logs at step 2784: {'loss': 2.3876, 'grad_norm': 1.3678510189056396, 'learning_rate': 1.9139498780217345e-05, 'epoch': 1.8522954091816368}\n",
            "Captured loss: 2.3876 at step 2784\n",
            "Logs at step 2785: {'loss': 2.5764, 'grad_norm': 1.1632243394851685, 'learning_rate': 1.9128409846972723e-05, 'epoch': 1.852960745176314}\n",
            "Captured loss: 2.5764 at step 2785\n",
            "Logs at step 2786: {'loss': 2.5713, 'grad_norm': 1.1689454317092896, 'learning_rate': 1.91173209137281e-05, 'epoch': 1.8536260811709915}\n",
            "Captured loss: 2.5713 at step 2786\n",
            "Logs at step 2787: {'loss': 2.5012, 'grad_norm': 1.2420947551727295, 'learning_rate': 1.9106231980483478e-05, 'epoch': 1.8542914171656686}\n",
            "Captured loss: 2.5012 at step 2787\n",
            "Logs at step 2788: {'loss': 2.6422, 'grad_norm': 1.4426894187927246, 'learning_rate': 1.9095143047238855e-05, 'epoch': 1.8549567531603461}\n",
            "Captured loss: 2.6422 at step 2788\n",
            "Logs at step 2789: {'loss': 2.4879, 'grad_norm': 0.935998797416687, 'learning_rate': 1.9084054113994236e-05, 'epoch': 1.8556220891550232}\n",
            "Captured loss: 2.4879 at step 2789\n",
            "Logs at step 2790: {'loss': 2.3647, 'grad_norm': 1.252525806427002, 'learning_rate': 1.9072965180749614e-05, 'epoch': 1.8562874251497006}\n",
            "Captured loss: 2.3647 at step 2790\n",
            "Logs at step 2791: {'loss': 2.6174, 'grad_norm': 1.1393811702728271, 'learning_rate': 1.906187624750499e-05, 'epoch': 1.856952761144378}\n",
            "Captured loss: 2.6174 at step 2791\n",
            "Logs at step 2792: {'loss': 2.4722, 'grad_norm': 1.5148485898971558, 'learning_rate': 1.905078731426037e-05, 'epoch': 1.8576180971390552}\n",
            "Captured loss: 2.4722 at step 2792\n",
            "Logs at step 2793: {'loss': 2.5493, 'grad_norm': 1.1784403324127197, 'learning_rate': 1.9039698381015746e-05, 'epoch': 1.8582834331337326}\n",
            "Captured loss: 2.5493 at step 2793\n",
            "Logs at step 2794: {'loss': 2.2544, 'grad_norm': 1.8738598823547363, 'learning_rate': 1.9028609447771127e-05, 'epoch': 1.85894876912841}\n",
            "Captured loss: 2.2544 at step 2794\n",
            "Logs at step 2795: {'loss': 2.3884, 'grad_norm': 1.441693663597107, 'learning_rate': 1.9017520514526505e-05, 'epoch': 1.8596141051230872}\n",
            "Captured loss: 2.3884 at step 2795\n",
            "Logs at step 2796: {'loss': 2.2079, 'grad_norm': 1.252137541770935, 'learning_rate': 1.900643158128188e-05, 'epoch': 1.8602794411177643}\n",
            "Captured loss: 2.2079 at step 2796\n",
            "Logs at step 2797: {'loss': 2.2159, 'grad_norm': 1.3878034353256226, 'learning_rate': 1.899534264803726e-05, 'epoch': 1.8609447771124419}\n",
            "Captured loss: 2.2159 at step 2797\n",
            "Logs at step 2798: {'loss': 1.9281, 'grad_norm': 1.5530308485031128, 'learning_rate': 1.8984253714792637e-05, 'epoch': 1.861610113107119}\n",
            "Captured loss: 1.9281 at step 2798\n",
            "Logs at step 2799: {'loss': 2.5086, 'grad_norm': 1.150122046470642, 'learning_rate': 1.897316478154802e-05, 'epoch': 1.8622754491017965}\n",
            "Captured loss: 2.5086 at step 2799\n",
            "Logs at step 2800: {'loss': 2.6311, 'grad_norm': 1.7489756345748901, 'learning_rate': 1.8962075848303393e-05, 'epoch': 1.8629407850964737}\n",
            "Captured loss: 2.6311 at step 2800\n",
            "Logs at step 2801: {'loss': 2.6891, 'grad_norm': 1.864168405532837, 'learning_rate': 1.895098691505877e-05, 'epoch': 1.863606121091151}\n",
            "Captured loss: 2.6891 at step 2801\n",
            "Logs at step 2802: {'loss': 2.546, 'grad_norm': 1.1829475164413452, 'learning_rate': 1.893989798181415e-05, 'epoch': 1.8642714570858283}\n",
            "Captured loss: 2.546 at step 2802\n",
            "Logs at step 2803: {'loss': 2.3717, 'grad_norm': 2.651808261871338, 'learning_rate': 1.892880904856953e-05, 'epoch': 1.8649367930805056}\n",
            "Captured loss: 2.3717 at step 2803\n",
            "Logs at step 2804: {'loss': 2.4749, 'grad_norm': 1.269923448562622, 'learning_rate': 1.891772011532491e-05, 'epoch': 1.865602129075183}\n",
            "Captured loss: 2.4749 at step 2804\n",
            "Logs at step 2805: {'loss': 2.5115, 'grad_norm': 1.5846402645111084, 'learning_rate': 1.8906631182080284e-05, 'epoch': 1.8662674650698603}\n",
            "Captured loss: 2.5115 at step 2805\n",
            "Logs at step 2806: {'loss': 2.4954, 'grad_norm': 1.608443260192871, 'learning_rate': 1.889554224883566e-05, 'epoch': 1.8669328010645376}\n",
            "Captured loss: 2.4954 at step 2806\n",
            "Logs at step 2807: {'loss': 2.6825, 'grad_norm': 1.4242662191390991, 'learning_rate': 1.8884453315591042e-05, 'epoch': 1.867598137059215}\n",
            "Captured loss: 2.6825 at step 2807\n",
            "Logs at step 2808: {'loss': 2.2298, 'grad_norm': 2.0562522411346436, 'learning_rate': 1.887336438234642e-05, 'epoch': 1.8682634730538923}\n",
            "Captured loss: 2.2298 at step 2808\n",
            "Logs at step 2809: {'loss': 2.4623, 'grad_norm': 1.4005588293075562, 'learning_rate': 1.8862275449101797e-05, 'epoch': 1.8689288090485694}\n",
            "Captured loss: 2.4623 at step 2809\n",
            "Logs at step 2810: {'loss': 2.5633, 'grad_norm': 1.404911994934082, 'learning_rate': 1.8851186515857175e-05, 'epoch': 1.869594145043247}\n",
            "Captured loss: 2.5633 at step 2810\n",
            "Logs at step 2811: {'loss': 2.1649, 'grad_norm': 1.3550224304199219, 'learning_rate': 1.8840097582612552e-05, 'epoch': 1.870259481037924}\n",
            "Captured loss: 2.1649 at step 2811\n",
            "Logs at step 2812: {'loss': 2.6197, 'grad_norm': 1.140644907951355, 'learning_rate': 1.8829008649367933e-05, 'epoch': 1.8709248170326016}\n",
            "Captured loss: 2.6197 at step 2812\n",
            "Logs at step 2813: {'loss': 2.4713, 'grad_norm': 1.243190050125122, 'learning_rate': 1.881791971612331e-05, 'epoch': 1.8715901530272787}\n",
            "Captured loss: 2.4713 at step 2813\n",
            "Logs at step 2814: {'loss': 2.5714, 'grad_norm': 1.8386887311935425, 'learning_rate': 1.880683078287869e-05, 'epoch': 1.872255489021956}\n",
            "Captured loss: 2.5714 at step 2814\n",
            "Logs at step 2815: {'loss': 2.5268, 'grad_norm': 1.137773871421814, 'learning_rate': 1.8795741849634066e-05, 'epoch': 1.8729208250166334}\n",
            "Captured loss: 2.5268 at step 2815\n",
            "Logs at step 2816: {'loss': 2.5152, 'grad_norm': 1.2014148235321045, 'learning_rate': 1.8784652916389444e-05, 'epoch': 1.8735861610113107}\n",
            "Captured loss: 2.5152 at step 2816\n",
            "Logs at step 2817: {'loss': 2.3236, 'grad_norm': 1.6705480813980103, 'learning_rate': 1.8773563983144824e-05, 'epoch': 1.874251497005988}\n",
            "Captured loss: 2.3236 at step 2817\n",
            "Logs at step 2818: {'loss': 2.586, 'grad_norm': 0.9537334442138672, 'learning_rate': 1.87624750499002e-05, 'epoch': 1.8749168330006654}\n",
            "Captured loss: 2.586 at step 2818\n",
            "Logs at step 2819: {'loss': 2.5083, 'grad_norm': 1.1459839344024658, 'learning_rate': 1.875138611665558e-05, 'epoch': 1.8755821689953427}\n",
            "Captured loss: 2.5083 at step 2819\n",
            "Logs at step 2820: {'loss': 2.3824, 'grad_norm': 1.3395200967788696, 'learning_rate': 1.8740297183410957e-05, 'epoch': 1.8762475049900198}\n",
            "Captured loss: 2.3824 at step 2820\n",
            "Logs at step 2821: {'loss': 2.5218, 'grad_norm': 1.5701240301132202, 'learning_rate': 1.8729208250166335e-05, 'epoch': 1.8769128409846974}\n",
            "Captured loss: 2.5218 at step 2821\n",
            "Logs at step 2822: {'loss': 2.3337, 'grad_norm': 1.3511325120925903, 'learning_rate': 1.8718119316921712e-05, 'epoch': 1.8775781769793745}\n",
            "Captured loss: 2.3337 at step 2822\n",
            "Logs at step 2823: {'loss': 2.2052, 'grad_norm': 1.759232997894287, 'learning_rate': 1.870703038367709e-05, 'epoch': 1.878243512974052}\n",
            "Captured loss: 2.2052 at step 2823\n",
            "Logs at step 2824: {'loss': 2.1516, 'grad_norm': 1.293843388557434, 'learning_rate': 1.869594145043247e-05, 'epoch': 1.8789088489687291}\n",
            "Captured loss: 2.1516 at step 2824\n",
            "Logs at step 2825: {'loss': 2.2362, 'grad_norm': 1.2872073650360107, 'learning_rate': 1.8684852517187848e-05, 'epoch': 1.8795741849634067}\n",
            "Captured loss: 2.2362 at step 2825\n",
            "Logs at step 2826: {'loss': 2.4601, 'grad_norm': 1.1965044736862183, 'learning_rate': 1.8673763583943226e-05, 'epoch': 1.8802395209580838}\n",
            "Captured loss: 2.4601 at step 2826\n",
            "Logs at step 2827: {'loss': 2.4882, 'grad_norm': 1.3365029096603394, 'learning_rate': 1.8662674650698603e-05, 'epoch': 1.880904856952761}\n",
            "Captured loss: 2.4882 at step 2827\n",
            "Logs at step 2828: {'loss': 2.3732, 'grad_norm': 2.2690534591674805, 'learning_rate': 1.865158571745398e-05, 'epoch': 1.8815701929474384}\n",
            "Captured loss: 2.3732 at step 2828\n",
            "Logs at step 2829: {'loss': 2.6161, 'grad_norm': 1.1405084133148193, 'learning_rate': 1.8640496784209362e-05, 'epoch': 1.8822355289421158}\n",
            "Captured loss: 2.6161 at step 2829\n",
            "Logs at step 2830: {'loss': 2.631, 'grad_norm': 1.297507405281067, 'learning_rate': 1.862940785096474e-05, 'epoch': 1.882900864936793}\n",
            "Captured loss: 2.631 at step 2830\n",
            "Logs at step 2831: {'loss': 2.2596, 'grad_norm': 1.6355615854263306, 'learning_rate': 1.8618318917720114e-05, 'epoch': 1.8835662009314704}\n",
            "Captured loss: 2.2596 at step 2831\n",
            "Logs at step 2832: {'loss': 2.5268, 'grad_norm': 0.9698264002799988, 'learning_rate': 1.8607229984475494e-05, 'epoch': 1.8842315369261478}\n",
            "Captured loss: 2.5268 at step 2832\n",
            "Logs at step 2833: {'loss': 2.6597, 'grad_norm': 1.0681958198547363, 'learning_rate': 1.8596141051230872e-05, 'epoch': 1.8848968729208249}\n",
            "Captured loss: 2.6597 at step 2833\n",
            "Logs at step 2834: {'loss': 2.2344, 'grad_norm': 1.8595013618469238, 'learning_rate': 1.8585052117986253e-05, 'epoch': 1.8855622089155024}\n",
            "Captured loss: 2.2344 at step 2834\n",
            "Logs at step 2835: {'loss': 2.5156, 'grad_norm': 1.0657355785369873, 'learning_rate': 1.857396318474163e-05, 'epoch': 1.8862275449101795}\n",
            "Captured loss: 2.5156 at step 2835\n",
            "Logs at step 2836: {'loss': 2.2065, 'grad_norm': 2.123012065887451, 'learning_rate': 1.8562874251497005e-05, 'epoch': 1.886892880904857}\n",
            "Captured loss: 2.2065 at step 2836\n",
            "Logs at step 2837: {'loss': 2.2782, 'grad_norm': 1.274837613105774, 'learning_rate': 1.8551785318252386e-05, 'epoch': 1.8875582168995342}\n",
            "Captured loss: 2.2782 at step 2837\n",
            "Logs at step 2838: {'loss': 2.4528, 'grad_norm': 1.4929678440093994, 'learning_rate': 1.8540696385007763e-05, 'epoch': 1.8882235528942117}\n",
            "Captured loss: 2.4528 at step 2838\n",
            "Logs at step 2839: {'loss': 2.565, 'grad_norm': 2.1307413578033447, 'learning_rate': 1.852960745176314e-05, 'epoch': 1.8888888888888888}\n",
            "Captured loss: 2.565 at step 2839\n",
            "Logs at step 2840: {'loss': 2.69, 'grad_norm': 1.659164309501648, 'learning_rate': 1.8518518518518518e-05, 'epoch': 1.8895542248835662}\n",
            "Captured loss: 2.69 at step 2840\n",
            "Logs at step 2841: {'loss': 2.2688, 'grad_norm': 1.649495244026184, 'learning_rate': 1.8507429585273896e-05, 'epoch': 1.8902195608782435}\n",
            "Captured loss: 2.2688 at step 2841\n",
            "Logs at step 2842: {'loss': 2.5456, 'grad_norm': 1.4958479404449463, 'learning_rate': 1.8496340652029277e-05, 'epoch': 1.8908848968729208}\n",
            "Captured loss: 2.5456 at step 2842\n",
            "Logs at step 2843: {'loss': 2.4941, 'grad_norm': 1.5474427938461304, 'learning_rate': 1.8485251718784654e-05, 'epoch': 1.8915502328675982}\n",
            "Captured loss: 2.4941 at step 2843\n",
            "Logs at step 2844: {'loss': 2.4215, 'grad_norm': 1.2939319610595703, 'learning_rate': 1.8474162785540032e-05, 'epoch': 1.8922155688622755}\n",
            "Captured loss: 2.4215 at step 2844\n",
            "Logs at step 2845: {'loss': 2.4949, 'grad_norm': 1.1118179559707642, 'learning_rate': 1.846307385229541e-05, 'epoch': 1.8928809048569528}\n",
            "Captured loss: 2.4949 at step 2845\n",
            "Logs at step 2846: {'loss': 2.1579, 'grad_norm': 1.5182461738586426, 'learning_rate': 1.8451984919050787e-05, 'epoch': 1.89354624085163}\n",
            "Captured loss: 2.1579 at step 2846\n",
            "Logs at step 2847: {'loss': 2.2238, 'grad_norm': 2.456817626953125, 'learning_rate': 1.8440895985806168e-05, 'epoch': 1.8942115768463075}\n",
            "Captured loss: 2.2238 at step 2847\n",
            "Logs at step 2848: {'loss': 2.4402, 'grad_norm': 0.9767812490463257, 'learning_rate': 1.8429807052561545e-05, 'epoch': 1.8948769128409846}\n",
            "Captured loss: 2.4402 at step 2848\n",
            "Logs at step 2849: {'loss': 2.2734, 'grad_norm': 1.8174197673797607, 'learning_rate': 1.8418718119316923e-05, 'epoch': 1.8955422488356621}\n",
            "Captured loss: 2.2734 at step 2849\n",
            "Logs at step 2850: {'loss': 2.5771, 'grad_norm': 2.6598615646362305, 'learning_rate': 1.84076291860723e-05, 'epoch': 1.8962075848303392}\n",
            "Captured loss: 2.5771 at step 2850\n",
            "Logs at step 2851: {'loss': 2.4843, 'grad_norm': 1.298727035522461, 'learning_rate': 1.8396540252827678e-05, 'epoch': 1.8968729208250168}\n",
            "Captured loss: 2.4843 at step 2851\n",
            "Logs at step 2852: {'loss': 2.2036, 'grad_norm': 1.6630752086639404, 'learning_rate': 1.838545131958306e-05, 'epoch': 1.897538256819694}\n",
            "Captured loss: 2.2036 at step 2852\n",
            "Logs at step 2853: {'loss': 2.4995, 'grad_norm': 1.1862735748291016, 'learning_rate': 1.8374362386338433e-05, 'epoch': 1.8982035928143712}\n",
            "Captured loss: 2.4995 at step 2853\n",
            "Logs at step 2854: {'loss': 2.1745, 'grad_norm': 1.2290658950805664, 'learning_rate': 1.8363273453093814e-05, 'epoch': 1.8988689288090486}\n",
            "Captured loss: 2.1745 at step 2854\n",
            "Logs at step 2855: {'loss': 2.499, 'grad_norm': 1.045103907585144, 'learning_rate': 1.8352184519849192e-05, 'epoch': 1.899534264803726}\n",
            "Captured loss: 2.499 at step 2855\n",
            "Logs at step 2856: {'loss': 2.3327, 'grad_norm': 1.3428555727005005, 'learning_rate': 1.834109558660457e-05, 'epoch': 1.9001996007984032}\n",
            "Captured loss: 2.3327 at step 2856\n",
            "Logs at step 2857: {'loss': 2.3358, 'grad_norm': 1.5949511528015137, 'learning_rate': 1.8330006653359947e-05, 'epoch': 1.9008649367930806}\n",
            "Captured loss: 2.3358 at step 2857\n",
            "Logs at step 2858: {'loss': 2.518, 'grad_norm': 1.4637837409973145, 'learning_rate': 1.8318917720115324e-05, 'epoch': 1.9015302727877579}\n",
            "Captured loss: 2.518 at step 2858\n",
            "Logs at step 2859: {'loss': 2.1661, 'grad_norm': 1.307990550994873, 'learning_rate': 1.8307828786870705e-05, 'epoch': 1.902195608782435}\n",
            "Captured loss: 2.1661 at step 2859\n",
            "Logs at step 2860: {'loss': 2.3644, 'grad_norm': 2.14764142036438, 'learning_rate': 1.8296739853626083e-05, 'epoch': 1.9028609447771125}\n",
            "Captured loss: 2.3644 at step 2860\n",
            "Logs at step 2861: {'loss': 2.569, 'grad_norm': 1.1567195653915405, 'learning_rate': 1.828565092038146e-05, 'epoch': 1.9035262807717896}\n",
            "Captured loss: 2.569 at step 2861\n",
            "Logs at step 2862: {'loss': 2.3442, 'grad_norm': 1.2657865285873413, 'learning_rate': 1.8274561987136838e-05, 'epoch': 1.9041916167664672}\n",
            "Captured loss: 2.3442 at step 2862\n",
            "Logs at step 2863: {'loss': 2.583, 'grad_norm': 1.640389084815979, 'learning_rate': 1.8263473053892215e-05, 'epoch': 1.9048569527611443}\n",
            "Captured loss: 2.583 at step 2863\n",
            "Logs at step 2864: {'loss': 2.5457, 'grad_norm': 1.1018414497375488, 'learning_rate': 1.8252384120647596e-05, 'epoch': 1.9055222887558216}\n",
            "Captured loss: 2.5457 at step 2864\n",
            "Logs at step 2865: {'loss': 2.5035, 'grad_norm': 1.150610089302063, 'learning_rate': 1.8241295187402974e-05, 'epoch': 1.906187624750499}\n",
            "Captured loss: 2.5035 at step 2865\n",
            "Logs at step 2866: {'loss': 2.5065, 'grad_norm': 1.4625792503356934, 'learning_rate': 1.8230206254158348e-05, 'epoch': 1.9068529607451763}\n",
            "Captured loss: 2.5065 at step 2866\n",
            "Logs at step 2867: {'loss': 2.3084, 'grad_norm': 1.5789294242858887, 'learning_rate': 1.821911732091373e-05, 'epoch': 1.9075182967398536}\n",
            "Captured loss: 2.3084 at step 2867\n",
            "Logs at step 2868: {'loss': 2.4609, 'grad_norm': 1.122118592262268, 'learning_rate': 1.8208028387669107e-05, 'epoch': 1.908183632734531}\n",
            "Captured loss: 2.4609 at step 2868\n",
            "Logs at step 2869: {'loss': 2.4849, 'grad_norm': 1.09996497631073, 'learning_rate': 1.8196939454424488e-05, 'epoch': 1.9088489687292083}\n",
            "Captured loss: 2.4849 at step 2869\n",
            "Logs at step 2870: {'loss': 2.6783, 'grad_norm': 2.1446022987365723, 'learning_rate': 1.8185850521179865e-05, 'epoch': 1.9095143047238856}\n",
            "Captured loss: 2.6783 at step 2870\n",
            "Logs at step 2871: {'loss': 2.6057, 'grad_norm': 1.0610047578811646, 'learning_rate': 1.817476158793524e-05, 'epoch': 1.910179640718563}\n",
            "Captured loss: 2.6057 at step 2871\n",
            "Logs at step 2872: {'loss': 2.1865, 'grad_norm': 1.2875336408615112, 'learning_rate': 1.816367265469062e-05, 'epoch': 1.91084497671324}\n",
            "Captured loss: 2.1865 at step 2872\n",
            "Logs at step 2873: {'loss': 2.28, 'grad_norm': 2.8039588928222656, 'learning_rate': 1.8152583721445998e-05, 'epoch': 1.9115103127079176}\n",
            "Captured loss: 2.28 at step 2873\n",
            "Logs at step 2874: {'loss': 2.6035, 'grad_norm': 1.1152207851409912, 'learning_rate': 1.8141494788201375e-05, 'epoch': 1.9121756487025947}\n",
            "Captured loss: 2.6035 at step 2874\n",
            "Logs at step 2875: {'loss': 2.4371, 'grad_norm': 1.8047736883163452, 'learning_rate': 1.8130405854956753e-05, 'epoch': 1.9128409846972723}\n",
            "Captured loss: 2.4371 at step 2875\n",
            "Logs at step 2876: {'loss': 2.5975, 'grad_norm': 1.369032859802246, 'learning_rate': 1.811931692171213e-05, 'epoch': 1.9135063206919494}\n",
            "Captured loss: 2.5975 at step 2876\n",
            "Logs at step 2877: {'loss': 2.5208, 'grad_norm': 1.0335150957107544, 'learning_rate': 1.810822798846751e-05, 'epoch': 1.9141716566866267}\n",
            "Captured loss: 2.5208 at step 2877\n",
            "Logs at step 2878: {'loss': 2.5618, 'grad_norm': 1.2789230346679688, 'learning_rate': 1.809713905522289e-05, 'epoch': 1.914836992681304}\n",
            "Captured loss: 2.5618 at step 2878\n",
            "Logs at step 2879: {'loss': 2.5166, 'grad_norm': 1.2085826396942139, 'learning_rate': 1.8086050121978266e-05, 'epoch': 1.9155023286759814}\n",
            "Captured loss: 2.5166 at step 2879\n",
            "Logs at step 2880: {'loss': 2.6684, 'grad_norm': 1.184148907661438, 'learning_rate': 1.8074961188733644e-05, 'epoch': 1.9161676646706587}\n",
            "Captured loss: 2.6684 at step 2880\n",
            "Logs at step 2881: {'loss': 2.3844, 'grad_norm': 1.121120572090149, 'learning_rate': 1.806387225548902e-05, 'epoch': 1.916833000665336}\n",
            "Captured loss: 2.3844 at step 2881\n",
            "Logs at step 2882: {'loss': 2.3452, 'grad_norm': 1.6444278955459595, 'learning_rate': 1.8052783322244403e-05, 'epoch': 1.9174983366600133}\n",
            "Captured loss: 2.3452 at step 2882\n",
            "Logs at step 2883: {'loss': 2.6269, 'grad_norm': 1.2430614233016968, 'learning_rate': 1.804169438899978e-05, 'epoch': 1.9181636726546905}\n",
            "Captured loss: 2.6269 at step 2883\n",
            "Logs at step 2884: {'loss': 2.4363, 'grad_norm': 1.8858827352523804, 'learning_rate': 1.8030605455755158e-05, 'epoch': 1.918829008649368}\n",
            "Captured loss: 2.4363 at step 2884\n",
            "Logs at step 2885: {'loss': 2.1584, 'grad_norm': 1.701599359512329, 'learning_rate': 1.8019516522510535e-05, 'epoch': 1.9194943446440451}\n",
            "Captured loss: 2.1584 at step 2885\n",
            "Logs at step 2886: {'loss': 2.5135, 'grad_norm': 1.4206198453903198, 'learning_rate': 1.8008427589265913e-05, 'epoch': 1.9201596806387227}\n",
            "Captured loss: 2.5135 at step 2886\n",
            "Logs at step 2887: {'loss': 2.8491, 'grad_norm': 1.7741774320602417, 'learning_rate': 1.7997338656021294e-05, 'epoch': 1.9208250166333998}\n",
            "Captured loss: 2.8491 at step 2887\n",
            "Logs at step 2888: {'loss': 2.5394, 'grad_norm': 1.1351251602172852, 'learning_rate': 1.7986249722776668e-05, 'epoch': 1.9214903526280773}\n",
            "Captured loss: 2.5394 at step 2888\n",
            "Logs at step 2889: {'loss': 2.435, 'grad_norm': 1.400746464729309, 'learning_rate': 1.797516078953205e-05, 'epoch': 1.9221556886227544}\n",
            "Captured loss: 2.435 at step 2889\n",
            "Logs at step 2890: {'loss': 2.5557, 'grad_norm': 1.122170329093933, 'learning_rate': 1.7964071856287426e-05, 'epoch': 1.9228210246174318}\n",
            "Captured loss: 2.5557 at step 2890\n",
            "Logs at step 2891: {'loss': 2.5604, 'grad_norm': 1.407654047012329, 'learning_rate': 1.7952982923042804e-05, 'epoch': 1.923486360612109}\n",
            "Captured loss: 2.5604 at step 2891\n",
            "Logs at step 2892: {'loss': 2.5058, 'grad_norm': 1.2408511638641357, 'learning_rate': 1.7941893989798185e-05, 'epoch': 1.9241516966067864}\n",
            "Captured loss: 2.5058 at step 2892\n",
            "Logs at step 2893: {'loss': 2.2269, 'grad_norm': 1.2194974422454834, 'learning_rate': 1.793080505655356e-05, 'epoch': 1.9248170326014638}\n",
            "Captured loss: 2.2269 at step 2893\n",
            "Logs at step 2894: {'loss': 2.3792, 'grad_norm': 1.1619120836257935, 'learning_rate': 1.791971612330894e-05, 'epoch': 1.925482368596141}\n",
            "Captured loss: 2.3792 at step 2894\n",
            "Logs at step 2895: {'loss': 2.1589, 'grad_norm': 1.1582359075546265, 'learning_rate': 1.7908627190064317e-05, 'epoch': 1.9261477045908184}\n",
            "Captured loss: 2.1589 at step 2895\n",
            "Logs at step 2896: {'loss': 2.618, 'grad_norm': 1.1571438312530518, 'learning_rate': 1.7897538256819695e-05, 'epoch': 1.9268130405854955}\n",
            "Captured loss: 2.618 at step 2896\n",
            "Logs at step 2897: {'loss': 2.4415, 'grad_norm': 1.9868944883346558, 'learning_rate': 1.7886449323575073e-05, 'epoch': 1.927478376580173}\n",
            "Captured loss: 2.4415 at step 2897\n",
            "Logs at step 2898: {'loss': 2.5071, 'grad_norm': 1.2326120138168335, 'learning_rate': 1.787536039033045e-05, 'epoch': 1.9281437125748502}\n",
            "Captured loss: 2.5071 at step 2898\n",
            "Logs at step 2899: {'loss': 2.5279, 'grad_norm': 2.28556752204895, 'learning_rate': 1.786427145708583e-05, 'epoch': 1.9288090485695277}\n",
            "Captured loss: 2.5279 at step 2899\n",
            "Logs at step 2900: {'loss': 2.6736, 'grad_norm': 1.4853157997131348, 'learning_rate': 1.785318252384121e-05, 'epoch': 1.9294743845642048}\n",
            "Captured loss: 2.6736 at step 2900\n",
            "Logs at step 2901: {'loss': 2.3136, 'grad_norm': 1.8477778434753418, 'learning_rate': 1.7842093590596583e-05, 'epoch': 1.9301397205588824}\n",
            "Captured loss: 2.3136 at step 2901\n",
            "Logs at step 2902: {'loss': 2.1406, 'grad_norm': 2.067303419113159, 'learning_rate': 1.7831004657351964e-05, 'epoch': 1.9308050565535595}\n",
            "Captured loss: 2.1406 at step 2902\n",
            "Logs at step 2903: {'loss': 2.344, 'grad_norm': 1.267214298248291, 'learning_rate': 1.781991572410734e-05, 'epoch': 1.9314703925482368}\n",
            "Captured loss: 2.344 at step 2903\n",
            "Logs at step 2904: {'loss': 2.107, 'grad_norm': 1.2523237466812134, 'learning_rate': 1.780882679086272e-05, 'epoch': 1.9321357285429142}\n",
            "Captured loss: 2.107 at step 2904\n",
            "Logs at step 2905: {'loss': 2.699, 'grad_norm': 1.5729713439941406, 'learning_rate': 1.77977378576181e-05, 'epoch': 1.9328010645375915}\n",
            "Captured loss: 2.699 at step 2905\n",
            "Logs at step 2906: {'loss': 2.589, 'grad_norm': 1.369441270828247, 'learning_rate': 1.7786648924373474e-05, 'epoch': 1.9334664005322688}\n",
            "Captured loss: 2.589 at step 2906\n",
            "Logs at step 2907: {'loss': 2.3382, 'grad_norm': 1.4643371105194092, 'learning_rate': 1.7775559991128855e-05, 'epoch': 1.9341317365269461}\n",
            "Captured loss: 2.3382 at step 2907\n",
            "Logs at step 2908: {'loss': 2.2069, 'grad_norm': 1.348310947418213, 'learning_rate': 1.7764471057884232e-05, 'epoch': 1.9347970725216235}\n",
            "Captured loss: 2.2069 at step 2908\n",
            "Logs at step 2909: {'loss': 2.2106, 'grad_norm': 1.5121310949325562, 'learning_rate': 1.775338212463961e-05, 'epoch': 1.9354624085163006}\n",
            "Captured loss: 2.2106 at step 2909\n",
            "Logs at step 2910: {'loss': 2.6096, 'grad_norm': 1.276552438735962, 'learning_rate': 1.7742293191394987e-05, 'epoch': 1.9361277445109781}\n",
            "Captured loss: 2.6096 at step 2910\n",
            "Logs at step 2911: {'loss': 2.203, 'grad_norm': 1.8097548484802246, 'learning_rate': 1.7731204258150365e-05, 'epoch': 1.9367930805056552}\n",
            "Captured loss: 2.203 at step 2911\n",
            "Logs at step 2912: {'loss': 2.3541, 'grad_norm': 1.4776625633239746, 'learning_rate': 1.7720115324905746e-05, 'epoch': 1.9374584165003328}\n",
            "Captured loss: 2.3541 at step 2912\n",
            "Logs at step 2913: {'loss': 2.2269, 'grad_norm': 1.5144774913787842, 'learning_rate': 1.7709026391661124e-05, 'epoch': 1.93812375249501}\n",
            "Captured loss: 2.2269 at step 2913\n",
            "Logs at step 2914: {'loss': 2.2187, 'grad_norm': 1.6322520971298218, 'learning_rate': 1.76979374584165e-05, 'epoch': 1.9387890884896875}\n",
            "Captured loss: 2.2187 at step 2914\n",
            "Logs at step 2915: {'loss': 2.4711, 'grad_norm': 1.49123215675354, 'learning_rate': 1.768684852517188e-05, 'epoch': 1.9394544244843646}\n",
            "Captured loss: 2.4711 at step 2915\n",
            "Logs at step 2916: {'loss': 2.442, 'grad_norm': 1.5464919805526733, 'learning_rate': 1.7675759591927256e-05, 'epoch': 1.9401197604790419}\n",
            "Captured loss: 2.442 at step 2916\n",
            "Logs at step 2917: {'loss': 2.2328, 'grad_norm': 1.3271605968475342, 'learning_rate': 1.7664670658682637e-05, 'epoch': 1.9407850964737192}\n",
            "Captured loss: 2.2328 at step 2917\n",
            "Logs at step 2918: {'loss': 2.5782, 'grad_norm': 1.6736682653427124, 'learning_rate': 1.7653581725438015e-05, 'epoch': 1.9414504324683965}\n",
            "Captured loss: 2.5782 at step 2918\n",
            "Logs at step 2919: {'loss': 2.4856, 'grad_norm': 1.1308622360229492, 'learning_rate': 1.7642492792193392e-05, 'epoch': 1.9421157684630739}\n",
            "Captured loss: 2.4856 at step 2919\n",
            "Logs at step 2920: {'loss': 2.4458, 'grad_norm': 1.0188167095184326, 'learning_rate': 1.763140385894877e-05, 'epoch': 1.9427811044577512}\n",
            "Captured loss: 2.4458 at step 2920\n",
            "Logs at step 2921: {'loss': 2.6523, 'grad_norm': 5.417952060699463, 'learning_rate': 1.7620314925704147e-05, 'epoch': 1.9434464404524285}\n",
            "Captured loss: 2.6523 at step 2921\n",
            "Logs at step 2922: {'loss': 2.1727, 'grad_norm': 1.3887033462524414, 'learning_rate': 1.7609225992459528e-05, 'epoch': 1.9441117764471056}\n",
            "Captured loss: 2.1727 at step 2922\n",
            "Logs at step 2923: {'loss': 2.4743, 'grad_norm': 0.9988183975219727, 'learning_rate': 1.7598137059214902e-05, 'epoch': 1.9447771124417832}\n",
            "Captured loss: 2.4743 at step 2923\n",
            "Logs at step 2924: {'loss': 2.5558, 'grad_norm': 1.6957626342773438, 'learning_rate': 1.7587048125970283e-05, 'epoch': 1.9454424484364603}\n",
            "Captured loss: 2.5558 at step 2924\n",
            "Logs at step 2925: {'loss': 2.2799, 'grad_norm': 1.181911826133728, 'learning_rate': 1.757595919272566e-05, 'epoch': 1.9461077844311379}\n",
            "Captured loss: 2.2799 at step 2925\n",
            "Logs at step 2926: {'loss': 2.2371, 'grad_norm': 1.3982622623443604, 'learning_rate': 1.756487025948104e-05, 'epoch': 1.946773120425815}\n",
            "Captured loss: 2.2371 at step 2926\n",
            "Logs at step 2927: {'loss': 2.1713, 'grad_norm': 1.4687364101409912, 'learning_rate': 1.755378132623642e-05, 'epoch': 1.9474384564204925}\n",
            "Captured loss: 2.1713 at step 2927\n",
            "Logs at step 2928: {'loss': 2.6199, 'grad_norm': 2.197890043258667, 'learning_rate': 1.7542692392991794e-05, 'epoch': 1.9481037924151696}\n",
            "Captured loss: 2.6199 at step 2928\n",
            "Logs at step 2929: {'loss': 2.5381, 'grad_norm': 1.2648555040359497, 'learning_rate': 1.7531603459747174e-05, 'epoch': 1.948769128409847}\n",
            "Captured loss: 2.5381 at step 2929\n",
            "Logs at step 2930: {'loss': 2.6347, 'grad_norm': 1.4341984987258911, 'learning_rate': 1.7520514526502552e-05, 'epoch': 1.9494344644045243}\n",
            "Captured loss: 2.6347 at step 2930\n",
            "Logs at step 2931: {'loss': 2.3546, 'grad_norm': 2.029097318649292, 'learning_rate': 1.750942559325793e-05, 'epoch': 1.9500998003992016}\n",
            "Captured loss: 2.3546 at step 2931\n",
            "Logs at step 2932: {'loss': 2.4854, 'grad_norm': 0.9369439482688904, 'learning_rate': 1.7498336660013307e-05, 'epoch': 1.950765136393879}\n",
            "Captured loss: 2.4854 at step 2932\n",
            "Logs at step 2933: {'loss': 2.4345, 'grad_norm': 1.0411664247512817, 'learning_rate': 1.7487247726768685e-05, 'epoch': 1.9514304723885563}\n",
            "Captured loss: 2.4345 at step 2933\n",
            "Logs at step 2934: {'loss': 2.5048, 'grad_norm': 1.2309025526046753, 'learning_rate': 1.7476158793524066e-05, 'epoch': 1.9520958083832336}\n",
            "Captured loss: 2.5048 at step 2934\n",
            "Logs at step 2935: {'loss': 2.0209, 'grad_norm': 1.4704499244689941, 'learning_rate': 1.7465069860279443e-05, 'epoch': 1.9527611443779107}\n",
            "Captured loss: 2.0209 at step 2935\n",
            "Logs at step 2936: {'loss': 2.4891, 'grad_norm': 1.9274787902832031, 'learning_rate': 1.745398092703482e-05, 'epoch': 1.9534264803725883}\n",
            "Captured loss: 2.4891 at step 2936\n",
            "Logs at step 2937: {'loss': 2.4931, 'grad_norm': 1.444126009941101, 'learning_rate': 1.7442891993790198e-05, 'epoch': 1.9540918163672654}\n",
            "Captured loss: 2.4931 at step 2937\n",
            "Logs at step 2938: {'loss': 2.4919, 'grad_norm': 1.2660143375396729, 'learning_rate': 1.7431803060545576e-05, 'epoch': 1.954757152361943}\n",
            "Captured loss: 2.4919 at step 2938\n",
            "Logs at step 2939: {'loss': 2.3429, 'grad_norm': 1.474955439567566, 'learning_rate': 1.7420714127300953e-05, 'epoch': 1.95542248835662}\n",
            "Captured loss: 2.3429 at step 2939\n",
            "Logs at step 2940: {'loss': 2.6216, 'grad_norm': 1.2818700075149536, 'learning_rate': 1.7409625194056334e-05, 'epoch': 1.9560878243512974}\n",
            "Captured loss: 2.6216 at step 2940\n",
            "Logs at step 2941: {'loss': 2.4488, 'grad_norm': 1.39106023311615, 'learning_rate': 1.739853626081171e-05, 'epoch': 1.9567531603459747}\n",
            "Captured loss: 2.4488 at step 2941\n",
            "Logs at step 2942: {'loss': 2.6104, 'grad_norm': 1.4208835363388062, 'learning_rate': 1.738744732756709e-05, 'epoch': 1.957418496340652}\n",
            "Captured loss: 2.6104 at step 2942\n",
            "Logs at step 2943: {'loss': 2.3774, 'grad_norm': 1.8665248155593872, 'learning_rate': 1.7376358394322467e-05, 'epoch': 1.9580838323353293}\n",
            "Captured loss: 2.3774 at step 2943\n",
            "Logs at step 2944: {'loss': 2.4717, 'grad_norm': 1.0020240545272827, 'learning_rate': 1.7365269461077845e-05, 'epoch': 1.9587491683300067}\n",
            "Captured loss: 2.4717 at step 2944\n",
            "Logs at step 2945: {'loss': 2.5581, 'grad_norm': 1.5051078796386719, 'learning_rate': 1.7354180527833222e-05, 'epoch': 1.959414504324684}\n",
            "Captured loss: 2.5581 at step 2945\n",
            "Logs at step 2946: {'loss': 2.3575, 'grad_norm': 1.2314146757125854, 'learning_rate': 1.73430915945886e-05, 'epoch': 1.9600798403193613}\n",
            "Captured loss: 2.3575 at step 2946\n",
            "Logs at step 2947: {'loss': 2.3839, 'grad_norm': 1.456527829170227, 'learning_rate': 1.733200266134398e-05, 'epoch': 1.9607451763140387}\n",
            "Captured loss: 2.3839 at step 2947\n",
            "Logs at step 2948: {'loss': 1.9329, 'grad_norm': 1.8254141807556152, 'learning_rate': 1.7320913728099358e-05, 'epoch': 1.9614105123087158}\n",
            "Captured loss: 1.9329 at step 2948\n",
            "Logs at step 2949: {'loss': 2.6182, 'grad_norm': 1.4015756845474243, 'learning_rate': 1.7309824794854736e-05, 'epoch': 1.9620758483033933}\n",
            "Captured loss: 2.6182 at step 2949\n",
            "Logs at step 2950: {'loss': 2.3511, 'grad_norm': 1.466532826423645, 'learning_rate': 1.7298735861610113e-05, 'epoch': 1.9627411842980704}\n",
            "Captured loss: 2.3511 at step 2950\n",
            "Logs at step 2951: {'loss': 2.8911, 'grad_norm': 1.6448585987091064, 'learning_rate': 1.728764692836549e-05, 'epoch': 1.963406520292748}\n",
            "Captured loss: 2.8911 at step 2951\n",
            "Logs at step 2952: {'loss': 2.3923, 'grad_norm': 1.5604716539382935, 'learning_rate': 1.7276557995120872e-05, 'epoch': 1.964071856287425}\n",
            "Captured loss: 2.3923 at step 2952\n",
            "Logs at step 2953: {'loss': 2.4088, 'grad_norm': 1.7994226217269897, 'learning_rate': 1.726546906187625e-05, 'epoch': 1.9647371922821024}\n",
            "Captured loss: 2.4088 at step 2953\n",
            "Logs at step 2954: {'loss': 2.3232, 'grad_norm': 1.4897270202636719, 'learning_rate': 1.7254380128631627e-05, 'epoch': 1.9654025282767797}\n",
            "Captured loss: 2.3232 at step 2954\n",
            "Logs at step 2955: {'loss': 2.4604, 'grad_norm': 1.4093961715698242, 'learning_rate': 1.7243291195387004e-05, 'epoch': 1.966067864271457}\n",
            "Captured loss: 2.4604 at step 2955\n",
            "Logs at step 2956: {'loss': 2.5127, 'grad_norm': 1.4277071952819824, 'learning_rate': 1.7232202262142382e-05, 'epoch': 1.9667332002661344}\n",
            "Captured loss: 2.5127 at step 2956\n",
            "Logs at step 2957: {'loss': 2.3042, 'grad_norm': 1.6625279188156128, 'learning_rate': 1.7221113328897763e-05, 'epoch': 1.9673985362608117}\n",
            "Captured loss: 2.3042 at step 2957\n",
            "Logs at step 2958: {'loss': 2.548, 'grad_norm': 1.2685011625289917, 'learning_rate': 1.7210024395653137e-05, 'epoch': 1.968063872255489}\n",
            "Captured loss: 2.548 at step 2958\n",
            "Logs at step 2959: {'loss': 2.4019, 'grad_norm': 1.5178179740905762, 'learning_rate': 1.7198935462408518e-05, 'epoch': 1.9687292082501662}\n",
            "Captured loss: 2.4019 at step 2959\n",
            "Logs at step 2960: {'loss': 1.8889, 'grad_norm': 2.132889986038208, 'learning_rate': 1.7187846529163895e-05, 'epoch': 1.9693945442448437}\n",
            "Captured loss: 1.8889 at step 2960\n",
            "Logs at step 2961: {'loss': 2.4963, 'grad_norm': 1.1568281650543213, 'learning_rate': 1.7176757595919273e-05, 'epoch': 1.9700598802395208}\n",
            "Captured loss: 2.4963 at step 2961\n",
            "Logs at step 2962: {'loss': 2.6028, 'grad_norm': 1.261383056640625, 'learning_rate': 1.7165668662674654e-05, 'epoch': 1.9707252162341984}\n",
            "Captured loss: 2.6028 at step 2962\n",
            "Logs at step 2963: {'loss': 2.1726, 'grad_norm': 1.455739974975586, 'learning_rate': 1.7154579729430028e-05, 'epoch': 1.9713905522288755}\n",
            "Captured loss: 2.1726 at step 2963\n",
            "Logs at step 2964: {'loss': 2.525, 'grad_norm': 1.3609671592712402, 'learning_rate': 1.714349079618541e-05, 'epoch': 1.972055888223553}\n",
            "Captured loss: 2.525 at step 2964\n",
            "Logs at step 2965: {'loss': 2.4962, 'grad_norm': 1.1021263599395752, 'learning_rate': 1.7132401862940787e-05, 'epoch': 1.9727212242182302}\n",
            "Captured loss: 2.4962 at step 2965\n",
            "Logs at step 2966: {'loss': 2.4552, 'grad_norm': 1.8240984678268433, 'learning_rate': 1.7121312929696164e-05, 'epoch': 1.9733865602129075}\n",
            "Captured loss: 2.4552 at step 2966\n",
            "Logs at step 2967: {'loss': 2.2472, 'grad_norm': 1.4453563690185547, 'learning_rate': 1.7110223996451542e-05, 'epoch': 1.9740518962075848}\n",
            "Captured loss: 2.2472 at step 2967\n",
            "Logs at step 2968: {'loss': 2.2932, 'grad_norm': 1.861228108406067, 'learning_rate': 1.709913506320692e-05, 'epoch': 1.9747172322022621}\n",
            "Captured loss: 2.2932 at step 2968\n",
            "Logs at step 2969: {'loss': 2.2873, 'grad_norm': 1.284803867340088, 'learning_rate': 1.7088046129962297e-05, 'epoch': 1.9753825681969395}\n",
            "Captured loss: 2.2873 at step 2969\n",
            "Logs at step 2970: {'loss': 2.0368, 'grad_norm': 1.4956365823745728, 'learning_rate': 1.7076957196717678e-05, 'epoch': 1.9760479041916168}\n",
            "Captured loss: 2.0368 at step 2970\n",
            "Logs at step 2971: {'loss': 2.2443, 'grad_norm': 1.5505481958389282, 'learning_rate': 1.7065868263473055e-05, 'epoch': 1.9767132401862941}\n",
            "Captured loss: 2.2443 at step 2971\n",
            "Logs at step 2972: {'loss': 2.5737, 'grad_norm': 2.398334264755249, 'learning_rate': 1.7054779330228433e-05, 'epoch': 1.9773785761809712}\n",
            "Captured loss: 2.5737 at step 2972\n",
            "Logs at step 2973: {'loss': 2.3029, 'grad_norm': 1.1589657068252563, 'learning_rate': 1.704369039698381e-05, 'epoch': 1.9780439121756488}\n",
            "Captured loss: 2.3029 at step 2973\n",
            "Logs at step 2974: {'loss': 2.6233, 'grad_norm': 1.4024462699890137, 'learning_rate': 1.7032601463739188e-05, 'epoch': 1.978709248170326}\n",
            "Captured loss: 2.6233 at step 2974\n",
            "Logs at step 2975: {'loss': 2.4345, 'grad_norm': 1.3702174425125122, 'learning_rate': 1.702151253049457e-05, 'epoch': 1.9793745841650034}\n",
            "Captured loss: 2.4345 at step 2975\n",
            "Logs at step 2976: {'loss': 2.3424, 'grad_norm': 1.3287339210510254, 'learning_rate': 1.7010423597249943e-05, 'epoch': 1.9800399201596806}\n",
            "Captured loss: 2.3424 at step 2976\n",
            "Logs at step 2977: {'loss': 2.7258, 'grad_norm': 2.977687120437622, 'learning_rate': 1.6999334664005324e-05, 'epoch': 1.980705256154358}\n",
            "Captured loss: 2.7258 at step 2977\n",
            "Logs at step 2978: {'loss': 2.1299, 'grad_norm': 1.5627416372299194, 'learning_rate': 1.69882457307607e-05, 'epoch': 1.9813705921490352}\n",
            "Captured loss: 2.1299 at step 2978\n",
            "Logs at step 2979: {'loss': 2.2212, 'grad_norm': 1.9590697288513184, 'learning_rate': 1.697715679751608e-05, 'epoch': 1.9820359281437125}\n",
            "Captured loss: 2.2212 at step 2979\n",
            "Logs at step 2980: {'loss': 2.5242, 'grad_norm': 1.5161365270614624, 'learning_rate': 1.6966067864271457e-05, 'epoch': 1.9827012641383899}\n",
            "Captured loss: 2.5242 at step 2980\n",
            "Logs at step 2981: {'loss': 2.6342, 'grad_norm': 2.0638673305511475, 'learning_rate': 1.6954978931026834e-05, 'epoch': 1.9833666001330672}\n",
            "Captured loss: 2.6342 at step 2981\n",
            "Logs at step 2982: {'loss': 2.2834, 'grad_norm': 2.0177948474884033, 'learning_rate': 1.6943889997782215e-05, 'epoch': 1.9840319361277445}\n",
            "Captured loss: 2.2834 at step 2982\n",
            "Logs at step 2983: {'loss': 2.5051, 'grad_norm': 0.9526177048683167, 'learning_rate': 1.6932801064537593e-05, 'epoch': 1.9846972721224219}\n",
            "Captured loss: 2.5051 at step 2983\n",
            "Logs at step 2984: {'loss': 2.559, 'grad_norm': 1.176538109779358, 'learning_rate': 1.692171213129297e-05, 'epoch': 1.9853626081170992}\n",
            "Captured loss: 2.559 at step 2984\n",
            "Logs at step 2985: {'loss': 2.4473, 'grad_norm': 1.156661868095398, 'learning_rate': 1.6910623198048348e-05, 'epoch': 1.9860279441117763}\n",
            "Captured loss: 2.4473 at step 2985\n",
            "Logs at step 2986: {'loss': 2.5297, 'grad_norm': 1.371934175491333, 'learning_rate': 1.6899534264803725e-05, 'epoch': 1.9866932801064539}\n",
            "Captured loss: 2.5297 at step 2986\n",
            "Logs at step 2987: {'loss': 2.565, 'grad_norm': 1.099063754081726, 'learning_rate': 1.6888445331559106e-05, 'epoch': 1.987358616101131}\n",
            "Captured loss: 2.565 at step 2987\n",
            "Logs at step 2988: {'loss': 2.0918, 'grad_norm': 1.3996492624282837, 'learning_rate': 1.6877356398314484e-05, 'epoch': 1.9880239520958085}\n",
            "Captured loss: 2.0918 at step 2988\n",
            "Logs at step 2989: {'loss': 2.469, 'grad_norm': 1.4544490575790405, 'learning_rate': 1.686626746506986e-05, 'epoch': 1.9886892880904856}\n",
            "Captured loss: 2.469 at step 2989\n",
            "Logs at step 2990: {'loss': 2.3244, 'grad_norm': 1.4716112613677979, 'learning_rate': 1.685517853182524e-05, 'epoch': 1.9893546240851632}\n",
            "Captured loss: 2.3244 at step 2990\n",
            "Logs at step 2991: {'loss': 2.5604, 'grad_norm': 2.896672487258911, 'learning_rate': 1.6844089598580616e-05, 'epoch': 1.9900199600798403}\n",
            "Captured loss: 2.5604 at step 2991\n",
            "Logs at step 2992: {'loss': 2.3852, 'grad_norm': 1.5894720554351807, 'learning_rate': 1.6833000665335997e-05, 'epoch': 1.9906852960745176}\n",
            "Captured loss: 2.3852 at step 2992\n",
            "Logs at step 2993: {'loss': 2.5026, 'grad_norm': 1.113567590713501, 'learning_rate': 1.6821911732091375e-05, 'epoch': 1.991350632069195}\n",
            "Captured loss: 2.5026 at step 2993\n",
            "Logs at step 2994: {'loss': 2.4833, 'grad_norm': 1.0401438474655151, 'learning_rate': 1.6810822798846753e-05, 'epoch': 1.9920159680638723}\n",
            "Captured loss: 2.4833 at step 2994\n",
            "Logs at step 2995: {'loss': 2.1339, 'grad_norm': 1.6451849937438965, 'learning_rate': 1.679973386560213e-05, 'epoch': 1.9926813040585496}\n",
            "Captured loss: 2.1339 at step 2995\n",
            "Logs at step 2996: {'loss': 2.4429, 'grad_norm': 1.3356465101242065, 'learning_rate': 1.6788644932357508e-05, 'epoch': 1.993346640053227}\n",
            "Captured loss: 2.4429 at step 2996\n",
            "Logs at step 2997: {'loss': 2.1432, 'grad_norm': 1.5370392799377441, 'learning_rate': 1.677755599911289e-05, 'epoch': 1.9940119760479043}\n",
            "Captured loss: 2.1432 at step 2997\n",
            "Logs at step 2998: {'loss': 2.1563, 'grad_norm': 1.2028379440307617, 'learning_rate': 1.6766467065868263e-05, 'epoch': 1.9946773120425814}\n",
            "Captured loss: 2.1563 at step 2998\n",
            "Logs at step 2999: {'loss': 2.5863, 'grad_norm': 1.1498699188232422, 'learning_rate': 1.675537813262364e-05, 'epoch': 1.995342648037259}\n",
            "Captured loss: 2.5863 at step 2999\n",
            "Logs at step 3000: {'loss': 2.57, 'grad_norm': 1.1520452499389648, 'learning_rate': 1.674428919937902e-05, 'epoch': 1.996007984031936}\n",
            "Captured loss: 2.57 at step 3000\n",
            "Logs at step 3001: {'loss': 2.5989, 'grad_norm': 1.1842118501663208, 'learning_rate': 1.67332002661344e-05, 'epoch': 1.9966733200266136}\n",
            "Captured loss: 2.5989 at step 3001\n",
            "Logs at step 3002: {'loss': 2.1423, 'grad_norm': 1.6431347131729126, 'learning_rate': 1.6722111332889776e-05, 'epoch': 1.9973386560212907}\n",
            "Captured loss: 2.1423 at step 3002\n",
            "Logs at step 3003: {'loss': 2.4654, 'grad_norm': 1.3193713426589966, 'learning_rate': 1.6711022399645154e-05, 'epoch': 1.998003992015968}\n",
            "Captured loss: 2.4654 at step 3003\n",
            "Logs at step 3004: {'loss': 2.4759, 'grad_norm': 1.3226732015609741, 'learning_rate': 1.669993346640053e-05, 'epoch': 1.9986693280106453}\n",
            "Captured loss: 2.4759 at step 3004\n",
            "Logs at step 3005: {'loss': 2.5448, 'grad_norm': 1.061506986618042, 'learning_rate': 1.6688844533155912e-05, 'epoch': 1.9993346640053227}\n",
            "Captured loss: 2.5448 at step 3005\n",
            "Logs at step 3006: {'loss': 2.5606, 'grad_norm': 2.003971815109253, 'learning_rate': 1.667775559991129e-05, 'epoch': 2.0}\n",
            "Captured loss: 2.5606 at step 3006\n",
            "Logs at step 3007: {'loss': 2.4123, 'grad_norm': 1.5848102569580078, 'learning_rate': 1.6666666666666667e-05, 'epoch': 2.000665335994677}\n",
            "Captured loss: 2.4123 at step 3007\n",
            "Logs at step 3008: {'loss': 2.5346, 'grad_norm': 1.426763892173767, 'learning_rate': 1.6655577733422045e-05, 'epoch': 2.0013306719893547}\n",
            "Captured loss: 2.5346 at step 3008\n",
            "Logs at step 3009: {'loss': 2.4704, 'grad_norm': 1.4555991888046265, 'learning_rate': 1.6644488800177423e-05, 'epoch': 2.0019960079840318}\n",
            "Captured loss: 2.4704 at step 3009\n",
            "Logs at step 3010: {'loss': 2.127, 'grad_norm': 1.6256115436553955, 'learning_rate': 1.6633399866932804e-05, 'epoch': 2.0026613439787093}\n",
            "Captured loss: 2.127 at step 3010\n",
            "Logs at step 3011: {'loss': 2.3253, 'grad_norm': 1.4845033884048462, 'learning_rate': 1.6622310933688178e-05, 'epoch': 2.0033266799733864}\n",
            "Captured loss: 2.3253 at step 3011\n",
            "Logs at step 3012: {'loss': 2.519, 'grad_norm': 1.6251211166381836, 'learning_rate': 1.661122200044356e-05, 'epoch': 2.003992015968064}\n",
            "Captured loss: 2.519 at step 3012\n",
            "Logs at step 3013: {'loss': 2.507, 'grad_norm': 1.6004431247711182, 'learning_rate': 1.6600133067198936e-05, 'epoch': 2.004657351962741}\n",
            "Captured loss: 2.507 at step 3013\n",
            "Logs at step 3014: {'loss': 2.0043, 'grad_norm': 1.559654951095581, 'learning_rate': 1.6589044133954314e-05, 'epoch': 2.0053226879574186}\n",
            "Captured loss: 2.0043 at step 3014\n",
            "Logs at step 3015: {'loss': 2.2693, 'grad_norm': 1.3693211078643799, 'learning_rate': 1.6577955200709695e-05, 'epoch': 2.0059880239520957}\n",
            "Captured loss: 2.2693 at step 3015\n",
            "Logs at step 3016: {'loss': 2.5761, 'grad_norm': 1.3843662738800049, 'learning_rate': 1.656686626746507e-05, 'epoch': 2.0066533599467733}\n",
            "Captured loss: 2.5761 at step 3016\n",
            "Logs at step 3017: {'loss': 2.1553, 'grad_norm': 1.6490495204925537, 'learning_rate': 1.655577733422045e-05, 'epoch': 2.0073186959414504}\n",
            "Captured loss: 2.1553 at step 3017\n",
            "Logs at step 3018: {'loss': 2.5692, 'grad_norm': 1.1744014024734497, 'learning_rate': 1.6544688400975827e-05, 'epoch': 2.007984031936128}\n",
            "Captured loss: 2.5692 at step 3018\n",
            "Logs at step 3019: {'loss': 2.5677, 'grad_norm': 1.158637285232544, 'learning_rate': 1.6533599467731205e-05, 'epoch': 2.008649367930805}\n",
            "Captured loss: 2.5677 at step 3019\n",
            "Logs at step 3020: {'loss': 2.6468, 'grad_norm': 1.977530837059021, 'learning_rate': 1.6522510534486582e-05, 'epoch': 2.009314703925482}\n",
            "Captured loss: 2.6468 at step 3020\n",
            "Logs at step 3021: {'loss': 2.5775, 'grad_norm': 1.1872780323028564, 'learning_rate': 1.651142160124196e-05, 'epoch': 2.0099800399201597}\n",
            "Captured loss: 2.5775 at step 3021\n",
            "Logs at step 3022: {'loss': 1.9795, 'grad_norm': 1.976902961730957, 'learning_rate': 1.650033266799734e-05, 'epoch': 2.010645375914837}\n",
            "Captured loss: 1.9795 at step 3022\n",
            "Logs at step 3023: {'loss': 2.4985, 'grad_norm': 1.1798993349075317, 'learning_rate': 1.648924373475272e-05, 'epoch': 2.0113107119095144}\n",
            "Captured loss: 2.4985 at step 3023\n",
            "Logs at step 3024: {'loss': 2.4032, 'grad_norm': 1.0903657674789429, 'learning_rate': 1.6478154801508096e-05, 'epoch': 2.0119760479041915}\n",
            "Captured loss: 2.4032 at step 3024\n",
            "Logs at step 3025: {'loss': 2.2219, 'grad_norm': 1.5574842691421509, 'learning_rate': 1.6467065868263474e-05, 'epoch': 2.012641383898869}\n",
            "Captured loss: 2.2219 at step 3025\n",
            "Logs at step 3026: {'loss': 2.118, 'grad_norm': 1.3987382650375366, 'learning_rate': 1.645597693501885e-05, 'epoch': 2.013306719893546}\n",
            "Captured loss: 2.118 at step 3026\n",
            "Logs at step 3027: {'loss': 2.4664, 'grad_norm': 1.52579927444458, 'learning_rate': 1.6444888001774232e-05, 'epoch': 2.0139720558882237}\n",
            "Captured loss: 2.4664 at step 3027\n",
            "Logs at step 3028: {'loss': 2.1664, 'grad_norm': 1.5218812227249146, 'learning_rate': 1.643379906852961e-05, 'epoch': 2.014637391882901}\n",
            "Captured loss: 2.1664 at step 3028\n",
            "Logs at step 3029: {'loss': 2.4933, 'grad_norm': 1.7554633617401123, 'learning_rate': 1.6422710135284987e-05, 'epoch': 2.0153027278775784}\n",
            "Captured loss: 2.4933 at step 3029\n",
            "Logs at step 3030: {'loss': 2.2493, 'grad_norm': 1.3989698886871338, 'learning_rate': 1.6411621202040365e-05, 'epoch': 2.0159680638722555}\n",
            "Captured loss: 2.2493 at step 3030\n",
            "Logs at step 3031: {'loss': 2.5446, 'grad_norm': 1.1209027767181396, 'learning_rate': 1.6400532268795742e-05, 'epoch': 2.016633399866933}\n",
            "Captured loss: 2.5446 at step 3031\n",
            "Logs at step 3032: {'loss': 2.3479, 'grad_norm': 1.2843754291534424, 'learning_rate': 1.6389443335551123e-05, 'epoch': 2.01729873586161}\n",
            "Captured loss: 2.3479 at step 3032\n",
            "Logs at step 3033: {'loss': 2.213, 'grad_norm': 1.2990859746932983, 'learning_rate': 1.6378354402306497e-05, 'epoch': 2.0179640718562872}\n",
            "Captured loss: 2.213 at step 3033\n",
            "Logs at step 3034: {'loss': 2.217, 'grad_norm': 1.3973480463027954, 'learning_rate': 1.6367265469061875e-05, 'epoch': 2.018629407850965}\n",
            "Captured loss: 2.217 at step 3034\n",
            "Logs at step 3035: {'loss': 2.5456, 'grad_norm': 1.1680469512939453, 'learning_rate': 1.6356176535817256e-05, 'epoch': 2.019294743845642}\n",
            "Captured loss: 2.5456 at step 3035\n",
            "Logs at step 3036: {'loss': 2.5574, 'grad_norm': 1.1748909950256348, 'learning_rate': 1.6345087602572633e-05, 'epoch': 2.0199600798403194}\n",
            "Captured loss: 2.5574 at step 3036\n",
            "Logs at step 3037: {'loss': 2.4108, 'grad_norm': 1.786835789680481, 'learning_rate': 1.6333998669328014e-05, 'epoch': 2.0206254158349966}\n",
            "Captured loss: 2.4108 at step 3037\n",
            "Logs at step 3038: {'loss': 2.5327, 'grad_norm': 1.1848078966140747, 'learning_rate': 1.632290973608339e-05, 'epoch': 2.021290751829674}\n",
            "Captured loss: 2.5327 at step 3038\n",
            "Logs at step 3039: {'loss': 2.4061, 'grad_norm': 1.3415510654449463, 'learning_rate': 1.6311820802838766e-05, 'epoch': 2.021956087824351}\n",
            "Captured loss: 2.4061 at step 3039\n",
            "Logs at step 3040: {'loss': 2.5402, 'grad_norm': 1.1756165027618408, 'learning_rate': 1.6300731869594147e-05, 'epoch': 2.0226214238190288}\n",
            "Captured loss: 2.5402 at step 3040\n",
            "Logs at step 3041: {'loss': 2.6247, 'grad_norm': 1.7692946195602417, 'learning_rate': 1.6289642936349525e-05, 'epoch': 2.023286759813706}\n",
            "Captured loss: 2.6247 at step 3041\n",
            "Logs at step 3042: {'loss': 2.3852, 'grad_norm': 1.8982112407684326, 'learning_rate': 1.6278554003104902e-05, 'epoch': 2.0239520958083834}\n",
            "Captured loss: 2.3852 at step 3042\n",
            "Logs at step 3043: {'loss': 1.8657, 'grad_norm': 2.398219108581543, 'learning_rate': 1.626746506986028e-05, 'epoch': 2.0246174318030605}\n",
            "Captured loss: 1.8657 at step 3043\n",
            "Logs at step 3044: {'loss': 2.4369, 'grad_norm': 1.057454228401184, 'learning_rate': 1.6256376136615657e-05, 'epoch': 2.0252827677977376}\n",
            "Captured loss: 2.4369 at step 3044\n",
            "Logs at step 3045: {'loss': 2.2771, 'grad_norm': 1.6132498979568481, 'learning_rate': 1.6245287203371038e-05, 'epoch': 2.025948103792415}\n",
            "Captured loss: 2.2771 at step 3045\n",
            "Logs at step 3046: {'loss': 2.2709, 'grad_norm': 1.6498024463653564, 'learning_rate': 1.6234198270126412e-05, 'epoch': 2.0266134397870923}\n",
            "Captured loss: 2.2709 at step 3046\n",
            "Logs at step 3047: {'loss': 2.1505, 'grad_norm': 1.3331869840621948, 'learning_rate': 1.6223109336881793e-05, 'epoch': 2.02727877578177}\n",
            "Captured loss: 2.1505 at step 3047\n",
            "Logs at step 3048: {'loss': 1.9683, 'grad_norm': 1.7167540788650513, 'learning_rate': 1.621202040363717e-05, 'epoch': 2.027944111776447}\n",
            "Captured loss: 1.9683 at step 3048\n",
            "Logs at step 3049: {'loss': 2.1264, 'grad_norm': 1.8108444213867188, 'learning_rate': 1.6200931470392548e-05, 'epoch': 2.0286094477711245}\n",
            "Captured loss: 2.1264 at step 3049\n",
            "Logs at step 3050: {'loss': 2.5274, 'grad_norm': 1.8399823904037476, 'learning_rate': 1.618984253714793e-05, 'epoch': 2.0292747837658016}\n",
            "Captured loss: 2.5274 at step 3050\n",
            "Logs at step 3051: {'loss': 2.2105, 'grad_norm': 1.4201513528823853, 'learning_rate': 1.6178753603903303e-05, 'epoch': 2.029940119760479}\n",
            "Captured loss: 2.2105 at step 3051\n",
            "Logs at step 3052: {'loss': 2.4914, 'grad_norm': 1.5305665731430054, 'learning_rate': 1.6167664670658684e-05, 'epoch': 2.0306054557551563}\n",
            "Captured loss: 2.4914 at step 3052\n",
            "Logs at step 3053: {'loss': 2.549, 'grad_norm': 1.7196769714355469, 'learning_rate': 1.6156575737414062e-05, 'epoch': 2.031270791749834}\n",
            "Captured loss: 2.549 at step 3053\n",
            "Logs at step 3054: {'loss': 2.3918, 'grad_norm': 1.2956866025924683, 'learning_rate': 1.614548680416944e-05, 'epoch': 2.031936127744511}\n",
            "Captured loss: 2.3918 at step 3054\n",
            "Logs at step 3055: {'loss': 2.4774, 'grad_norm': 1.0634933710098267, 'learning_rate': 1.6134397870924817e-05, 'epoch': 2.0326014637391885}\n",
            "Captured loss: 2.4774 at step 3055\n",
            "Logs at step 3056: {'loss': 2.3908, 'grad_norm': 1.8103095293045044, 'learning_rate': 1.6123308937680195e-05, 'epoch': 2.0332667997338656}\n",
            "Captured loss: 2.3908 at step 3056\n",
            "Logs at step 3057: {'loss': 2.5258, 'grad_norm': 1.0884921550750732, 'learning_rate': 1.6112220004435575e-05, 'epoch': 2.0339321357285427}\n",
            "Captured loss: 2.5258 at step 3057\n",
            "Logs at step 3058: {'loss': 2.672, 'grad_norm': 1.3434733152389526, 'learning_rate': 1.6101131071190953e-05, 'epoch': 2.0345974717232203}\n",
            "Captured loss: 2.672 at step 3058\n",
            "Logs at step 3059: {'loss': 1.9866, 'grad_norm': 1.8277933597564697, 'learning_rate': 1.609004213794633e-05, 'epoch': 2.0352628077178974}\n",
            "Captured loss: 1.9866 at step 3059\n",
            "Logs at step 3060: {'loss': 2.3578, 'grad_norm': 1.4238979816436768, 'learning_rate': 1.6078953204701708e-05, 'epoch': 2.035928143712575}\n",
            "Captured loss: 2.3578 at step 3060\n",
            "Logs at step 3061: {'loss': 2.344, 'grad_norm': 1.4776904582977295, 'learning_rate': 1.6067864271457086e-05, 'epoch': 2.036593479707252}\n",
            "Captured loss: 2.344 at step 3061\n",
            "Logs at step 3062: {'loss': 2.2548, 'grad_norm': 2.116384506225586, 'learning_rate': 1.6056775338212467e-05, 'epoch': 2.0372588157019296}\n",
            "Captured loss: 2.2548 at step 3062\n",
            "Logs at step 3063: {'loss': 2.5317, 'grad_norm': 1.2118929624557495, 'learning_rate': 1.6045686404967844e-05, 'epoch': 2.0379241516966067}\n",
            "Captured loss: 2.5317 at step 3063\n",
            "Logs at step 3064: {'loss': 2.5118, 'grad_norm': 1.7122678756713867, 'learning_rate': 1.603459747172322e-05, 'epoch': 2.0385894876912842}\n",
            "Captured loss: 2.5118 at step 3064\n",
            "Logs at step 3065: {'loss': 2.3193, 'grad_norm': 1.0752278566360474, 'learning_rate': 1.60235085384786e-05, 'epoch': 2.0392548236859613}\n",
            "Captured loss: 2.3193 at step 3065\n",
            "Logs at step 3066: {'loss': 2.4968, 'grad_norm': 1.2188949584960938, 'learning_rate': 1.6012419605233977e-05, 'epoch': 2.039920159680639}\n",
            "Captured loss: 2.4968 at step 3066\n",
            "Logs at step 3067: {'loss': 2.129, 'grad_norm': 1.3622758388519287, 'learning_rate': 1.6001330671989358e-05, 'epoch': 2.040585495675316}\n",
            "Captured loss: 2.129 at step 3067\n",
            "Logs at step 3068: {'loss': 2.5414, 'grad_norm': 1.5938563346862793, 'learning_rate': 1.5990241738744732e-05, 'epoch': 2.0412508316699935}\n",
            "Captured loss: 2.5414 at step 3068\n",
            "Logs at step 3069: {'loss': 1.9576, 'grad_norm': 1.4058973789215088, 'learning_rate': 1.597915280550011e-05, 'epoch': 2.0419161676646707}\n",
            "Captured loss: 1.9576 at step 3069\n",
            "Logs at step 3070: {'loss': 2.3562, 'grad_norm': 1.3069506883621216, 'learning_rate': 1.596806387225549e-05, 'epoch': 2.0425815036593478}\n",
            "Captured loss: 2.3562 at step 3070\n",
            "Logs at step 3071: {'loss': 2.6216, 'grad_norm': 1.0916428565979004, 'learning_rate': 1.5956974939010868e-05, 'epoch': 2.0432468396540253}\n",
            "Captured loss: 2.6216 at step 3071\n",
            "Logs at step 3072: {'loss': 2.5092, 'grad_norm': 1.6634873151779175, 'learning_rate': 1.594588600576625e-05, 'epoch': 2.0439121756487024}\n",
            "Captured loss: 2.5092 at step 3072\n",
            "Logs at step 3073: {'loss': 2.5505, 'grad_norm': 1.1150943040847778, 'learning_rate': 1.5934797072521623e-05, 'epoch': 2.04457751164338}\n",
            "Captured loss: 2.5505 at step 3073\n",
            "Logs at step 3074: {'loss': 2.7275, 'grad_norm': 2.750354528427124, 'learning_rate': 1.5923708139277e-05, 'epoch': 2.045242847638057}\n",
            "Captured loss: 2.7275 at step 3074\n",
            "Logs at step 3075: {'loss': 2.2573, 'grad_norm': 1.5550503730773926, 'learning_rate': 1.591261920603238e-05, 'epoch': 2.0459081836327346}\n",
            "Captured loss: 2.2573 at step 3075\n",
            "Logs at step 3076: {'loss': 2.5604, 'grad_norm': 1.3870707750320435, 'learning_rate': 1.590153027278776e-05, 'epoch': 2.0465735196274117}\n",
            "Captured loss: 2.5604 at step 3076\n",
            "Logs at step 3077: {'loss': 2.0277, 'grad_norm': 1.8002442121505737, 'learning_rate': 1.5890441339543137e-05, 'epoch': 2.0472388556220893}\n",
            "Captured loss: 2.0277 at step 3077\n",
            "Logs at step 3078: {'loss': 2.5588, 'grad_norm': 1.4424333572387695, 'learning_rate': 1.5879352406298514e-05, 'epoch': 2.0479041916167664}\n",
            "Captured loss: 2.5588 at step 3078\n",
            "Logs at step 3079: {'loss': 2.6336, 'grad_norm': 1.6244765520095825, 'learning_rate': 1.5868263473053892e-05, 'epoch': 2.048569527611444}\n",
            "Captured loss: 2.6336 at step 3079\n",
            "Logs at step 3080: {'loss': 2.5737, 'grad_norm': 1.6357921361923218, 'learning_rate': 1.5857174539809273e-05, 'epoch': 2.049234863606121}\n",
            "Captured loss: 2.5737 at step 3080\n",
            "Logs at step 3081: {'loss': 2.6108, 'grad_norm': 2.81852388381958, 'learning_rate': 1.5846085606564647e-05, 'epoch': 2.0499001996007986}\n",
            "Captured loss: 2.6108 at step 3081\n",
            "Logs at step 3082: {'loss': 2.4851, 'grad_norm': 1.6137434244155884, 'learning_rate': 1.5834996673320028e-05, 'epoch': 2.0505655355954757}\n",
            "Captured loss: 2.4851 at step 3082\n",
            "Logs at step 3083: {'loss': 2.2611, 'grad_norm': 2.239525318145752, 'learning_rate': 1.5823907740075405e-05, 'epoch': 2.051230871590153}\n",
            "Captured loss: 2.2611 at step 3083\n",
            "Logs at step 3084: {'loss': 2.3404, 'grad_norm': 1.8082894086837769, 'learning_rate': 1.5812818806830783e-05, 'epoch': 2.0518962075848304}\n",
            "Captured loss: 2.3404 at step 3084\n",
            "Logs at step 3085: {'loss': 2.566, 'grad_norm': 1.1496100425720215, 'learning_rate': 1.5801729873586164e-05, 'epoch': 2.0525615435795075}\n",
            "Captured loss: 2.566 at step 3085\n",
            "Logs at step 3086: {'loss': 2.562, 'grad_norm': 1.1877497434616089, 'learning_rate': 1.5790640940341538e-05, 'epoch': 2.053226879574185}\n",
            "Captured loss: 2.562 at step 3086\n",
            "Logs at step 3087: {'loss': 2.1036, 'grad_norm': 1.4847551584243774, 'learning_rate': 1.577955200709692e-05, 'epoch': 2.053892215568862}\n",
            "Captured loss: 2.1036 at step 3087\n",
            "Logs at step 3088: {'loss': 2.4688, 'grad_norm': 1.1620025634765625, 'learning_rate': 1.5768463073852296e-05, 'epoch': 2.0545575515635397}\n",
            "Captured loss: 2.4688 at step 3088\n",
            "Logs at step 3089: {'loss': 2.4828, 'grad_norm': 1.3229442834854126, 'learning_rate': 1.5757374140607674e-05, 'epoch': 2.055222887558217}\n",
            "Captured loss: 2.4828 at step 3089\n",
            "Logs at step 3090: {'loss': 2.5424, 'grad_norm': 1.3248661756515503, 'learning_rate': 1.574628520736305e-05, 'epoch': 2.0558882235528944}\n",
            "Captured loss: 2.5424 at step 3090\n",
            "Logs at step 3091: {'loss': 2.1715, 'grad_norm': 1.872860074043274, 'learning_rate': 1.573519627411843e-05, 'epoch': 2.0565535595475715}\n",
            "Captured loss: 2.1715 at step 3091\n",
            "Logs at step 3092: {'loss': 2.3778, 'grad_norm': 1.107420563697815, 'learning_rate': 1.572410734087381e-05, 'epoch': 2.057218895542249}\n",
            "Captured loss: 2.3778 at step 3092\n",
            "Logs at step 3093: {'loss': 2.5043, 'grad_norm': 1.7408995628356934, 'learning_rate': 1.5713018407629188e-05, 'epoch': 2.057884231536926}\n",
            "Captured loss: 2.5043 at step 3093\n",
            "Logs at step 3094: {'loss': 2.1253, 'grad_norm': 1.5064268112182617, 'learning_rate': 1.5701929474384565e-05, 'epoch': 2.0585495675316037}\n",
            "Captured loss: 2.1253 at step 3094\n",
            "Logs at step 3095: {'loss': 2.4654, 'grad_norm': 0.9961721301078796, 'learning_rate': 1.5690840541139943e-05, 'epoch': 2.059214903526281}\n",
            "Captured loss: 2.4654 at step 3095\n",
            "Logs at step 3096: {'loss': 2.0989, 'grad_norm': 1.4615261554718018, 'learning_rate': 1.567975160789532e-05, 'epoch': 2.059880239520958}\n",
            "Captured loss: 2.0989 at step 3096\n",
            "Logs at step 3097: {'loss': 2.3678, 'grad_norm': 1.3227753639221191, 'learning_rate': 1.56686626746507e-05, 'epoch': 2.0605455755156354}\n",
            "Captured loss: 2.3678 at step 3097\n",
            "Logs at step 3098: {'loss': 2.5884, 'grad_norm': 1.3951090574264526, 'learning_rate': 1.565757374140608e-05, 'epoch': 2.0612109115103125}\n",
            "Captured loss: 2.5884 at step 3098\n",
            "Logs at step 3099: {'loss': 2.4937, 'grad_norm': 1.015745997428894, 'learning_rate': 1.5646484808161453e-05, 'epoch': 2.06187624750499}\n",
            "Captured loss: 2.4937 at step 3099\n",
            "Logs at step 3100: {'loss': 2.0298, 'grad_norm': 2.163388252258301, 'learning_rate': 1.5635395874916834e-05, 'epoch': 2.062541583499667}\n",
            "Captured loss: 2.0298 at step 3100\n",
            "Logs at step 3101: {'loss': 1.9947, 'grad_norm': 1.3101059198379517, 'learning_rate': 1.562430694167221e-05, 'epoch': 2.0632069194943448}\n",
            "Captured loss: 1.9947 at step 3101\n",
            "Logs at step 3102: {'loss': 2.5488, 'grad_norm': 1.0958247184753418, 'learning_rate': 1.5613218008427592e-05, 'epoch': 2.063872255489022}\n",
            "Captured loss: 2.5488 at step 3102\n",
            "Logs at step 3103: {'loss': 2.217, 'grad_norm': 1.6004095077514648, 'learning_rate': 1.5602129075182967e-05, 'epoch': 2.0645375914836994}\n",
            "Captured loss: 2.217 at step 3103\n",
            "Logs at step 3104: {'loss': 2.3146, 'grad_norm': 2.0712742805480957, 'learning_rate': 1.5591040141938344e-05, 'epoch': 2.0652029274783765}\n",
            "Captured loss: 2.3146 at step 3104\n",
            "Logs at step 3105: {'loss': 2.2155, 'grad_norm': 1.4797909259796143, 'learning_rate': 1.5579951208693725e-05, 'epoch': 2.065868263473054}\n",
            "Captured loss: 2.2155 at step 3105\n",
            "Logs at step 3106: {'loss': 2.5035, 'grad_norm': 1.6800537109375, 'learning_rate': 1.5568862275449103e-05, 'epoch': 2.066533599467731}\n",
            "Captured loss: 2.5035 at step 3106\n",
            "Logs at step 3107: {'loss': 2.5805, 'grad_norm': 1.6565784215927124, 'learning_rate': 1.5557773342204484e-05, 'epoch': 2.0671989354624083}\n",
            "Captured loss: 2.5805 at step 3107\n",
            "Logs at step 3108: {'loss': 2.4436, 'grad_norm': 1.3099229335784912, 'learning_rate': 1.5546684408959858e-05, 'epoch': 2.067864271457086}\n",
            "Captured loss: 2.4436 at step 3108\n",
            "Logs at step 3109: {'loss': 2.3567, 'grad_norm': 1.3467892408370972, 'learning_rate': 1.5535595475715235e-05, 'epoch': 2.068529607451763}\n",
            "Captured loss: 2.3567 at step 3109\n",
            "Logs at step 3110: {'loss': 2.35, 'grad_norm': 1.5960876941680908, 'learning_rate': 1.5524506542470616e-05, 'epoch': 2.0691949434464405}\n",
            "Captured loss: 2.35 at step 3110\n",
            "Logs at step 3111: {'loss': 2.2821, 'grad_norm': 1.0262939929962158, 'learning_rate': 1.5513417609225994e-05, 'epoch': 2.0698602794411176}\n",
            "Captured loss: 2.2821 at step 3111\n",
            "Logs at step 3112: {'loss': 2.8385, 'grad_norm': 2.221348285675049, 'learning_rate': 1.550232867598137e-05, 'epoch': 2.070525615435795}\n",
            "Captured loss: 2.8385 at step 3112\n",
            "Logs at step 3113: {'loss': 2.3346, 'grad_norm': 1.5474594831466675, 'learning_rate': 1.549123974273675e-05, 'epoch': 2.0711909514304723}\n",
            "Captured loss: 2.3346 at step 3113\n",
            "Logs at step 3114: {'loss': 2.175, 'grad_norm': 2.332519769668579, 'learning_rate': 1.5480150809492126e-05, 'epoch': 2.07185628742515}\n",
            "Captured loss: 2.175 at step 3114\n",
            "Logs at step 3115: {'loss': 2.6111, 'grad_norm': 1.5574564933776855, 'learning_rate': 1.5469061876247507e-05, 'epoch': 2.072521623419827}\n",
            "Captured loss: 2.6111 at step 3115\n",
            "Logs at step 3116: {'loss': 2.1173, 'grad_norm': 1.4201709032058716, 'learning_rate': 1.5457972943002885e-05, 'epoch': 2.0731869594145045}\n",
            "Captured loss: 2.1173 at step 3116\n",
            "Logs at step 3117: {'loss': 2.1417, 'grad_norm': 1.547430396080017, 'learning_rate': 1.5446884009758262e-05, 'epoch': 2.0738522954091816}\n",
            "Captured loss: 2.1417 at step 3117\n",
            "Logs at step 3118: {'loss': 2.6872, 'grad_norm': 1.6699848175048828, 'learning_rate': 1.543579507651364e-05, 'epoch': 2.074517631403859}\n",
            "Captured loss: 2.6872 at step 3118\n",
            "Logs at step 3119: {'loss': 1.9584, 'grad_norm': 1.497314214706421, 'learning_rate': 1.5424706143269017e-05, 'epoch': 2.0751829673985362}\n",
            "Captured loss: 1.9584 at step 3119\n",
            "Logs at step 3120: {'loss': 2.4139, 'grad_norm': 1.2623023986816406, 'learning_rate': 1.54136172100244e-05, 'epoch': 2.0758483033932134}\n",
            "Captured loss: 2.4139 at step 3120\n",
            "Logs at step 3121: {'loss': 2.5221, 'grad_norm': 1.2578247785568237, 'learning_rate': 1.5402528276779773e-05, 'epoch': 2.076513639387891}\n",
            "Captured loss: 2.5221 at step 3121\n",
            "Logs at step 3122: {'loss': 2.4358, 'grad_norm': 1.3715150356292725, 'learning_rate': 1.5391439343535154e-05, 'epoch': 2.077178975382568}\n",
            "Captured loss: 2.4358 at step 3122\n",
            "Logs at step 3123: {'loss': 2.2371, 'grad_norm': 1.7128241062164307, 'learning_rate': 1.538035041029053e-05, 'epoch': 2.0778443113772456}\n",
            "Captured loss: 2.2371 at step 3123\n",
            "Logs at step 3124: {'loss': 2.5779, 'grad_norm': 1.400010108947754, 'learning_rate': 1.536926147704591e-05, 'epoch': 2.0785096473719227}\n",
            "Captured loss: 2.5779 at step 3124\n",
            "Logs at step 3125: {'loss': 2.5433, 'grad_norm': 2.1989288330078125, 'learning_rate': 1.5358172543801286e-05, 'epoch': 2.0791749833666002}\n",
            "Captured loss: 2.5433 at step 3125\n",
            "Logs at step 3126: {'loss': 2.2186, 'grad_norm': 1.822657585144043, 'learning_rate': 1.5347083610556664e-05, 'epoch': 2.0798403193612773}\n",
            "Captured loss: 2.2186 at step 3126\n",
            "Logs at step 3127: {'loss': 2.4749, 'grad_norm': 1.7941349744796753, 'learning_rate': 1.5335994677312045e-05, 'epoch': 2.080505655355955}\n",
            "Captured loss: 2.4749 at step 3127\n",
            "Logs at step 3128: {'loss': 2.5711, 'grad_norm': 1.2144701480865479, 'learning_rate': 1.5324905744067422e-05, 'epoch': 2.081170991350632}\n",
            "Captured loss: 2.5711 at step 3128\n",
            "Logs at step 3129: {'loss': 2.6656, 'grad_norm': 1.9761987924575806, 'learning_rate': 1.53138168108228e-05, 'epoch': 2.0818363273453095}\n",
            "Captured loss: 2.6656 at step 3129\n",
            "Logs at step 3130: {'loss': 2.4018, 'grad_norm': 1.6082466840744019, 'learning_rate': 1.5302727877578177e-05, 'epoch': 2.0825016633399867}\n",
            "Captured loss: 2.4018 at step 3130\n",
            "Logs at step 3131: {'loss': 2.1445, 'grad_norm': 1.918337106704712, 'learning_rate': 1.5291638944333555e-05, 'epoch': 2.083166999334664}\n",
            "Captured loss: 2.1445 at step 3131\n",
            "Logs at step 3132: {'loss': 2.4838, 'grad_norm': 1.3083935976028442, 'learning_rate': 1.5280550011088936e-05, 'epoch': 2.0838323353293413}\n",
            "Captured loss: 2.4838 at step 3132\n",
            "Logs at step 3133: {'loss': 2.456, 'grad_norm': 0.9847540855407715, 'learning_rate': 1.5269461077844313e-05, 'epoch': 2.0844976713240184}\n",
            "Captured loss: 2.456 at step 3133\n",
            "Logs at step 3134: {'loss': 2.7019, 'grad_norm': 1.9216303825378418, 'learning_rate': 1.525837214459969e-05, 'epoch': 2.085163007318696}\n",
            "Captured loss: 2.7019 at step 3134\n",
            "Logs at step 3135: {'loss': 2.6258, 'grad_norm': 1.6928514242172241, 'learning_rate': 1.5247283211355068e-05, 'epoch': 2.085828343313373}\n",
            "Captured loss: 2.6258 at step 3135\n",
            "Logs at step 3136: {'loss': 2.2888, 'grad_norm': 1.5569201707839966, 'learning_rate': 1.5236194278110446e-05, 'epoch': 2.0864936793080506}\n",
            "Captured loss: 2.2888 at step 3136\n",
            "Logs at step 3137: {'loss': 2.5943, 'grad_norm': 1.296398401260376, 'learning_rate': 1.5225105344865825e-05, 'epoch': 2.0871590153027277}\n",
            "Captured loss: 2.5943 at step 3137\n",
            "Logs at step 3138: {'loss': 2.3058, 'grad_norm': 1.3378159999847412, 'learning_rate': 1.5214016411621201e-05, 'epoch': 2.0878243512974053}\n",
            "Captured loss: 2.3058 at step 3138\n",
            "Logs at step 3139: {'loss': 2.5236, 'grad_norm': 1.1555606126785278, 'learning_rate': 1.520292747837658e-05, 'epoch': 2.0884896872920824}\n",
            "Captured loss: 2.5236 at step 3139\n",
            "Logs at step 3140: {'loss': 2.483, 'grad_norm': 1.151169776916504, 'learning_rate': 1.519183854513196e-05, 'epoch': 2.08915502328676}\n",
            "Captured loss: 2.483 at step 3140\n",
            "Logs at step 3141: {'loss': 2.5697, 'grad_norm': 1.532389521598816, 'learning_rate': 1.5180749611887337e-05, 'epoch': 2.089820359281437}\n",
            "Captured loss: 2.5697 at step 3141\n",
            "Logs at step 3142: {'loss': 2.3406, 'grad_norm': 1.210480809211731, 'learning_rate': 1.5169660678642716e-05, 'epoch': 2.0904856952761146}\n",
            "Captured loss: 2.3406 at step 3142\n",
            "Logs at step 3143: {'loss': 2.4427, 'grad_norm': 1.5910885334014893, 'learning_rate': 1.5158571745398092e-05, 'epoch': 2.0911510312707917}\n",
            "Captured loss: 2.4427 at step 3143\n",
            "Logs at step 3144: {'loss': 2.5662, 'grad_norm': 1.4180985689163208, 'learning_rate': 1.5147482812153472e-05, 'epoch': 2.0918163672654693}\n",
            "Captured loss: 2.5662 at step 3144\n",
            "Logs at step 3145: {'loss': 2.6352, 'grad_norm': 1.3715934753417969, 'learning_rate': 1.513639387890885e-05, 'epoch': 2.0924817032601464}\n",
            "Captured loss: 2.6352 at step 3145\n",
            "Logs at step 3146: {'loss': 2.3574, 'grad_norm': 1.227170705795288, 'learning_rate': 1.5125304945664228e-05, 'epoch': 2.0931470392548235}\n",
            "Captured loss: 2.3574 at step 3146\n",
            "Logs at step 3147: {'loss': 2.0881, 'grad_norm': 1.4249343872070312, 'learning_rate': 1.5114216012419604e-05, 'epoch': 2.093812375249501}\n",
            "Captured loss: 2.0881 at step 3147\n",
            "Logs at step 3148: {'loss': 2.2547, 'grad_norm': 1.9185672998428345, 'learning_rate': 1.5103127079174983e-05, 'epoch': 2.094477711244178}\n",
            "Captured loss: 2.2547 at step 3148\n",
            "Logs at step 3149: {'loss': 2.466, 'grad_norm': 1.4755033254623413, 'learning_rate': 1.5092038145930363e-05, 'epoch': 2.0951430472388557}\n",
            "Captured loss: 2.466 at step 3149\n",
            "Logs at step 3150: {'loss': 2.4396, 'grad_norm': 1.3969601392745972, 'learning_rate': 1.508094921268574e-05, 'epoch': 2.095808383233533}\n",
            "Captured loss: 2.4396 at step 3150\n",
            "Logs at step 3151: {'loss': 2.5878, 'grad_norm': 1.2145378589630127, 'learning_rate': 1.506986027944112e-05, 'epoch': 2.0964737192282104}\n",
            "Captured loss: 2.5878 at step 3151\n",
            "Logs at step 3152: {'loss': 2.5458, 'grad_norm': 1.131165623664856, 'learning_rate': 1.5058771346196495e-05, 'epoch': 2.0971390552228875}\n",
            "Captured loss: 2.5458 at step 3152\n",
            "Logs at step 3153: {'loss': 2.5099, 'grad_norm': 1.2423440217971802, 'learning_rate': 1.5047682412951875e-05, 'epoch': 2.097804391217565}\n",
            "Captured loss: 2.5099 at step 3153\n",
            "Logs at step 3154: {'loss': 2.5442, 'grad_norm': 1.3687406778335571, 'learning_rate': 1.5036593479707254e-05, 'epoch': 2.098469727212242}\n",
            "Captured loss: 2.5442 at step 3154\n",
            "Logs at step 3155: {'loss': 2.1164, 'grad_norm': 1.3283294439315796, 'learning_rate': 1.5025504546462631e-05, 'epoch': 2.0991350632069197}\n",
            "Captured loss: 2.1164 at step 3155\n",
            "Logs at step 3156: {'loss': 2.4599, 'grad_norm': 1.5018419027328491, 'learning_rate': 1.5014415613218009e-05, 'epoch': 2.099800399201597}\n",
            "Captured loss: 2.4599 at step 3156\n",
            "Logs at step 3157: {'loss': 2.5046, 'grad_norm': 1.595942735671997, 'learning_rate': 1.5003326679973386e-05, 'epoch': 2.1004657351962743}\n",
            "Captured loss: 2.5046 at step 3157\n",
            "Logs at step 3158: {'loss': 2.5803, 'grad_norm': 1.3748035430908203, 'learning_rate': 1.4992237746728766e-05, 'epoch': 2.1011310711909514}\n",
            "Captured loss: 2.5803 at step 3158\n",
            "Logs at step 3159: {'loss': 2.487, 'grad_norm': 1.497011661529541, 'learning_rate': 1.4981148813484145e-05, 'epoch': 2.1017964071856285}\n",
            "Captured loss: 2.487 at step 3159\n",
            "Logs at step 3160: {'loss': 2.526, 'grad_norm': 1.354711890220642, 'learning_rate': 1.497005988023952e-05, 'epoch': 2.102461743180306}\n",
            "Captured loss: 2.526 at step 3160\n",
            "Logs at step 3161: {'loss': 2.4303, 'grad_norm': 1.8644579648971558, 'learning_rate': 1.4958970946994898e-05, 'epoch': 2.103127079174983}\n",
            "Captured loss: 2.4303 at step 3161\n",
            "Logs at step 3162: {'loss': 2.1103, 'grad_norm': 1.6614090204238892, 'learning_rate': 1.4947882013750278e-05, 'epoch': 2.1037924151696608}\n",
            "Captured loss: 2.1103 at step 3162\n",
            "Logs at step 3163: {'loss': 2.3028, 'grad_norm': 1.351539969444275, 'learning_rate': 1.4936793080505657e-05, 'epoch': 2.104457751164338}\n",
            "Captured loss: 2.3028 at step 3163\n",
            "Logs at step 3164: {'loss': 2.3394, 'grad_norm': 1.3056682348251343, 'learning_rate': 1.4925704147261036e-05, 'epoch': 2.1051230871590154}\n",
            "Captured loss: 2.3394 at step 3164\n",
            "Logs at step 3165: {'loss': 2.5724, 'grad_norm': 1.4289945363998413, 'learning_rate': 1.4914615214016412e-05, 'epoch': 2.1057884231536925}\n",
            "Captured loss: 2.5724 at step 3165\n",
            "Logs at step 3166: {'loss': 2.426, 'grad_norm': 1.3041952848434448, 'learning_rate': 1.490352628077179e-05, 'epoch': 2.10645375914837}\n",
            "Captured loss: 2.426 at step 3166\n",
            "Logs at step 3167: {'loss': 2.4722, 'grad_norm': 1.1732330322265625, 'learning_rate': 1.4892437347527169e-05, 'epoch': 2.107119095143047}\n",
            "Captured loss: 2.4722 at step 3167\n",
            "Logs at step 3168: {'loss': 2.492, 'grad_norm': 1.251899242401123, 'learning_rate': 1.4881348414282548e-05, 'epoch': 2.1077844311377247}\n",
            "Captured loss: 2.492 at step 3168\n",
            "Logs at step 3169: {'loss': 2.5548, 'grad_norm': 1.1373423337936401, 'learning_rate': 1.4870259481037924e-05, 'epoch': 2.108449767132402}\n",
            "Captured loss: 2.5548 at step 3169\n",
            "Logs at step 3170: {'loss': 2.4538, 'grad_norm': 1.8525962829589844, 'learning_rate': 1.4859170547793303e-05, 'epoch': 2.1091151031270794}\n",
            "Captured loss: 2.4538 at step 3170\n",
            "Logs at step 3171: {'loss': 2.471, 'grad_norm': 1.160483717918396, 'learning_rate': 1.484808161454868e-05, 'epoch': 2.1097804391217565}\n",
            "Captured loss: 2.471 at step 3171\n",
            "Logs at step 3172: {'loss': 2.4411, 'grad_norm': 1.215579628944397, 'learning_rate': 1.483699268130406e-05, 'epoch': 2.1104457751164336}\n",
            "Captured loss: 2.4411 at step 3172\n",
            "Logs at step 3173: {'loss': 2.4174, 'grad_norm': 1.6009222269058228, 'learning_rate': 1.4825903748059439e-05, 'epoch': 2.111111111111111}\n",
            "Captured loss: 2.4174 at step 3173\n",
            "Logs at step 3174: {'loss': 2.5626, 'grad_norm': 1.9447698593139648, 'learning_rate': 1.4814814814814815e-05, 'epoch': 2.1117764471057883}\n",
            "Captured loss: 2.5626 at step 3174\n",
            "Logs at step 3175: {'loss': 2.4607, 'grad_norm': 1.4097379446029663, 'learning_rate': 1.4803725881570194e-05, 'epoch': 2.112441783100466}\n",
            "Captured loss: 2.4607 at step 3175\n",
            "Logs at step 3176: {'loss': 2.4443, 'grad_norm': 0.9635981917381287, 'learning_rate': 1.4792636948325572e-05, 'epoch': 2.113107119095143}\n",
            "Captured loss: 2.4443 at step 3176\n",
            "Logs at step 3177: {'loss': 2.532, 'grad_norm': 1.5200477838516235, 'learning_rate': 1.4781548015080951e-05, 'epoch': 2.1137724550898205}\n",
            "Captured loss: 2.532 at step 3177\n",
            "Logs at step 3178: {'loss': 2.145, 'grad_norm': 1.362524390220642, 'learning_rate': 1.4770459081836327e-05, 'epoch': 2.1144377910844976}\n",
            "Captured loss: 2.145 at step 3178\n",
            "Logs at step 3179: {'loss': 2.4159, 'grad_norm': 1.8885412216186523, 'learning_rate': 1.4759370148591706e-05, 'epoch': 2.115103127079175}\n",
            "Captured loss: 2.4159 at step 3179\n",
            "Logs at step 3180: {'loss': 2.4097, 'grad_norm': 1.4577724933624268, 'learning_rate': 1.4748281215347085e-05, 'epoch': 2.1157684630738522}\n",
            "Captured loss: 2.4097 at step 3180\n",
            "Logs at step 3181: {'loss': 2.271, 'grad_norm': 1.3410416841506958, 'learning_rate': 1.4737192282102463e-05, 'epoch': 2.11643379906853}\n",
            "Captured loss: 2.271 at step 3181\n",
            "Logs at step 3182: {'loss': 2.2429, 'grad_norm': 1.6199122667312622, 'learning_rate': 1.4726103348857839e-05, 'epoch': 2.117099135063207}\n",
            "Captured loss: 2.2429 at step 3182\n",
            "Logs at step 3183: {'loss': 2.5704, 'grad_norm': 1.1118510961532593, 'learning_rate': 1.4715014415613218e-05, 'epoch': 2.1177644710578845}\n",
            "Captured loss: 2.5704 at step 3183\n",
            "Logs at step 3184: {'loss': 2.3198, 'grad_norm': 1.1983184814453125, 'learning_rate': 1.4703925482368597e-05, 'epoch': 2.1184298070525616}\n",
            "Captured loss: 2.3198 at step 3184\n",
            "Logs at step 3185: {'loss': 2.5265, 'grad_norm': 1.5909433364868164, 'learning_rate': 1.4692836549123975e-05, 'epoch': 2.1190951430472387}\n",
            "Captured loss: 2.5265 at step 3185\n",
            "Logs at step 3186: {'loss': 2.2226, 'grad_norm': 1.5291775465011597, 'learning_rate': 1.4681747615879354e-05, 'epoch': 2.1197604790419162}\n",
            "Captured loss: 2.2226 at step 3186\n",
            "Logs at step 3187: {'loss': 2.5317, 'grad_norm': 1.2820590734481812, 'learning_rate': 1.467065868263473e-05, 'epoch': 2.1204258150365933}\n",
            "Captured loss: 2.5317 at step 3187\n",
            "Logs at step 3188: {'loss': 2.5405, 'grad_norm': 1.5974910259246826, 'learning_rate': 1.4659569749390109e-05, 'epoch': 2.121091151031271}\n",
            "Captured loss: 2.5405 at step 3188\n",
            "Logs at step 3189: {'loss': 2.5252, 'grad_norm': 1.3309528827667236, 'learning_rate': 1.4648480816145488e-05, 'epoch': 2.121756487025948}\n",
            "Captured loss: 2.5252 at step 3189\n",
            "Logs at step 3190: {'loss': 2.4806, 'grad_norm': 1.0531998872756958, 'learning_rate': 1.4637391882900866e-05, 'epoch': 2.1224218230206255}\n",
            "Captured loss: 2.4806 at step 3190\n",
            "Logs at step 3191: {'loss': 2.567, 'grad_norm': 1.0817956924438477, 'learning_rate': 1.4626302949656242e-05, 'epoch': 2.1230871590153026}\n",
            "Captured loss: 2.567 at step 3191\n",
            "Logs at step 3192: {'loss': 2.5221, 'grad_norm': 1.766990065574646, 'learning_rate': 1.4615214016411621e-05, 'epoch': 2.12375249500998}\n",
            "Captured loss: 2.5221 at step 3192\n",
            "Logs at step 3193: {'loss': 2.6763, 'grad_norm': 1.7455592155456543, 'learning_rate': 1.4604125083167e-05, 'epoch': 2.1244178310046573}\n",
            "Captured loss: 2.6763 at step 3193\n",
            "Logs at step 3194: {'loss': 2.6376, 'grad_norm': 1.422341227531433, 'learning_rate': 1.459303614992238e-05, 'epoch': 2.125083166999335}\n",
            "Captured loss: 2.6376 at step 3194\n",
            "Logs at step 3195: {'loss': 2.1283, 'grad_norm': 1.4444029331207275, 'learning_rate': 1.4581947216677757e-05, 'epoch': 2.125748502994012}\n",
            "Captured loss: 2.1283 at step 3195\n",
            "Logs at step 3196: {'loss': 2.5915, 'grad_norm': 1.1839162111282349, 'learning_rate': 1.4570858283433133e-05, 'epoch': 2.1264138389886895}\n",
            "Captured loss: 2.5915 at step 3196\n",
            "Logs at step 3197: {'loss': 2.2203, 'grad_norm': 1.8099521398544312, 'learning_rate': 1.4559769350188512e-05, 'epoch': 2.1270791749833666}\n",
            "Captured loss: 2.2203 at step 3197\n",
            "Logs at step 3198: {'loss': 2.446, 'grad_norm': 1.1729474067687988, 'learning_rate': 1.4548680416943891e-05, 'epoch': 2.1277445109780437}\n",
            "Captured loss: 2.446 at step 3198\n",
            "Logs at step 3199: {'loss': 2.1394, 'grad_norm': 1.7599555253982544, 'learning_rate': 1.453759148369927e-05, 'epoch': 2.1284098469727213}\n",
            "Captured loss: 2.1394 at step 3199\n",
            "Logs at step 3200: {'loss': 2.4171, 'grad_norm': 2.205218553543091, 'learning_rate': 1.4526502550454647e-05, 'epoch': 2.1290751829673984}\n",
            "Captured loss: 2.4171 at step 3200\n",
            "Logs at step 3201: {'loss': 2.02, 'grad_norm': 1.7064995765686035, 'learning_rate': 1.4515413617210024e-05, 'epoch': 2.129740518962076}\n",
            "Captured loss: 2.02 at step 3201\n",
            "Logs at step 3202: {'loss': 2.1912, 'grad_norm': 1.2760978937149048, 'learning_rate': 1.4504324683965403e-05, 'epoch': 2.130405854956753}\n",
            "Captured loss: 2.1912 at step 3202\n",
            "Logs at step 3203: {'loss': 2.1623, 'grad_norm': 1.5576410293579102, 'learning_rate': 1.4493235750720783e-05, 'epoch': 2.1310711909514306}\n",
            "Captured loss: 2.1623 at step 3203\n",
            "Logs at step 3204: {'loss': 2.3784, 'grad_norm': 1.1694871187210083, 'learning_rate': 1.4482146817476158e-05, 'epoch': 2.1317365269461077}\n",
            "Captured loss: 2.3784 at step 3204\n",
            "Logs at step 3205: {'loss': 2.2461, 'grad_norm': 1.4396147727966309, 'learning_rate': 1.4471057884231538e-05, 'epoch': 2.1324018629407853}\n",
            "Captured loss: 2.2461 at step 3205\n",
            "Logs at step 3206: {'loss': 2.586, 'grad_norm': 1.6590182781219482, 'learning_rate': 1.4459968950986915e-05, 'epoch': 2.1330671989354624}\n",
            "Captured loss: 2.586 at step 3206\n",
            "Logs at step 3207: {'loss': 2.2318, 'grad_norm': 1.2130038738250732, 'learning_rate': 1.4448880017742294e-05, 'epoch': 2.13373253493014}\n",
            "Captured loss: 2.2318 at step 3207\n",
            "Logs at step 3208: {'loss': 2.4003, 'grad_norm': 1.8093080520629883, 'learning_rate': 1.4437791084497674e-05, 'epoch': 2.134397870924817}\n",
            "Captured loss: 2.4003 at step 3208\n",
            "Logs at step 3209: {'loss': 1.9314, 'grad_norm': 1.5440869331359863, 'learning_rate': 1.442670215125305e-05, 'epoch': 2.135063206919494}\n",
            "Captured loss: 1.9314 at step 3209\n",
            "Logs at step 3210: {'loss': 2.5273, 'grad_norm': 1.2298377752304077, 'learning_rate': 1.4415613218008429e-05, 'epoch': 2.1357285429141717}\n",
            "Captured loss: 2.5273 at step 3210\n",
            "Logs at step 3211: {'loss': 2.2901, 'grad_norm': 1.6973803043365479, 'learning_rate': 1.4404524284763806e-05, 'epoch': 2.136393878908849}\n",
            "Captured loss: 2.2901 at step 3211\n",
            "Logs at step 3212: {'loss': 2.5961, 'grad_norm': 2.9457590579986572, 'learning_rate': 1.4393435351519186e-05, 'epoch': 2.1370592149035263}\n",
            "Captured loss: 2.5961 at step 3212\n",
            "Logs at step 3213: {'loss': 2.6126, 'grad_norm': 1.1873009204864502, 'learning_rate': 1.4382346418274561e-05, 'epoch': 2.1377245508982035}\n",
            "Captured loss: 2.6126 at step 3213\n",
            "Logs at step 3214: {'loss': 2.5797, 'grad_norm': 1.3282450437545776, 'learning_rate': 1.437125748502994e-05, 'epoch': 2.138389886892881}\n",
            "Captured loss: 2.5797 at step 3214\n",
            "Logs at step 3215: {'loss': 2.4788, 'grad_norm': 1.1652576923370361, 'learning_rate': 1.4360168551785318e-05, 'epoch': 2.139055222887558}\n",
            "Captured loss: 2.4788 at step 3215\n",
            "Logs at step 3216: {'loss': 2.2338, 'grad_norm': 4.308282375335693, 'learning_rate': 1.4349079618540697e-05, 'epoch': 2.1397205588822357}\n",
            "Captured loss: 2.2338 at step 3216\n",
            "Logs at step 3217: {'loss': 2.6919, 'grad_norm': 1.4260960817337036, 'learning_rate': 1.4337990685296077e-05, 'epoch': 2.1403858948769128}\n",
            "Captured loss: 2.6919 at step 3217\n",
            "Logs at step 3218: {'loss': 2.501, 'grad_norm': 1.2585275173187256, 'learning_rate': 1.4326901752051453e-05, 'epoch': 2.1410512308715903}\n",
            "Captured loss: 2.501 at step 3218\n",
            "Logs at step 3219: {'loss': 2.5204, 'grad_norm': 1.3172667026519775, 'learning_rate': 1.4315812818806832e-05, 'epoch': 2.1417165668662674}\n",
            "Captured loss: 2.5204 at step 3219\n",
            "Logs at step 3220: {'loss': 2.3846, 'grad_norm': 1.6340276002883911, 'learning_rate': 1.430472388556221e-05, 'epoch': 2.1423819028609445}\n",
            "Captured loss: 2.3846 at step 3220\n",
            "Logs at step 3221: {'loss': 2.463, 'grad_norm': 1.2143783569335938, 'learning_rate': 1.4293634952317589e-05, 'epoch': 2.143047238855622}\n",
            "Captured loss: 2.463 at step 3221\n",
            "Logs at step 3222: {'loss': 2.4721, 'grad_norm': 1.3469901084899902, 'learning_rate': 1.4282546019072964e-05, 'epoch': 2.143712574850299}\n",
            "Captured loss: 2.4721 at step 3222\n",
            "Logs at step 3223: {'loss': 2.446, 'grad_norm': 1.2143436670303345, 'learning_rate': 1.4271457085828344e-05, 'epoch': 2.1443779108449768}\n",
            "Captured loss: 2.446 at step 3223\n",
            "Logs at step 3224: {'loss': 2.5279, 'grad_norm': 1.952357292175293, 'learning_rate': 1.4260368152583723e-05, 'epoch': 2.145043246839654}\n",
            "Captured loss: 2.5279 at step 3224\n",
            "Logs at step 3225: {'loss': 2.0944, 'grad_norm': 2.217507839202881, 'learning_rate': 1.42492792193391e-05, 'epoch': 2.1457085828343314}\n",
            "Captured loss: 2.0944 at step 3225\n",
            "Logs at step 3226: {'loss': 2.2757, 'grad_norm': 1.71242356300354, 'learning_rate': 1.4238190286094476e-05, 'epoch': 2.1463739188290085}\n",
            "Captured loss: 2.2757 at step 3226\n",
            "Logs at step 3227: {'loss': 2.5714, 'grad_norm': 1.3165708780288696, 'learning_rate': 1.4227101352849856e-05, 'epoch': 2.147039254823686}\n",
            "Captured loss: 2.5714 at step 3227\n",
            "Logs at step 3228: {'loss': 2.5874, 'grad_norm': 1.7118133306503296, 'learning_rate': 1.4216012419605235e-05, 'epoch': 2.147704590818363}\n",
            "Captured loss: 2.5874 at step 3228\n",
            "Logs at step 3229: {'loss': 2.2959, 'grad_norm': 1.1114016771316528, 'learning_rate': 1.4204923486360614e-05, 'epoch': 2.1483699268130407}\n",
            "Captured loss: 2.2959 at step 3229\n",
            "Logs at step 3230: {'loss': 2.4752, 'grad_norm': 0.9706200361251831, 'learning_rate': 1.4193834553115992e-05, 'epoch': 2.149035262807718}\n",
            "Captured loss: 2.4752 at step 3230\n",
            "Logs at step 3231: {'loss': 1.9366, 'grad_norm': 1.515915870666504, 'learning_rate': 1.4182745619871368e-05, 'epoch': 2.1497005988023954}\n",
            "Captured loss: 1.9366 at step 3231\n",
            "Logs at step 3232: {'loss': 2.0979, 'grad_norm': 1.891417384147644, 'learning_rate': 1.4171656686626747e-05, 'epoch': 2.1503659347970725}\n",
            "Captured loss: 2.0979 at step 3232\n",
            "Logs at step 3233: {'loss': 2.6184, 'grad_norm': 1.4446443319320679, 'learning_rate': 1.4160567753382126e-05, 'epoch': 2.1510312707917496}\n",
            "Captured loss: 2.6184 at step 3233\n",
            "Logs at step 3234: {'loss': 2.1043, 'grad_norm': 1.5527665615081787, 'learning_rate': 1.4149478820137505e-05, 'epoch': 2.151696606786427}\n",
            "Captured loss: 2.1043 at step 3234\n",
            "Logs at step 3235: {'loss': 2.5269, 'grad_norm': 1.597240686416626, 'learning_rate': 1.4138389886892881e-05, 'epoch': 2.1523619427811043}\n",
            "Captured loss: 2.5269 at step 3235\n",
            "Logs at step 3236: {'loss': 2.2093, 'grad_norm': 1.1574152708053589, 'learning_rate': 1.4127300953648259e-05, 'epoch': 2.153027278775782}\n",
            "Captured loss: 2.2093 at step 3236\n",
            "Logs at step 3237: {'loss': 2.1365, 'grad_norm': 1.5946766138076782, 'learning_rate': 1.4116212020403638e-05, 'epoch': 2.153692614770459}\n",
            "Captured loss: 2.1365 at step 3237\n",
            "Logs at step 3238: {'loss': 2.5131, 'grad_norm': 1.5363186597824097, 'learning_rate': 1.4105123087159017e-05, 'epoch': 2.1543579507651365}\n",
            "Captured loss: 2.5131 at step 3238\n",
            "Logs at step 3239: {'loss': 2.7298, 'grad_norm': 1.7540194988250732, 'learning_rate': 1.4094034153914393e-05, 'epoch': 2.1550232867598136}\n",
            "Captured loss: 2.7298 at step 3239\n",
            "Logs at step 3240: {'loss': 2.4081, 'grad_norm': 1.2886871099472046, 'learning_rate': 1.4082945220669772e-05, 'epoch': 2.155688622754491}\n",
            "Captured loss: 2.4081 at step 3240\n",
            "Logs at step 3241: {'loss': 2.5343, 'grad_norm': 1.3794916868209839, 'learning_rate': 1.407185628742515e-05, 'epoch': 2.1563539587491682}\n",
            "Captured loss: 2.5343 at step 3241\n",
            "Logs at step 3242: {'loss': 2.6592, 'grad_norm': 1.3485544919967651, 'learning_rate': 1.4060767354180529e-05, 'epoch': 2.157019294743846}\n",
            "Captured loss: 2.6592 at step 3242\n",
            "Logs at step 3243: {'loss': 2.4581, 'grad_norm': 1.7376103401184082, 'learning_rate': 1.4049678420935908e-05, 'epoch': 2.157684630738523}\n",
            "Captured loss: 2.4581 at step 3243\n",
            "Logs at step 3244: {'loss': 2.3246, 'grad_norm': 1.3166232109069824, 'learning_rate': 1.4038589487691284e-05, 'epoch': 2.1583499667332005}\n",
            "Captured loss: 2.3246 at step 3244\n",
            "Logs at step 3245: {'loss': 2.1557, 'grad_norm': 1.8795051574707031, 'learning_rate': 1.4027500554446663e-05, 'epoch': 2.1590153027278776}\n",
            "Captured loss: 2.1557 at step 3245\n",
            "Logs at step 3246: {'loss': 2.1488, 'grad_norm': 1.453503131866455, 'learning_rate': 1.4016411621202041e-05, 'epoch': 2.1596806387225547}\n",
            "Captured loss: 2.1488 at step 3246\n",
            "Logs at step 3247: {'loss': 2.5551, 'grad_norm': 1.3968697786331177, 'learning_rate': 1.400532268795742e-05, 'epoch': 2.160345974717232}\n",
            "Captured loss: 2.5551 at step 3247\n",
            "Logs at step 3248: {'loss': 2.5496, 'grad_norm': 1.244912028312683, 'learning_rate': 1.3994233754712796e-05, 'epoch': 2.1610113107119093}\n",
            "Captured loss: 2.5496 at step 3248\n",
            "Logs at step 3249: {'loss': 1.8411, 'grad_norm': 2.2444732189178467, 'learning_rate': 1.3983144821468175e-05, 'epoch': 2.161676646706587}\n",
            "Captured loss: 1.8411 at step 3249\n",
            "Logs at step 3250: {'loss': 2.7019, 'grad_norm': 1.4765499830245972, 'learning_rate': 1.3972055888223553e-05, 'epoch': 2.162341982701264}\n",
            "Captured loss: 2.7019 at step 3250\n",
            "Logs at step 3251: {'loss': 2.347, 'grad_norm': 1.3109641075134277, 'learning_rate': 1.3960966954978932e-05, 'epoch': 2.1630073186959415}\n",
            "Captured loss: 2.347 at step 3251\n",
            "Logs at step 3252: {'loss': 2.8183, 'grad_norm': 1.8186628818511963, 'learning_rate': 1.3949878021734311e-05, 'epoch': 2.1636726546906186}\n",
            "Captured loss: 2.8183 at step 3252\n",
            "Logs at step 3253: {'loss': 2.1204, 'grad_norm': 1.414742112159729, 'learning_rate': 1.3938789088489687e-05, 'epoch': 2.164337990685296}\n",
            "Captured loss: 2.1204 at step 3253\n",
            "Logs at step 3254: {'loss': 2.4356, 'grad_norm': 1.314927101135254, 'learning_rate': 1.3927700155245066e-05, 'epoch': 2.1650033266799733}\n",
            "Captured loss: 2.4356 at step 3254\n",
            "Logs at step 3255: {'loss': 2.5127, 'grad_norm': 1.5113673210144043, 'learning_rate': 1.3916611222000444e-05, 'epoch': 2.165668662674651}\n",
            "Captured loss: 2.5127 at step 3255\n",
            "Logs at step 3256: {'loss': 2.5565, 'grad_norm': 1.2641217708587646, 'learning_rate': 1.3905522288755823e-05, 'epoch': 2.166333998669328}\n",
            "Captured loss: 2.5565 at step 3256\n",
            "Logs at step 3257: {'loss': 2.5496, 'grad_norm': 1.4132318496704102, 'learning_rate': 1.3894433355511199e-05, 'epoch': 2.1669993346640055}\n",
            "Captured loss: 2.5496 at step 3257\n",
            "Logs at step 3258: {'loss': 2.441, 'grad_norm': 1.6486010551452637, 'learning_rate': 1.3883344422266578e-05, 'epoch': 2.1676646706586826}\n",
            "Captured loss: 2.441 at step 3258\n",
            "Logs at step 3259: {'loss': 2.7608, 'grad_norm': 1.89570951461792, 'learning_rate': 1.3872255489021958e-05, 'epoch': 2.1683300066533597}\n",
            "Captured loss: 2.7608 at step 3259\n",
            "Logs at step 3260: {'loss': 2.4122, 'grad_norm': 1.1937363147735596, 'learning_rate': 1.3861166555777335e-05, 'epoch': 2.1689953426480373}\n",
            "Captured loss: 2.4122 at step 3260\n",
            "Logs at step 3261: {'loss': 2.5181, 'grad_norm': 1.4689011573791504, 'learning_rate': 1.3850077622532711e-05, 'epoch': 2.1696606786427144}\n",
            "Captured loss: 2.5181 at step 3261\n",
            "Logs at step 3262: {'loss': 2.1846, 'grad_norm': 1.5752030611038208, 'learning_rate': 1.383898868928809e-05, 'epoch': 2.170326014637392}\n",
            "Captured loss: 2.1846 at step 3262\n",
            "Logs at step 3263: {'loss': 2.5216, 'grad_norm': 1.6082650423049927, 'learning_rate': 1.382789975604347e-05, 'epoch': 2.170991350632069}\n",
            "Captured loss: 2.5216 at step 3263\n",
            "Logs at step 3264: {'loss': 2.4842, 'grad_norm': 1.1987630128860474, 'learning_rate': 1.3816810822798849e-05, 'epoch': 2.1716566866267466}\n",
            "Captured loss: 2.4842 at step 3264\n",
            "Logs at step 3265: {'loss': 2.5064, 'grad_norm': 1.3438295125961304, 'learning_rate': 1.3805721889554226e-05, 'epoch': 2.1723220226214237}\n",
            "Captured loss: 2.5064 at step 3265\n",
            "Logs at step 3266: {'loss': 2.4886, 'grad_norm': 1.9504749774932861, 'learning_rate': 1.3794632956309602e-05, 'epoch': 2.1729873586161013}\n",
            "Captured loss: 2.4886 at step 3266\n",
            "Logs at step 3267: {'loss': 2.1999, 'grad_norm': 2.0385215282440186, 'learning_rate': 1.3783544023064981e-05, 'epoch': 2.1736526946107784}\n",
            "Captured loss: 2.1999 at step 3267\n",
            "Logs at step 3268: {'loss': 2.4825, 'grad_norm': 1.9480657577514648, 'learning_rate': 1.377245508982036e-05, 'epoch': 2.174318030605456}\n",
            "Captured loss: 2.4825 at step 3268\n",
            "Logs at step 3269: {'loss': 2.3088, 'grad_norm': 1.4679169654846191, 'learning_rate': 1.376136615657574e-05, 'epoch': 2.174983366600133}\n",
            "Captured loss: 2.3088 at step 3269\n",
            "Logs at step 3270: {'loss': 2.1145, 'grad_norm': 1.5374759435653687, 'learning_rate': 1.3750277223331116e-05, 'epoch': 2.1756487025948106}\n",
            "Captured loss: 2.1145 at step 3270\n",
            "Logs at step 3271: {'loss': 2.3848, 'grad_norm': 1.2192168235778809, 'learning_rate': 1.3739188290086493e-05, 'epoch': 2.1763140385894877}\n",
            "Captured loss: 2.3848 at step 3271\n",
            "Logs at step 3272: {'loss': 2.4163, 'grad_norm': 1.024269461631775, 'learning_rate': 1.3728099356841873e-05, 'epoch': 2.176979374584165}\n",
            "Captured loss: 2.4163 at step 3272\n",
            "Logs at step 3273: {'loss': 2.622, 'grad_norm': 1.8661936521530151, 'learning_rate': 1.3717010423597252e-05, 'epoch': 2.1776447105788423}\n",
            "Captured loss: 2.622 at step 3273\n",
            "Logs at step 3274: {'loss': 2.2018, 'grad_norm': 1.7360880374908447, 'learning_rate': 1.370592149035263e-05, 'epoch': 2.1783100465735195}\n",
            "Captured loss: 2.2018 at step 3274\n",
            "Logs at step 3275: {'loss': 2.5964, 'grad_norm': 1.2999117374420166, 'learning_rate': 1.3694832557108007e-05, 'epoch': 2.178975382568197}\n",
            "Captured loss: 2.5964 at step 3275\n",
            "Logs at step 3276: {'loss': 2.0858, 'grad_norm': 1.3948601484298706, 'learning_rate': 1.3683743623863384e-05, 'epoch': 2.179640718562874}\n",
            "Captured loss: 2.0858 at step 3276\n",
            "Logs at step 3277: {'loss': 2.547, 'grad_norm': 1.1179051399230957, 'learning_rate': 1.3672654690618764e-05, 'epoch': 2.1803060545575517}\n",
            "Captured loss: 2.547 at step 3277\n",
            "Logs at step 3278: {'loss': 2.1975, 'grad_norm': 1.90520441532135, 'learning_rate': 1.3661565757374143e-05, 'epoch': 2.1809713905522288}\n",
            "Captured loss: 2.1975 at step 3278\n",
            "Logs at step 3279: {'loss': 2.1204, 'grad_norm': 1.484532117843628, 'learning_rate': 1.3650476824129519e-05, 'epoch': 2.1816367265469063}\n",
            "Captured loss: 2.1204 at step 3279\n",
            "Logs at step 3280: {'loss': 2.528, 'grad_norm': 0.9982765316963196, 'learning_rate': 1.3639387890884896e-05, 'epoch': 2.1823020625415834}\n",
            "Captured loss: 2.528 at step 3280\n",
            "Logs at step 3281: {'loss': 2.5246, 'grad_norm': 1.5568803548812866, 'learning_rate': 1.3628298957640276e-05, 'epoch': 2.182967398536261}\n",
            "Captured loss: 2.5246 at step 3281\n",
            "Logs at step 3282: {'loss': 2.4099, 'grad_norm': 2.128451347351074, 'learning_rate': 1.3617210024395655e-05, 'epoch': 2.183632734530938}\n",
            "Captured loss: 2.4099 at step 3282\n",
            "Logs at step 3283: {'loss': 2.459, 'grad_norm': 1.8565887212753296, 'learning_rate': 1.360612109115103e-05, 'epoch': 2.1842980705256156}\n",
            "Captured loss: 2.459 at step 3283\n",
            "Logs at step 3284: {'loss': 2.1925, 'grad_norm': 1.4846621751785278, 'learning_rate': 1.359503215790641e-05, 'epoch': 2.1849634065202928}\n",
            "Captured loss: 2.1925 at step 3284\n",
            "Logs at step 3285: {'loss': 2.5187, 'grad_norm': 1.5359264612197876, 'learning_rate': 1.3583943224661787e-05, 'epoch': 2.18562874251497}\n",
            "Captured loss: 2.5187 at step 3285\n",
            "Logs at step 3286: {'loss': 2.0683, 'grad_norm': 1.234832763671875, 'learning_rate': 1.3572854291417167e-05, 'epoch': 2.1862940785096474}\n",
            "Captured loss: 2.0683 at step 3286\n",
            "Logs at step 3287: {'loss': 2.6505, 'grad_norm': 1.5223243236541748, 'learning_rate': 1.3561765358172546e-05, 'epoch': 2.1869594145043245}\n",
            "Captured loss: 2.6505 at step 3287\n",
            "Logs at step 3288: {'loss': 2.5905, 'grad_norm': 1.262578010559082, 'learning_rate': 1.3550676424927922e-05, 'epoch': 2.187624750499002}\n",
            "Captured loss: 2.5905 at step 3288\n",
            "Logs at step 3289: {'loss': 2.4406, 'grad_norm': 2.065675735473633, 'learning_rate': 1.3539587491683301e-05, 'epoch': 2.188290086493679}\n",
            "Captured loss: 2.4406 at step 3289\n",
            "Logs at step 3290: {'loss': 2.3151, 'grad_norm': 1.7556170225143433, 'learning_rate': 1.3528498558438679e-05, 'epoch': 2.1889554224883567}\n",
            "Captured loss: 2.3151 at step 3290\n",
            "Logs at step 3291: {'loss': 2.0345, 'grad_norm': 2.440971612930298, 'learning_rate': 1.3517409625194058e-05, 'epoch': 2.189620758483034}\n",
            "Captured loss: 2.0345 at step 3291\n",
            "Logs at step 3292: {'loss': 2.4372, 'grad_norm': 1.6469919681549072, 'learning_rate': 1.3506320691949434e-05, 'epoch': 2.1902860944777114}\n",
            "Captured loss: 2.4372 at step 3292\n",
            "Logs at step 3293: {'loss': 2.518, 'grad_norm': 1.3590351343154907, 'learning_rate': 1.3495231758704813e-05, 'epoch': 2.1909514304723885}\n",
            "Captured loss: 2.518 at step 3293\n",
            "Logs at step 3294: {'loss': 2.2947, 'grad_norm': 1.7765008211135864, 'learning_rate': 1.3484142825460192e-05, 'epoch': 2.191616766467066}\n",
            "Captured loss: 2.2947 at step 3294\n",
            "Logs at step 3295: {'loss': 2.4456, 'grad_norm': 1.4737944602966309, 'learning_rate': 1.347305389221557e-05, 'epoch': 2.192282102461743}\n",
            "Captured loss: 2.4456 at step 3295\n",
            "Logs at step 3296: {'loss': 2.5027, 'grad_norm': 1.3008571863174438, 'learning_rate': 1.3461964958970949e-05, 'epoch': 2.1929474384564207}\n",
            "Captured loss: 2.5027 at step 3296\n",
            "Logs at step 3297: {'loss': 2.3121, 'grad_norm': 1.4436687231063843, 'learning_rate': 1.3450876025726325e-05, 'epoch': 2.193612774451098}\n",
            "Captured loss: 2.3121 at step 3297\n",
            "Logs at step 3298: {'loss': 2.1543, 'grad_norm': 1.9205058813095093, 'learning_rate': 1.3439787092481704e-05, 'epoch': 2.194278110445775}\n",
            "Captured loss: 2.1543 at step 3298\n",
            "Logs at step 3299: {'loss': 2.1545, 'grad_norm': 1.4991137981414795, 'learning_rate': 1.3428698159237083e-05, 'epoch': 2.1949434464404525}\n",
            "Captured loss: 2.1545 at step 3299\n",
            "Logs at step 3300: {'loss': 2.5925, 'grad_norm': 1.3471693992614746, 'learning_rate': 1.3417609225992461e-05, 'epoch': 2.1956087824351296}\n",
            "Captured loss: 2.5925 at step 3300\n",
            "Logs at step 3301: {'loss': 2.1871, 'grad_norm': 1.6455987691879272, 'learning_rate': 1.3406520292747837e-05, 'epoch': 2.196274118429807}\n",
            "Captured loss: 2.1871 at step 3301\n",
            "Logs at step 3302: {'loss': 2.6105, 'grad_norm': 1.287707805633545, 'learning_rate': 1.3395431359503216e-05, 'epoch': 2.1969394544244842}\n",
            "Captured loss: 2.6105 at step 3302\n",
            "Logs at step 3303: {'loss': 2.5202, 'grad_norm': 1.366274118423462, 'learning_rate': 1.3384342426258595e-05, 'epoch': 2.197604790419162}\n",
            "Captured loss: 2.5202 at step 3303\n",
            "Logs at step 3304: {'loss': 1.9145, 'grad_norm': 1.6127746105194092, 'learning_rate': 1.3373253493013973e-05, 'epoch': 2.198270126413839}\n",
            "Captured loss: 1.9145 at step 3304\n",
            "Logs at step 3305: {'loss': 2.4088, 'grad_norm': 1.962543249130249, 'learning_rate': 1.336216455976935e-05, 'epoch': 2.1989354624085165}\n",
            "Captured loss: 2.4088 at step 3305\n",
            "Logs at step 3306: {'loss': 2.5185, 'grad_norm': 1.1937930583953857, 'learning_rate': 1.3351075626524728e-05, 'epoch': 2.1996007984031936}\n",
            "Captured loss: 2.5185 at step 3306\n",
            "Logs at step 3307: {'loss': 2.5712, 'grad_norm': 1.3753221035003662, 'learning_rate': 1.3339986693280107e-05, 'epoch': 2.200266134397871}\n",
            "Captured loss: 2.5712 at step 3307\n",
            "Logs at step 3308: {'loss': 2.42, 'grad_norm': 1.900730848312378, 'learning_rate': 1.3328897760035486e-05, 'epoch': 2.200931470392548}\n",
            "Captured loss: 2.42 at step 3308\n",
            "Logs at step 3309: {'loss': 2.5161, 'grad_norm': 1.1347328424453735, 'learning_rate': 1.3317808826790864e-05, 'epoch': 2.2015968063872258}\n",
            "Captured loss: 2.5161 at step 3309\n",
            "Logs at step 3310: {'loss': 2.5126, 'grad_norm': 1.1586713790893555, 'learning_rate': 1.330671989354624e-05, 'epoch': 2.202262142381903}\n",
            "Captured loss: 2.5126 at step 3310\n",
            "Logs at step 3311: {'loss': 2.3893, 'grad_norm': 1.4262584447860718, 'learning_rate': 1.3295630960301619e-05, 'epoch': 2.20292747837658}\n",
            "Captured loss: 2.3893 at step 3311\n",
            "Logs at step 3312: {'loss': 2.4806, 'grad_norm': 1.2704353332519531, 'learning_rate': 1.3284542027056998e-05, 'epoch': 2.2035928143712575}\n",
            "Captured loss: 2.4806 at step 3312\n",
            "Logs at step 3313: {'loss': 2.5176, 'grad_norm': 1.4333316087722778, 'learning_rate': 1.3273453093812377e-05, 'epoch': 2.2042581503659346}\n",
            "Captured loss: 2.5176 at step 3313\n",
            "Logs at step 3314: {'loss': 2.4068, 'grad_norm': 1.2697538137435913, 'learning_rate': 1.3262364160567753e-05, 'epoch': 2.204923486360612}\n",
            "Captured loss: 2.4068 at step 3314\n",
            "Logs at step 3315: {'loss': 2.2567, 'grad_norm': 1.823487639427185, 'learning_rate': 1.3251275227323131e-05, 'epoch': 2.2055888223552893}\n",
            "Captured loss: 2.2567 at step 3315\n",
            "Logs at step 3316: {'loss': 2.1145, 'grad_norm': 1.6391844749450684, 'learning_rate': 1.324018629407851e-05, 'epoch': 2.206254158349967}\n",
            "Captured loss: 2.1145 at step 3316\n",
            "Logs at step 3317: {'loss': 2.3371, 'grad_norm': 1.911380410194397, 'learning_rate': 1.322909736083389e-05, 'epoch': 2.206919494344644}\n",
            "Captured loss: 2.3371 at step 3317\n",
            "Logs at step 3318: {'loss': 2.1426, 'grad_norm': 1.739062786102295, 'learning_rate': 1.3218008427589265e-05, 'epoch': 2.2075848303393215}\n",
            "Captured loss: 2.1426 at step 3318\n",
            "Logs at step 3319: {'loss': 2.2482, 'grad_norm': 1.314746379852295, 'learning_rate': 1.3206919494344644e-05, 'epoch': 2.2082501663339986}\n",
            "Captured loss: 2.2482 at step 3319\n",
            "Logs at step 3320: {'loss': 2.5094, 'grad_norm': 1.4351534843444824, 'learning_rate': 1.3195830561100022e-05, 'epoch': 2.208915502328676}\n",
            "Captured loss: 2.5094 at step 3320\n",
            "Logs at step 3321: {'loss': 2.6613, 'grad_norm': 1.5593808889389038, 'learning_rate': 1.3184741627855401e-05, 'epoch': 2.2095808383233533}\n",
            "Captured loss: 2.6613 at step 3321\n",
            "Logs at step 3322: {'loss': 2.4121, 'grad_norm': 1.6127749681472778, 'learning_rate': 1.317365269461078e-05, 'epoch': 2.210246174318031}\n",
            "Captured loss: 2.4121 at step 3322\n",
            "Logs at step 3323: {'loss': 2.308, 'grad_norm': 2.1450600624084473, 'learning_rate': 1.3162563761366156e-05, 'epoch': 2.210911510312708}\n",
            "Captured loss: 2.308 at step 3323\n",
            "Logs at step 3324: {'loss': 2.1743, 'grad_norm': 1.8045752048492432, 'learning_rate': 1.3151474828121536e-05, 'epoch': 2.211576846307385}\n",
            "Captured loss: 2.1743 at step 3324\n",
            "Logs at step 3325: {'loss': 2.5753, 'grad_norm': 1.631496548652649, 'learning_rate': 1.3140385894876913e-05, 'epoch': 2.2122421823020626}\n",
            "Captured loss: 2.5753 at step 3325\n",
            "Logs at step 3326: {'loss': 2.3565, 'grad_norm': 1.3347878456115723, 'learning_rate': 1.3129296961632292e-05, 'epoch': 2.2129075182967397}\n",
            "Captured loss: 2.3565 at step 3326\n",
            "Logs at step 3327: {'loss': 2.0916, 'grad_norm': 1.9905511140823364, 'learning_rate': 1.3118208028387668e-05, 'epoch': 2.2135728542914173}\n",
            "Captured loss: 2.0916 at step 3327\n",
            "Logs at step 3328: {'loss': 2.351, 'grad_norm': 1.3698146343231201, 'learning_rate': 1.3107119095143048e-05, 'epoch': 2.2142381902860944}\n",
            "Captured loss: 2.351 at step 3328\n",
            "Logs at step 3329: {'loss': 2.3958, 'grad_norm': 1.4240788221359253, 'learning_rate': 1.3096030161898427e-05, 'epoch': 2.214903526280772}\n",
            "Captured loss: 2.3958 at step 3329\n",
            "Logs at step 3330: {'loss': 2.2198, 'grad_norm': 1.5517905950546265, 'learning_rate': 1.3084941228653804e-05, 'epoch': 2.215568862275449}\n",
            "Captured loss: 2.2198 at step 3330\n",
            "Logs at step 3331: {'loss': 2.4793, 'grad_norm': 1.6464399099349976, 'learning_rate': 1.3073852295409184e-05, 'epoch': 2.2162341982701266}\n",
            "Captured loss: 2.4793 at step 3331\n",
            "Logs at step 3332: {'loss': 2.2956, 'grad_norm': 1.6986184120178223, 'learning_rate': 1.306276336216456e-05, 'epoch': 2.2168995342648037}\n",
            "Captured loss: 2.2956 at step 3332\n",
            "Logs at step 3333: {'loss': 2.2181, 'grad_norm': 1.69700026512146, 'learning_rate': 1.3051674428919939e-05, 'epoch': 2.2175648702594812}\n",
            "Captured loss: 2.2181 at step 3333\n",
            "Logs at step 3334: {'loss': 2.4086, 'grad_norm': 1.3012831211090088, 'learning_rate': 1.3040585495675318e-05, 'epoch': 2.2182302062541583}\n",
            "Captured loss: 2.4086 at step 3334\n",
            "Logs at step 3335: {'loss': 2.1387, 'grad_norm': 1.669471025466919, 'learning_rate': 1.3029496562430695e-05, 'epoch': 2.218895542248836}\n",
            "Captured loss: 2.1387 at step 3335\n",
            "Logs at step 3336: {'loss': 2.0286, 'grad_norm': 1.750512957572937, 'learning_rate': 1.3018407629186071e-05, 'epoch': 2.219560878243513}\n",
            "Captured loss: 2.0286 at step 3336\n",
            "Logs at step 3337: {'loss': 2.4666, 'grad_norm': 1.8332037925720215, 'learning_rate': 1.300731869594145e-05, 'epoch': 2.22022621423819}\n",
            "Captured loss: 2.4666 at step 3337\n",
            "Logs at step 3338: {'loss': 2.3954, 'grad_norm': 1.251311182975769, 'learning_rate': 1.299622976269683e-05, 'epoch': 2.2208915502328677}\n",
            "Captured loss: 2.3954 at step 3338\n",
            "Logs at step 3339: {'loss': 2.6303, 'grad_norm': 1.9705888032913208, 'learning_rate': 1.2985140829452207e-05, 'epoch': 2.2215568862275448}\n",
            "Captured loss: 2.6303 at step 3339\n",
            "Logs at step 3340: {'loss': 2.6969, 'grad_norm': 2.220377206802368, 'learning_rate': 1.2974051896207585e-05, 'epoch': 2.2222222222222223}\n",
            "Captured loss: 2.6969 at step 3340\n",
            "Logs at step 3341: {'loss': 2.3251, 'grad_norm': 1.603647232055664, 'learning_rate': 1.2962962962962962e-05, 'epoch': 2.2228875582168994}\n",
            "Captured loss: 2.3251 at step 3341\n",
            "Logs at step 3342: {'loss': 2.4866, 'grad_norm': 1.1025735139846802, 'learning_rate': 1.2951874029718342e-05, 'epoch': 2.223552894211577}\n",
            "Captured loss: 2.4866 at step 3342\n",
            "Logs at step 3343: {'loss': 1.9129, 'grad_norm': 1.701983094215393, 'learning_rate': 1.2940785096473721e-05, 'epoch': 2.224218230206254}\n",
            "Captured loss: 1.9129 at step 3343\n",
            "Logs at step 3344: {'loss': 2.1992, 'grad_norm': 1.9567729234695435, 'learning_rate': 1.2929696163229098e-05, 'epoch': 2.2248835662009316}\n",
            "Captured loss: 2.1992 at step 3344\n",
            "Logs at step 3345: {'loss': 2.1275, 'grad_norm': 1.5170884132385254, 'learning_rate': 1.2918607229984474e-05, 'epoch': 2.2255489021956087}\n",
            "Captured loss: 2.1275 at step 3345\n",
            "Logs at step 3346: {'loss': 2.4238, 'grad_norm': 1.5907080173492432, 'learning_rate': 1.2907518296739854e-05, 'epoch': 2.2262142381902863}\n",
            "Captured loss: 2.4238 at step 3346\n",
            "Logs at step 3347: {'loss': 2.4588, 'grad_norm': 1.2020477056503296, 'learning_rate': 1.2896429363495233e-05, 'epoch': 2.2268795741849634}\n",
            "Captured loss: 2.4588 at step 3347\n",
            "Logs at step 3348: {'loss': 2.0888, 'grad_norm': 1.6515262126922607, 'learning_rate': 1.2885340430250612e-05, 'epoch': 2.2275449101796405}\n",
            "Captured loss: 2.0888 at step 3348\n",
            "Logs at step 3349: {'loss': 2.2544, 'grad_norm': 1.540182113647461, 'learning_rate': 1.2874251497005988e-05, 'epoch': 2.228210246174318}\n",
            "Captured loss: 2.2544 at step 3349\n",
            "Logs at step 3350: {'loss': 2.1928, 'grad_norm': 1.305996298789978, 'learning_rate': 1.2863162563761365e-05, 'epoch': 2.228875582168995}\n",
            "Captured loss: 2.1928 at step 3350\n",
            "Logs at step 3351: {'loss': 2.4249, 'grad_norm': 1.3789106607437134, 'learning_rate': 1.2852073630516745e-05, 'epoch': 2.2295409181636727}\n",
            "Captured loss: 2.4249 at step 3351\n",
            "Logs at step 3352: {'loss': 2.2673, 'grad_norm': 1.807441234588623, 'learning_rate': 1.2840984697272124e-05, 'epoch': 2.23020625415835}\n",
            "Captured loss: 2.2673 at step 3352\n",
            "Logs at step 3353: {'loss': 2.4286, 'grad_norm': 1.8019227981567383, 'learning_rate': 1.2829895764027503e-05, 'epoch': 2.2308715901530274}\n",
            "Captured loss: 2.4286 at step 3353\n",
            "Logs at step 3354: {'loss': 2.5115, 'grad_norm': 1.3362168073654175, 'learning_rate': 1.2818806830782879e-05, 'epoch': 2.2315369261477045}\n",
            "Captured loss: 2.5115 at step 3354\n",
            "Logs at step 3355: {'loss': 2.4924, 'grad_norm': 1.0437337160110474, 'learning_rate': 1.2807717897538257e-05, 'epoch': 2.232202262142382}\n",
            "Captured loss: 2.4924 at step 3355\n",
            "Logs at step 3356: {'loss': 2.4617, 'grad_norm': 1.671247959136963, 'learning_rate': 1.2796628964293636e-05, 'epoch': 2.232867598137059}\n",
            "Captured loss: 2.4617 at step 3356\n",
            "Logs at step 3357: {'loss': 2.3981, 'grad_norm': 1.1482956409454346, 'learning_rate': 1.2785540031049015e-05, 'epoch': 2.2335329341317367}\n",
            "Captured loss: 2.3981 at step 3357\n",
            "Logs at step 3358: {'loss': 2.2094, 'grad_norm': 1.6358307600021362, 'learning_rate': 1.2774451097804391e-05, 'epoch': 2.234198270126414}\n",
            "Captured loss: 2.2094 at step 3358\n",
            "Logs at step 3359: {'loss': 2.2052, 'grad_norm': 1.8445180654525757, 'learning_rate': 1.276336216455977e-05, 'epoch': 2.234863606121091}\n",
            "Captured loss: 2.2052 at step 3359\n",
            "Logs at step 3360: {'loss': 2.5913, 'grad_norm': 1.3165310621261597, 'learning_rate': 1.2752273231315148e-05, 'epoch': 2.2355289421157685}\n",
            "Captured loss: 2.5913 at step 3360\n",
            "Logs at step 3361: {'loss': 2.0533, 'grad_norm': 1.6857722997665405, 'learning_rate': 1.2741184298070527e-05, 'epoch': 2.2361942781104456}\n",
            "Captured loss: 2.0533 at step 3361\n",
            "Logs at step 3362: {'loss': 2.4439, 'grad_norm': 1.5004620552062988, 'learning_rate': 1.2730095364825903e-05, 'epoch': 2.236859614105123}\n",
            "Captured loss: 2.4439 at step 3362\n",
            "Logs at step 3363: {'loss': 2.4753, 'grad_norm': 1.1454641819000244, 'learning_rate': 1.2719006431581282e-05, 'epoch': 2.2375249500998002}\n",
            "Captured loss: 2.4753 at step 3363\n",
            "Logs at step 3364: {'loss': 2.1392, 'grad_norm': 1.9120787382125854, 'learning_rate': 1.2707917498336661e-05, 'epoch': 2.238190286094478}\n",
            "Captured loss: 2.1392 at step 3364\n",
            "Logs at step 3365: {'loss': 2.1263, 'grad_norm': 1.5216275453567505, 'learning_rate': 1.2696828565092039e-05, 'epoch': 2.238855622089155}\n",
            "Captured loss: 2.1263 at step 3365\n",
            "Logs at step 3366: {'loss': 2.0278, 'grad_norm': 1.844051480293274, 'learning_rate': 1.2685739631847418e-05, 'epoch': 2.2395209580838324}\n",
            "Captured loss: 2.0278 at step 3366\n",
            "Logs at step 3367: {'loss': 2.1573, 'grad_norm': 1.4806630611419678, 'learning_rate': 1.2674650698602794e-05, 'epoch': 2.2401862940785096}\n",
            "Captured loss: 2.1573 at step 3367\n",
            "Logs at step 3368: {'loss': 2.1298, 'grad_norm': 1.9737842082977295, 'learning_rate': 1.2663561765358173e-05, 'epoch': 2.240851630073187}\n",
            "Captured loss: 2.1298 at step 3368\n",
            "Logs at step 3369: {'loss': 2.5821, 'grad_norm': 1.2523517608642578, 'learning_rate': 1.265247283211355e-05, 'epoch': 2.241516966067864}\n",
            "Captured loss: 2.5821 at step 3369\n",
            "Logs at step 3370: {'loss': 2.4025, 'grad_norm': 1.5593347549438477, 'learning_rate': 1.264138389886893e-05, 'epoch': 2.2421823020625418}\n",
            "Captured loss: 2.4025 at step 3370\n",
            "Logs at step 3371: {'loss': 2.4367, 'grad_norm': 1.4838217496871948, 'learning_rate': 1.2630294965624306e-05, 'epoch': 2.242847638057219}\n",
            "Captured loss: 2.4367 at step 3371\n",
            "Logs at step 3372: {'loss': 2.4774, 'grad_norm': 1.744604229927063, 'learning_rate': 1.2619206032379685e-05, 'epoch': 2.243512974051896}\n",
            "Captured loss: 2.4774 at step 3372\n",
            "Logs at step 3373: {'loss': 2.4063, 'grad_norm': 1.7989414930343628, 'learning_rate': 1.2608117099135064e-05, 'epoch': 2.2441783100465735}\n",
            "Captured loss: 2.4063 at step 3373\n",
            "Logs at step 3374: {'loss': 2.2891, 'grad_norm': 1.5277221202850342, 'learning_rate': 1.2597028165890442e-05, 'epoch': 2.2448436460412506}\n",
            "Captured loss: 2.2891 at step 3374\n",
            "Logs at step 3375: {'loss': 2.1956, 'grad_norm': 1.710234522819519, 'learning_rate': 1.2585939232645821e-05, 'epoch': 2.245508982035928}\n",
            "Captured loss: 2.1956 at step 3375\n",
            "Logs at step 3376: {'loss': 2.4439, 'grad_norm': 2.3201723098754883, 'learning_rate': 1.2574850299401197e-05, 'epoch': 2.2461743180306053}\n",
            "Captured loss: 2.4439 at step 3376\n",
            "Logs at step 3377: {'loss': 2.3626, 'grad_norm': 2.685373544692993, 'learning_rate': 1.2563761366156576e-05, 'epoch': 2.246839654025283}\n",
            "Captured loss: 2.3626 at step 3377\n",
            "Logs at step 3378: {'loss': 2.645, 'grad_norm': 1.6805046796798706, 'learning_rate': 1.2552672432911956e-05, 'epoch': 2.24750499001996}\n",
            "Captured loss: 2.645 at step 3378\n",
            "Logs at step 3379: {'loss': 2.4703, 'grad_norm': 1.2217079401016235, 'learning_rate': 1.2541583499667333e-05, 'epoch': 2.2481703260146375}\n",
            "Captured loss: 2.4703 at step 3379\n",
            "Logs at step 3380: {'loss': 2.5208, 'grad_norm': 1.6782561540603638, 'learning_rate': 1.2530494566422709e-05, 'epoch': 2.2488356620093146}\n",
            "Captured loss: 2.5208 at step 3380\n",
            "Logs at step 3381: {'loss': 2.2431, 'grad_norm': 1.3916860818862915, 'learning_rate': 1.2519405633178088e-05, 'epoch': 2.249500998003992}\n",
            "Captured loss: 2.2431 at step 3381\n",
            "Logs at step 3382: {'loss': 2.3681, 'grad_norm': 1.5101149082183838, 'learning_rate': 1.2508316699933467e-05, 'epoch': 2.2501663339986693}\n",
            "Captured loss: 2.3681 at step 3382\n",
            "Logs at step 3383: {'loss': 2.5239, 'grad_norm': 1.402740478515625, 'learning_rate': 1.2497227766688845e-05, 'epoch': 2.250831669993347}\n",
            "Captured loss: 2.5239 at step 3383\n",
            "Logs at step 3384: {'loss': 2.3478, 'grad_norm': 1.774101734161377, 'learning_rate': 1.2486138833444223e-05, 'epoch': 2.251497005988024}\n",
            "Captured loss: 2.3478 at step 3384\n",
            "Logs at step 3385: {'loss': 2.4796, 'grad_norm': 1.7859668731689453, 'learning_rate': 1.24750499001996e-05, 'epoch': 2.252162341982701}\n",
            "Captured loss: 2.4796 at step 3385\n",
            "Logs at step 3386: {'loss': 2.0958, 'grad_norm': 1.4903137683868408, 'learning_rate': 1.246396096695498e-05, 'epoch': 2.2528276779773786}\n",
            "Captured loss: 2.0958 at step 3386\n",
            "Logs at step 3387: {'loss': 2.4451, 'grad_norm': 1.7500079870224, 'learning_rate': 1.2452872033710359e-05, 'epoch': 2.2534930139720557}\n",
            "Captured loss: 2.4451 at step 3387\n",
            "Logs at step 3388: {'loss': 2.3651, 'grad_norm': 2.845900058746338, 'learning_rate': 1.2441783100465736e-05, 'epoch': 2.2541583499667333}\n",
            "Captured loss: 2.3651 at step 3388\n",
            "Logs at step 3389: {'loss': 2.5026, 'grad_norm': 1.6340384483337402, 'learning_rate': 1.2430694167221114e-05, 'epoch': 2.2548236859614104}\n",
            "Captured loss: 2.5026 at step 3389\n",
            "Logs at step 3390: {'loss': 2.5095, 'grad_norm': 1.4525604248046875, 'learning_rate': 1.2419605233976491e-05, 'epoch': 2.255489021956088}\n",
            "Captured loss: 2.5095 at step 3390\n",
            "Logs at step 3391: {'loss': 2.0664, 'grad_norm': 1.69024658203125, 'learning_rate': 1.240851630073187e-05, 'epoch': 2.256154357950765}\n",
            "Captured loss: 2.0664 at step 3391\n",
            "Logs at step 3392: {'loss': 2.5125, 'grad_norm': 1.2758488655090332, 'learning_rate': 1.2397427367487248e-05, 'epoch': 2.2568196939454426}\n",
            "Captured loss: 2.5125 at step 3392\n",
            "Logs at step 3393: {'loss': 2.2061, 'grad_norm': 1.4184056520462036, 'learning_rate': 1.2386338434242627e-05, 'epoch': 2.2574850299401197}\n",
            "Captured loss: 2.2061 at step 3393\n",
            "Logs at step 3394: {'loss': 1.9419, 'grad_norm': 1.6680903434753418, 'learning_rate': 1.2375249500998005e-05, 'epoch': 2.2581503659347972}\n",
            "Captured loss: 1.9419 at step 3394\n",
            "Logs at step 3395: {'loss': 2.0308, 'grad_norm': 2.059217929840088, 'learning_rate': 1.2364160567753382e-05, 'epoch': 2.2588157019294743}\n",
            "Captured loss: 2.0308 at step 3395\n",
            "Logs at step 3396: {'loss': 2.6018, 'grad_norm': 1.5223585367202759, 'learning_rate': 1.235307163450876e-05, 'epoch': 2.259481037924152}\n",
            "Captured loss: 2.6018 at step 3396\n",
            "Logs at step 3397: {'loss': 2.5493, 'grad_norm': 1.3350094556808472, 'learning_rate': 1.234198270126414e-05, 'epoch': 2.260146373918829}\n",
            "Captured loss: 2.5493 at step 3397\n",
            "Logs at step 3398: {'loss': 2.1719, 'grad_norm': 1.2719781398773193, 'learning_rate': 1.2330893768019518e-05, 'epoch': 2.260811709913506}\n",
            "Captured loss: 2.1719 at step 3398\n",
            "Logs at step 3399: {'loss': 2.3824, 'grad_norm': 1.5828453302383423, 'learning_rate': 1.2319804834774894e-05, 'epoch': 2.2614770459081837}\n",
            "Captured loss: 2.3824 at step 3399\n",
            "Logs at step 3400: {'loss': 2.5627, 'grad_norm': 1.4322673082351685, 'learning_rate': 1.2308715901530274e-05, 'epoch': 2.2621423819028608}\n",
            "Captured loss: 2.5627 at step 3400\n",
            "Logs at step 3401: {'loss': 2.1003, 'grad_norm': 1.8234492540359497, 'learning_rate': 1.2297626968285651e-05, 'epoch': 2.2628077178975383}\n",
            "Captured loss: 2.1003 at step 3401\n",
            "Logs at step 3402: {'loss': 2.2762, 'grad_norm': 2.7403976917266846, 'learning_rate': 1.228653803504103e-05, 'epoch': 2.2634730538922154}\n",
            "Captured loss: 2.2762 at step 3402\n",
            "Logs at step 3403: {'loss': 2.3812, 'grad_norm': 1.855555534362793, 'learning_rate': 1.2275449101796408e-05, 'epoch': 2.264138389886893}\n",
            "Captured loss: 2.3812 at step 3403\n",
            "Logs at step 3404: {'loss': 2.3052, 'grad_norm': 1.4491350650787354, 'learning_rate': 1.2264360168551785e-05, 'epoch': 2.26480372588157}\n",
            "Captured loss: 2.3052 at step 3404\n",
            "Logs at step 3405: {'loss': 2.3804, 'grad_norm': 1.9086757898330688, 'learning_rate': 1.2253271235307163e-05, 'epoch': 2.2654690618762476}\n",
            "Captured loss: 2.3804 at step 3405\n",
            "Logs at step 3406: {'loss': 2.5096, 'grad_norm': 1.1200828552246094, 'learning_rate': 1.2242182302062542e-05, 'epoch': 2.2661343978709247}\n",
            "Captured loss: 2.5096 at step 3406\n",
            "Logs at step 3407: {'loss': 1.8762, 'grad_norm': 2.4036476612091064, 'learning_rate': 1.223109336881792e-05, 'epoch': 2.2667997338656023}\n",
            "Captured loss: 1.8762 at step 3407\n",
            "Logs at step 3408: {'loss': 2.5438, 'grad_norm': 1.123430609703064, 'learning_rate': 1.2220004435573299e-05, 'epoch': 2.2674650698602794}\n",
            "Captured loss: 2.5438 at step 3408\n",
            "Logs at step 3409: {'loss': 2.4367, 'grad_norm': 1.0971699953079224, 'learning_rate': 1.2208915502328677e-05, 'epoch': 2.268130405854957}\n",
            "Captured loss: 2.4367 at step 3409\n",
            "Logs at step 3410: {'loss': 2.1029, 'grad_norm': 2.9354147911071777, 'learning_rate': 1.2197826569084054e-05, 'epoch': 2.268795741849634}\n",
            "Captured loss: 2.1029 at step 3410\n",
            "Logs at step 3411: {'loss': 2.554, 'grad_norm': 1.7390568256378174, 'learning_rate': 1.2186737635839433e-05, 'epoch': 2.269461077844311}\n",
            "Captured loss: 2.554 at step 3411\n",
            "Logs at step 3412: {'loss': 2.1415, 'grad_norm': 1.4657257795333862, 'learning_rate': 1.2175648702594811e-05, 'epoch': 2.2701264138389887}\n",
            "Captured loss: 2.1415 at step 3412\n",
            "Logs at step 3413: {'loss': 2.1796, 'grad_norm': 1.4974511861801147, 'learning_rate': 1.216455976935019e-05, 'epoch': 2.270791749833666}\n",
            "Captured loss: 2.1796 at step 3413\n",
            "Logs at step 3414: {'loss': 2.6386, 'grad_norm': 1.5608187913894653, 'learning_rate': 1.2153470836105568e-05, 'epoch': 2.2714570858283434}\n",
            "Captured loss: 2.6386 at step 3414\n",
            "Logs at step 3415: {'loss': 2.3557, 'grad_norm': 1.562281847000122, 'learning_rate': 1.2142381902860945e-05, 'epoch': 2.2721224218230205}\n",
            "Captured loss: 2.3557 at step 3415\n",
            "Logs at step 3416: {'loss': 1.9331, 'grad_norm': 1.660840392112732, 'learning_rate': 1.2131292969616323e-05, 'epoch': 2.272787757817698}\n",
            "Captured loss: 1.9331 at step 3416\n",
            "Logs at step 3417: {'loss': 2.6461, 'grad_norm': 2.085890769958496, 'learning_rate': 1.2120204036371702e-05, 'epoch': 2.273453093812375}\n",
            "Captured loss: 2.6461 at step 3417\n",
            "Logs at step 3418: {'loss': 2.3867, 'grad_norm': 1.53223717212677, 'learning_rate': 1.210911510312708e-05, 'epoch': 2.2741184298070527}\n",
            "Captured loss: 2.3867 at step 3418\n",
            "Logs at step 3419: {'loss': 2.5402, 'grad_norm': 1.6033588647842407, 'learning_rate': 1.2098026169882457e-05, 'epoch': 2.27478376580173}\n",
            "Captured loss: 2.5402 at step 3419\n",
            "Logs at step 3420: {'loss': 2.5558, 'grad_norm': 1.5982232093811035, 'learning_rate': 1.2086937236637836e-05, 'epoch': 2.2754491017964074}\n",
            "Captured loss: 2.5558 at step 3420\n",
            "Logs at step 3421: {'loss': 2.0673, 'grad_norm': 1.92898690700531, 'learning_rate': 1.2075848303393214e-05, 'epoch': 2.2761144377910845}\n",
            "Captured loss: 2.0673 at step 3421\n",
            "Logs at step 3422: {'loss': 2.5226, 'grad_norm': 1.726165533065796, 'learning_rate': 1.2064759370148593e-05, 'epoch': 2.276779773785762}\n",
            "Captured loss: 2.5226 at step 3422\n",
            "Logs at step 3423: {'loss': 2.1183, 'grad_norm': 2.0489344596862793, 'learning_rate': 1.205367043690397e-05, 'epoch': 2.277445109780439}\n",
            "Captured loss: 2.1183 at step 3423\n",
            "Logs at step 3424: {'loss': 2.1073, 'grad_norm': 1.622886061668396, 'learning_rate': 1.2042581503659348e-05, 'epoch': 2.2781104457751162}\n",
            "Captured loss: 2.1073 at step 3424\n",
            "Logs at step 3425: {'loss': 2.5837, 'grad_norm': 1.3459159135818481, 'learning_rate': 1.2031492570414726e-05, 'epoch': 2.278775781769794}\n",
            "Captured loss: 2.5837 at step 3425\n",
            "Logs at step 3426: {'loss': 2.1263, 'grad_norm': 1.5968035459518433, 'learning_rate': 1.2020403637170105e-05, 'epoch': 2.279441117764471}\n",
            "Captured loss: 2.1263 at step 3426\n",
            "Logs at step 3427: {'loss': 2.4453, 'grad_norm': 1.0436720848083496, 'learning_rate': 1.2009314703925483e-05, 'epoch': 2.2801064537591484}\n",
            "Captured loss: 2.4453 at step 3427\n",
            "Logs at step 3428: {'loss': 2.4199, 'grad_norm': 2.8980014324188232, 'learning_rate': 1.1998225770680862e-05, 'epoch': 2.2807717897538256}\n",
            "Captured loss: 2.4199 at step 3428\n",
            "Logs at step 3429: {'loss': 2.5743, 'grad_norm': 1.548796534538269, 'learning_rate': 1.198713683743624e-05, 'epoch': 2.281437125748503}\n",
            "Captured loss: 2.5743 at step 3429\n",
            "Logs at step 3430: {'loss': 2.1537, 'grad_norm': 2.3897862434387207, 'learning_rate': 1.1976047904191617e-05, 'epoch': 2.28210246174318}\n",
            "Captured loss: 2.1537 at step 3430\n",
            "Logs at step 3431: {'loss': 2.3748, 'grad_norm': 3.081873655319214, 'learning_rate': 1.1964958970946996e-05, 'epoch': 2.2827677977378578}\n",
            "Captured loss: 2.3748 at step 3431\n",
            "Logs at step 3432: {'loss': 2.0607, 'grad_norm': 1.8065705299377441, 'learning_rate': 1.1953870037702374e-05, 'epoch': 2.283433133732535}\n",
            "Captured loss: 2.0607 at step 3432\n",
            "Logs at step 3433: {'loss': 2.5784, 'grad_norm': 1.3819971084594727, 'learning_rate': 1.1942781104457753e-05, 'epoch': 2.2840984697272124}\n",
            "Captured loss: 2.5784 at step 3433\n",
            "Logs at step 3434: {'loss': 2.5617, 'grad_norm': 3.0795648097991943, 'learning_rate': 1.1931692171213129e-05, 'epoch': 2.2847638057218895}\n",
            "Captured loss: 2.5617 at step 3434\n",
            "Logs at step 3435: {'loss': 2.528, 'grad_norm': 3.1670291423797607, 'learning_rate': 1.1920603237968508e-05, 'epoch': 2.285429141716567}\n",
            "Captured loss: 2.528 at step 3435\n",
            "Logs at step 3436: {'loss': 2.2013, 'grad_norm': 1.5002467632293701, 'learning_rate': 1.1909514304723886e-05, 'epoch': 2.286094477711244}\n",
            "Captured loss: 2.2013 at step 3436\n",
            "Logs at step 3437: {'loss': 2.2285, 'grad_norm': 1.6528887748718262, 'learning_rate': 1.1898425371479265e-05, 'epoch': 2.2867598137059213}\n",
            "Captured loss: 2.2285 at step 3437\n",
            "Logs at step 3438: {'loss': 2.5657, 'grad_norm': 1.6563528776168823, 'learning_rate': 1.1887336438234642e-05, 'epoch': 2.287425149700599}\n",
            "Captured loss: 2.5657 at step 3438\n",
            "Logs at step 3439: {'loss': 1.8543, 'grad_norm': 2.132641315460205, 'learning_rate': 1.187624750499002e-05, 'epoch': 2.288090485695276}\n",
            "Captured loss: 1.8543 at step 3439\n",
            "Logs at step 3440: {'loss': 2.4713, 'grad_norm': 1.8126766681671143, 'learning_rate': 1.1865158571745398e-05, 'epoch': 2.2887558216899535}\n",
            "Captured loss: 2.4713 at step 3440\n",
            "Logs at step 3441: {'loss': 2.5106, 'grad_norm': 1.902185320854187, 'learning_rate': 1.1854069638500777e-05, 'epoch': 2.2894211576846306}\n",
            "Captured loss: 2.5106 at step 3441\n",
            "Logs at step 3442: {'loss': 2.1971, 'grad_norm': 1.4897247552871704, 'learning_rate': 1.1842980705256156e-05, 'epoch': 2.290086493679308}\n",
            "Captured loss: 2.1971 at step 3442\n",
            "Logs at step 3443: {'loss': 2.3991, 'grad_norm': 1.735910177230835, 'learning_rate': 1.1831891772011534e-05, 'epoch': 2.2907518296739853}\n",
            "Captured loss: 2.3991 at step 3443\n",
            "Logs at step 3444: {'loss': 2.402, 'grad_norm': 1.461706280708313, 'learning_rate': 1.1820802838766911e-05, 'epoch': 2.291417165668663}\n",
            "Captured loss: 2.402 at step 3444\n",
            "Logs at step 3445: {'loss': 2.5233, 'grad_norm': 1.2328112125396729, 'learning_rate': 1.1809713905522289e-05, 'epoch': 2.29208250166334}\n",
            "Captured loss: 2.5233 at step 3445\n",
            "Logs at step 3446: {'loss': 2.3282, 'grad_norm': 3.3139472007751465, 'learning_rate': 1.1798624972277668e-05, 'epoch': 2.2927478376580175}\n",
            "Captured loss: 2.3282 at step 3446\n",
            "Logs at step 3447: {'loss': 2.5794, 'grad_norm': 1.3197085857391357, 'learning_rate': 1.1787536039033045e-05, 'epoch': 2.2934131736526946}\n",
            "Captured loss: 2.5794 at step 3447\n",
            "Logs at step 3448: {'loss': 2.3667, 'grad_norm': 1.3416367769241333, 'learning_rate': 1.1776447105788425e-05, 'epoch': 2.294078509647372}\n",
            "Captured loss: 2.3667 at step 3448\n",
            "Logs at step 3449: {'loss': 2.3771, 'grad_norm': 1.3087058067321777, 'learning_rate': 1.17653581725438e-05, 'epoch': 2.2947438456420493}\n",
            "Captured loss: 2.3771 at step 3449\n",
            "Logs at step 3450: {'loss': 2.472, 'grad_norm': 1.300865888595581, 'learning_rate': 1.175426923929918e-05, 'epoch': 2.2954091816367264}\n",
            "Captured loss: 2.472 at step 3450\n",
            "Logs at step 3451: {'loss': 2.5057, 'grad_norm': 1.2354600429534912, 'learning_rate': 1.1743180306054557e-05, 'epoch': 2.296074517631404}\n",
            "Captured loss: 2.5057 at step 3451\n",
            "Logs at step 3452: {'loss': 2.3943, 'grad_norm': 1.4512993097305298, 'learning_rate': 1.1732091372809937e-05, 'epoch': 2.296739853626081}\n",
            "Captured loss: 2.3943 at step 3452\n",
            "Logs at step 3453: {'loss': 2.2619, 'grad_norm': 1.569631576538086, 'learning_rate': 1.1721002439565314e-05, 'epoch': 2.2974051896207586}\n",
            "Captured loss: 2.2619 at step 3453\n",
            "Logs at step 3454: {'loss': 2.6095, 'grad_norm': 1.4136905670166016, 'learning_rate': 1.1709913506320692e-05, 'epoch': 2.2980705256154357}\n",
            "Captured loss: 2.6095 at step 3454\n",
            "Logs at step 3455: {'loss': 2.4315, 'grad_norm': 0.9569618105888367, 'learning_rate': 1.1698824573076071e-05, 'epoch': 2.2987358616101132}\n",
            "Captured loss: 2.4315 at step 3455\n",
            "Logs at step 3456: {'loss': 2.4584, 'grad_norm': 1.3125168085098267, 'learning_rate': 1.1687735639831449e-05, 'epoch': 2.2994011976047903}\n",
            "Captured loss: 2.4584 at step 3456\n",
            "Logs at step 3457: {'loss': 2.4441, 'grad_norm': 1.182866096496582, 'learning_rate': 1.1676646706586828e-05, 'epoch': 2.300066533599468}\n",
            "Captured loss: 2.4441 at step 3457\n",
            "Logs at step 3458: {'loss': 2.551, 'grad_norm': 1.2264080047607422, 'learning_rate': 1.1665557773342205e-05, 'epoch': 2.300731869594145}\n",
            "Captured loss: 2.551 at step 3458\n",
            "Logs at step 3459: {'loss': 2.4503, 'grad_norm': 1.290997862815857, 'learning_rate': 1.1654468840097583e-05, 'epoch': 2.3013972055888225}\n",
            "Captured loss: 2.4503 at step 3459\n",
            "Logs at step 3460: {'loss': 2.4437, 'grad_norm': 1.3969218730926514, 'learning_rate': 1.164337990685296e-05, 'epoch': 2.3020625415834997}\n",
            "Captured loss: 2.4437 at step 3460\n",
            "Logs at step 3461: {'loss': 2.4767, 'grad_norm': 1.1826128959655762, 'learning_rate': 1.163229097360834e-05, 'epoch': 2.302727877578177}\n",
            "Captured loss: 2.4767 at step 3461\n",
            "Logs at step 3462: {'loss': 2.5892, 'grad_norm': 1.5015782117843628, 'learning_rate': 1.1621202040363717e-05, 'epoch': 2.3033932135728543}\n",
            "Captured loss: 2.5892 at step 3462\n",
            "Logs at step 3463: {'loss': 2.1395, 'grad_norm': 2.1648595333099365, 'learning_rate': 1.1610113107119096e-05, 'epoch': 2.3040585495675314}\n",
            "Captured loss: 2.1395 at step 3463\n",
            "Logs at step 3464: {'loss': 2.051, 'grad_norm': 1.6123793125152588, 'learning_rate': 1.1599024173874472e-05, 'epoch': 2.304723885562209}\n",
            "Captured loss: 2.051 at step 3464\n",
            "Logs at step 3465: {'loss': 2.5003, 'grad_norm': 1.790054202079773, 'learning_rate': 1.1587935240629852e-05, 'epoch': 2.305389221556886}\n",
            "Captured loss: 2.5003 at step 3465\n",
            "Logs at step 3466: {'loss': 2.1714, 'grad_norm': 2.0268118381500244, 'learning_rate': 1.157684630738523e-05, 'epoch': 2.3060545575515636}\n",
            "Captured loss: 2.1714 at step 3466\n",
            "Logs at step 3467: {'loss': 2.6838, 'grad_norm': 1.627678632736206, 'learning_rate': 1.1565757374140608e-05, 'epoch': 2.3067198935462407}\n",
            "Captured loss: 2.6838 at step 3467\n",
            "Logs at step 3468: {'loss': 2.3949, 'grad_norm': 1.660856008529663, 'learning_rate': 1.1554668440895988e-05, 'epoch': 2.3073852295409183}\n",
            "Captured loss: 2.3949 at step 3468\n",
            "Logs at step 3469: {'loss': 2.4382, 'grad_norm': 1.214530348777771, 'learning_rate': 1.1543579507651363e-05, 'epoch': 2.3080505655355954}\n",
            "Captured loss: 2.4382 at step 3469\n",
            "Logs at step 3470: {'loss': 2.3366, 'grad_norm': 1.561843752861023, 'learning_rate': 1.1532490574406743e-05, 'epoch': 2.308715901530273}\n",
            "Captured loss: 2.3366 at step 3470\n",
            "Logs at step 3471: {'loss': 2.4249, 'grad_norm': 1.515312671661377, 'learning_rate': 1.152140164116212e-05, 'epoch': 2.30938123752495}\n",
            "Captured loss: 2.4249 at step 3471\n",
            "Logs at step 3472: {'loss': 2.2789, 'grad_norm': 1.6096110343933105, 'learning_rate': 1.15103127079175e-05, 'epoch': 2.310046573519627}\n",
            "Captured loss: 2.2789 at step 3472\n",
            "Logs at step 3473: {'loss': 2.3223, 'grad_norm': 1.1566572189331055, 'learning_rate': 1.1499223774672877e-05, 'epoch': 2.3107119095143047}\n",
            "Captured loss: 2.3223 at step 3473\n",
            "Logs at step 3474: {'loss': 2.0465, 'grad_norm': 1.619903802871704, 'learning_rate': 1.1488134841428255e-05, 'epoch': 2.3113772455089823}\n",
            "Captured loss: 2.0465 at step 3474\n",
            "Logs at step 3475: {'loss': 2.5063, 'grad_norm': 1.5689698457717896, 'learning_rate': 1.1477045908183632e-05, 'epoch': 2.3120425815036594}\n",
            "Captured loss: 2.5063 at step 3475\n",
            "Logs at step 3476: {'loss': 2.1253, 'grad_norm': 1.7172276973724365, 'learning_rate': 1.1465956974939011e-05, 'epoch': 2.3127079174983365}\n",
            "Captured loss: 2.1253 at step 3476\n",
            "Logs at step 3477: {'loss': 2.0742, 'grad_norm': 1.8766118288040161, 'learning_rate': 1.145486804169439e-05, 'epoch': 2.313373253493014}\n",
            "Captured loss: 2.0742 at step 3477\n",
            "Logs at step 3478: {'loss': 2.59, 'grad_norm': 1.4111595153808594, 'learning_rate': 1.1443779108449768e-05, 'epoch': 2.314038589487691}\n",
            "Captured loss: 2.59 at step 3478\n",
            "Logs at step 3479: {'loss': 2.5415, 'grad_norm': 1.8254306316375732, 'learning_rate': 1.1432690175205146e-05, 'epoch': 2.3147039254823687}\n",
            "Captured loss: 2.5415 at step 3479\n",
            "Logs at step 3480: {'loss': 2.4667, 'grad_norm': 1.5802110433578491, 'learning_rate': 1.1421601241960523e-05, 'epoch': 2.315369261477046}\n",
            "Captured loss: 2.4667 at step 3480\n",
            "Logs at step 3481: {'loss': 2.1253, 'grad_norm': 1.6607786417007446, 'learning_rate': 1.1410512308715903e-05, 'epoch': 2.3160345974717234}\n",
            "Captured loss: 2.1253 at step 3481\n",
            "Logs at step 3482: {'loss': 2.4688, 'grad_norm': 1.0413079261779785, 'learning_rate': 1.139942337547128e-05, 'epoch': 2.3166999334664005}\n",
            "Captured loss: 2.4688 at step 3482\n",
            "Logs at step 3483: {'loss': 1.9914, 'grad_norm': 2.0658721923828125, 'learning_rate': 1.138833444222666e-05, 'epoch': 2.317365269461078}\n",
            "Captured loss: 1.9914 at step 3483\n",
            "Logs at step 3484: {'loss': 2.0809, 'grad_norm': 1.9244377613067627, 'learning_rate': 1.1377245508982035e-05, 'epoch': 2.318030605455755}\n",
            "Captured loss: 2.0809 at step 3484\n",
            "Logs at step 3485: {'loss': 2.5309, 'grad_norm': 1.6412626504898071, 'learning_rate': 1.1366156575737414e-05, 'epoch': 2.3186959414504322}\n",
            "Captured loss: 2.5309 at step 3485\n",
            "Logs at step 3486: {'loss': 2.1204, 'grad_norm': 1.7774513959884644, 'learning_rate': 1.1355067642492792e-05, 'epoch': 2.31936127744511}\n",
            "Captured loss: 2.1204 at step 3486\n",
            "Logs at step 3487: {'loss': 2.4865, 'grad_norm': 1.2195106744766235, 'learning_rate': 1.1343978709248171e-05, 'epoch': 2.3200266134397873}\n",
            "Captured loss: 2.4865 at step 3487\n",
            "Logs at step 3488: {'loss': 2.2403, 'grad_norm': 1.7713971138000488, 'learning_rate': 1.1332889776003549e-05, 'epoch': 2.3206919494344644}\n",
            "Captured loss: 2.2403 at step 3488\n",
            "Logs at step 3489: {'loss': 1.7992, 'grad_norm': 2.4710540771484375, 'learning_rate': 1.1321800842758926e-05, 'epoch': 2.3213572854291415}\n",
            "Captured loss: 1.7992 at step 3489\n",
            "Logs at step 3490: {'loss': 2.5319, 'grad_norm': 1.2040834426879883, 'learning_rate': 1.1310711909514306e-05, 'epoch': 2.322022621423819}\n",
            "Captured loss: 2.5319 at step 3490\n",
            "Logs at step 3491: {'loss': 2.3497, 'grad_norm': 1.9374679327011108, 'learning_rate': 1.1299622976269683e-05, 'epoch': 2.322687957418496}\n",
            "Captured loss: 2.3497 at step 3491\n",
            "Logs at step 3492: {'loss': 2.4456, 'grad_norm': 1.2301671504974365, 'learning_rate': 1.1288534043025062e-05, 'epoch': 2.3233532934131738}\n",
            "Captured loss: 2.4456 at step 3492\n",
            "Logs at step 3493: {'loss': 1.9461, 'grad_norm': 1.4085811376571655, 'learning_rate': 1.127744510978044e-05, 'epoch': 2.324018629407851}\n",
            "Captured loss: 1.9461 at step 3493\n",
            "Logs at step 3494: {'loss': 2.1904, 'grad_norm': 1.4690357446670532, 'learning_rate': 1.1266356176535817e-05, 'epoch': 2.3246839654025284}\n",
            "Captured loss: 2.1904 at step 3494\n",
            "Logs at step 3495: {'loss': 2.0046, 'grad_norm': 1.7339017391204834, 'learning_rate': 1.1255267243291195e-05, 'epoch': 2.3253493013972055}\n",
            "Captured loss: 2.0046 at step 3495\n",
            "Logs at step 3496: {'loss': 1.7484, 'grad_norm': 2.270768642425537, 'learning_rate': 1.1244178310046574e-05, 'epoch': 2.326014637391883}\n",
            "Captured loss: 1.7484 at step 3496\n",
            "Logs at step 3497: {'loss': 2.394, 'grad_norm': 2.0113794803619385, 'learning_rate': 1.1233089376801952e-05, 'epoch': 2.32667997338656}\n",
            "Captured loss: 2.394 at step 3497\n",
            "Logs at step 3498: {'loss': 2.5147, 'grad_norm': 1.6997153759002686, 'learning_rate': 1.1222000443557331e-05, 'epoch': 2.3273453093812373}\n",
            "Captured loss: 2.5147 at step 3498\n",
            "Logs at step 3499: {'loss': 2.0255, 'grad_norm': 1.5710704326629639, 'learning_rate': 1.1210911510312709e-05, 'epoch': 2.328010645375915}\n",
            "Captured loss: 2.0255 at step 3499\n",
            "Logs at step 3500: {'loss': 2.2675, 'grad_norm': 1.937260389328003, 'learning_rate': 1.1199822577068086e-05, 'epoch': 2.3286759813705924}\n",
            "Captured loss: 2.2675 at step 3500\n",
            "Logs at step 3501: {'loss': 1.7373, 'grad_norm': 2.0873000621795654, 'learning_rate': 1.1188733643823465e-05, 'epoch': 2.3293413173652695}\n",
            "Captured loss: 1.7373 at step 3501\n",
            "Logs at step 3502: {'loss': 2.3099, 'grad_norm': 1.4934204816818237, 'learning_rate': 1.1177644710578843e-05, 'epoch': 2.3300066533599466}\n",
            "Captured loss: 2.3099 at step 3502\n",
            "Logs at step 3503: {'loss': 2.353, 'grad_norm': 2.3898043632507324, 'learning_rate': 1.1166555777334222e-05, 'epoch': 2.330671989354624}\n",
            "Captured loss: 2.353 at step 3503\n",
            "Logs at step 3504: {'loss': 1.7722, 'grad_norm': 2.4412195682525635, 'learning_rate': 1.1155466844089598e-05, 'epoch': 2.3313373253493013}\n",
            "Captured loss: 1.7722 at step 3504\n",
            "Logs at step 3505: {'loss': 2.037, 'grad_norm': 1.4943995475769043, 'learning_rate': 1.1144377910844977e-05, 'epoch': 2.332002661343979}\n",
            "Captured loss: 2.037 at step 3505\n",
            "Logs at step 3506: {'loss': 2.3923, 'grad_norm': 1.6824408769607544, 'learning_rate': 1.1133288977600355e-05, 'epoch': 2.332667997338656}\n",
            "Captured loss: 2.3923 at step 3506\n",
            "Logs at step 3507: {'loss': 2.5031, 'grad_norm': 1.5440086126327515, 'learning_rate': 1.1122200044355734e-05, 'epoch': 2.3333333333333335}\n",
            "Captured loss: 2.5031 at step 3507\n",
            "Logs at step 3508: {'loss': 2.5034, 'grad_norm': 1.769616723060608, 'learning_rate': 1.1111111111111112e-05, 'epoch': 2.3339986693280106}\n",
            "Captured loss: 2.5034 at step 3508\n",
            "Logs at step 3509: {'loss': 2.0437, 'grad_norm': 1.5756022930145264, 'learning_rate': 1.110002217786649e-05, 'epoch': 2.334664005322688}\n",
            "Captured loss: 2.0437 at step 3509\n",
            "Logs at step 3510: {'loss': 2.4933, 'grad_norm': 1.938799262046814, 'learning_rate': 1.1088933244621868e-05, 'epoch': 2.3353293413173652}\n",
            "Captured loss: 2.4933 at step 3510\n",
            "Logs at step 3511: {'loss': 2.4652, 'grad_norm': 1.2563576698303223, 'learning_rate': 1.1077844311377246e-05, 'epoch': 2.3359946773120424}\n",
            "Captured loss: 2.4652 at step 3511\n",
            "Logs at step 3512: {'loss': 2.5839, 'grad_norm': 1.3339241743087769, 'learning_rate': 1.1066755378132625e-05, 'epoch': 2.33666001330672}\n",
            "Captured loss: 2.5839 at step 3512\n",
            "Logs at step 3513: {'loss': 2.4724, 'grad_norm': 1.1508532762527466, 'learning_rate': 1.1055666444888003e-05, 'epoch': 2.337325349301397}\n",
            "Captured loss: 2.4724 at step 3513\n",
            "Logs at step 3514: {'loss': 1.9033, 'grad_norm': 1.7566843032836914, 'learning_rate': 1.104457751164338e-05, 'epoch': 2.3379906852960746}\n",
            "Captured loss: 1.9033 at step 3514\n",
            "Logs at step 3515: {'loss': 2.3066, 'grad_norm': 1.5030044317245483, 'learning_rate': 1.1033488578398758e-05, 'epoch': 2.3386560212907517}\n",
            "Captured loss: 2.3066 at step 3515\n",
            "Logs at step 3516: {'loss': 2.3238, 'grad_norm': 1.3587977886199951, 'learning_rate': 1.1022399645154137e-05, 'epoch': 2.3393213572854292}\n",
            "Captured loss: 2.3238 at step 3516\n",
            "Logs at step 3517: {'loss': 2.1089, 'grad_norm': 1.8383535146713257, 'learning_rate': 1.1011310711909515e-05, 'epoch': 2.3399866932801063}\n",
            "Captured loss: 2.1089 at step 3517\n",
            "Logs at step 3518: {'loss': 2.4698, 'grad_norm': 1.3109220266342163, 'learning_rate': 1.1000221778664894e-05, 'epoch': 2.340652029274784}\n",
            "Captured loss: 2.4698 at step 3518\n",
            "Logs at step 3519: {'loss': 2.1393, 'grad_norm': 2.004958391189575, 'learning_rate': 1.098913284542027e-05, 'epoch': 2.341317365269461}\n",
            "Captured loss: 2.1393 at step 3519\n",
            "Logs at step 3520: {'loss': 2.3317, 'grad_norm': 1.4587527513504028, 'learning_rate': 1.0978043912175649e-05, 'epoch': 2.3419827012641385}\n",
            "Captured loss: 2.3317 at step 3520\n",
            "Logs at step 3521: {'loss': 2.4651, 'grad_norm': 1.7131396532058716, 'learning_rate': 1.0966954978931028e-05, 'epoch': 2.3426480372588157}\n",
            "Captured loss: 2.4651 at step 3521\n",
            "Logs at step 3522: {'loss': 2.3061, 'grad_norm': 2.205362319946289, 'learning_rate': 1.0955866045686406e-05, 'epoch': 2.343313373253493}\n",
            "Captured loss: 2.3061 at step 3522\n",
            "Logs at step 3523: {'loss': 2.5265, 'grad_norm': 1.630020260810852, 'learning_rate': 1.0944777112441783e-05, 'epoch': 2.3439787092481703}\n",
            "Captured loss: 2.5265 at step 3523\n",
            "Logs at step 3524: {'loss': 2.1266, 'grad_norm': 2.192957639694214, 'learning_rate': 1.0933688179197161e-05, 'epoch': 2.3446440452428474}\n",
            "Captured loss: 2.1266 at step 3524\n",
            "Logs at step 3525: {'loss': 2.5769, 'grad_norm': 1.2819949388504028, 'learning_rate': 1.092259924595254e-05, 'epoch': 2.345309381237525}\n",
            "Captured loss: 2.5769 at step 3525\n",
            "Logs at step 3526: {'loss': 2.5499, 'grad_norm': 1.133176326751709, 'learning_rate': 1.0911510312707918e-05, 'epoch': 2.345974717232202}\n",
            "Captured loss: 2.5499 at step 3526\n",
            "Logs at step 3527: {'loss': 2.1628, 'grad_norm': 2.1178057193756104, 'learning_rate': 1.0900421379463297e-05, 'epoch': 2.3466400532268796}\n",
            "Captured loss: 2.1628 at step 3527\n",
            "Logs at step 3528: {'loss': 1.9923, 'grad_norm': 1.4998507499694824, 'learning_rate': 1.0889332446218675e-05, 'epoch': 2.3473053892215567}\n",
            "Captured loss: 1.9923 at step 3528\n",
            "Logs at step 3529: {'loss': 2.6372, 'grad_norm': 1.9707473516464233, 'learning_rate': 1.0878243512974052e-05, 'epoch': 2.3479707252162343}\n",
            "Captured loss: 2.6372 at step 3529\n",
            "Logs at step 3530: {'loss': 2.2638, 'grad_norm': 1.5278669595718384, 'learning_rate': 1.086715457972943e-05, 'epoch': 2.3486360612109114}\n",
            "Captured loss: 2.2638 at step 3530\n",
            "Logs at step 3531: {'loss': 2.5434, 'grad_norm': 1.2991313934326172, 'learning_rate': 1.0856065646484809e-05, 'epoch': 2.349301397205589}\n",
            "Captured loss: 2.5434 at step 3531\n",
            "Logs at step 3532: {'loss': 2.4559, 'grad_norm': 1.2190302610397339, 'learning_rate': 1.0844976713240188e-05, 'epoch': 2.349966733200266}\n",
            "Captured loss: 2.4559 at step 3532\n",
            "Logs at step 3533: {'loss': 2.6316, 'grad_norm': 1.8942493200302124, 'learning_rate': 1.0833887779995566e-05, 'epoch': 2.3506320691949436}\n",
            "Captured loss: 2.6316 at step 3533\n",
            "Logs at step 3534: {'loss': 2.2827, 'grad_norm': 1.5579402446746826, 'learning_rate': 1.0822798846750943e-05, 'epoch': 2.3512974051896207}\n",
            "Captured loss: 2.2827 at step 3534\n",
            "Logs at step 3535: {'loss': 2.3712, 'grad_norm': 1.1541446447372437, 'learning_rate': 1.081170991350632e-05, 'epoch': 2.3519627411842983}\n",
            "Captured loss: 2.3712 at step 3535\n",
            "Logs at step 3536: {'loss': 2.4627, 'grad_norm': 1.7015177011489868, 'learning_rate': 1.08006209802617e-05, 'epoch': 2.3526280771789754}\n",
            "Captured loss: 2.4627 at step 3536\n",
            "Logs at step 3537: {'loss': 2.4712, 'grad_norm': 1.7631158828735352, 'learning_rate': 1.0789532047017078e-05, 'epoch': 2.3532934131736525}\n",
            "Captured loss: 2.4712 at step 3537\n",
            "Logs at step 3538: {'loss': 2.2981, 'grad_norm': 1.6643561124801636, 'learning_rate': 1.0778443113772455e-05, 'epoch': 2.35395874916833}\n",
            "Captured loss: 2.2981 at step 3538\n",
            "Logs at step 3539: {'loss': 2.5176, 'grad_norm': 1.1467515230178833, 'learning_rate': 1.0767354180527833e-05, 'epoch': 2.354624085163007}\n",
            "Captured loss: 2.5176 at step 3539\n",
            "Logs at step 3540: {'loss': 2.5876, 'grad_norm': 1.8822388648986816, 'learning_rate': 1.0756265247283212e-05, 'epoch': 2.3552894211576847}\n",
            "Captured loss: 2.5876 at step 3540\n",
            "Logs at step 3541: {'loss': 2.5047, 'grad_norm': 1.2727783918380737, 'learning_rate': 1.074517631403859e-05, 'epoch': 2.355954757152362}\n",
            "Captured loss: 2.5047 at step 3541\n",
            "Logs at step 3542: {'loss': 2.2248, 'grad_norm': 1.4415372610092163, 'learning_rate': 1.0734087380793969e-05, 'epoch': 2.3566200931470394}\n",
            "Captured loss: 2.2248 at step 3542\n",
            "Logs at step 3543: {'loss': 2.6926, 'grad_norm': 1.7379170656204224, 'learning_rate': 1.0722998447549346e-05, 'epoch': 2.3572854291417165}\n",
            "Captured loss: 2.6926 at step 3543\n",
            "Logs at step 3544: {'loss': 2.5433, 'grad_norm': 1.3200799226760864, 'learning_rate': 1.0711909514304724e-05, 'epoch': 2.357950765136394}\n",
            "Captured loss: 2.5433 at step 3544\n",
            "Logs at step 3545: {'loss': 2.1709, 'grad_norm': 1.9254120588302612, 'learning_rate': 1.0700820581060103e-05, 'epoch': 2.358616101131071}\n",
            "Captured loss: 2.1709 at step 3545\n",
            "Logs at step 3546: {'loss': 2.0936, 'grad_norm': 1.8221875429153442, 'learning_rate': 1.068973164781548e-05, 'epoch': 2.3592814371257487}\n",
            "Captured loss: 2.0936 at step 3546\n",
            "Logs at step 3547: {'loss': 2.5089, 'grad_norm': 1.8438068628311157, 'learning_rate': 1.067864271457086e-05, 'epoch': 2.359946773120426}\n",
            "Captured loss: 2.5089 at step 3547\n",
            "Logs at step 3548: {'loss': 2.1691, 'grad_norm': 1.677510142326355, 'learning_rate': 1.0667553781326237e-05, 'epoch': 2.3606121091151033}\n",
            "Captured loss: 2.1691 at step 3548\n",
            "Logs at step 3549: {'loss': 2.1562, 'grad_norm': 1.7969754934310913, 'learning_rate': 1.0656464848081615e-05, 'epoch': 2.3612774451097804}\n",
            "Captured loss: 2.1562 at step 3549\n",
            "Logs at step 3550: {'loss': 2.2598, 'grad_norm': 1.9734219312667847, 'learning_rate': 1.0645375914836992e-05, 'epoch': 2.3619427811044575}\n",
            "Captured loss: 2.2598 at step 3550\n",
            "Logs at step 3551: {'loss': 2.3861, 'grad_norm': 1.3387318849563599, 'learning_rate': 1.0634286981592372e-05, 'epoch': 2.362608117099135}\n",
            "Captured loss: 2.3861 at step 3551\n",
            "Logs at step 3552: {'loss': 2.3016, 'grad_norm': 2.096604824066162, 'learning_rate': 1.062319804834775e-05, 'epoch': 2.363273453093812}\n",
            "Captured loss: 2.3016 at step 3552\n",
            "Logs at step 3553: {'loss': 2.4861, 'grad_norm': 1.2935690879821777, 'learning_rate': 1.0612109115103127e-05, 'epoch': 2.3639387890884898}\n",
            "Captured loss: 2.4861 at step 3553\n",
            "Logs at step 3554: {'loss': 2.1525, 'grad_norm': 1.7558259963989258, 'learning_rate': 1.0601020181858504e-05, 'epoch': 2.364604125083167}\n",
            "Captured loss: 2.1525 at step 3554\n",
            "Logs at step 3555: {'loss': 2.1471, 'grad_norm': 1.7392455339431763, 'learning_rate': 1.0589931248613884e-05, 'epoch': 2.3652694610778444}\n",
            "Captured loss: 2.1471 at step 3555\n",
            "Logs at step 3556: {'loss': 2.5537, 'grad_norm': 1.730080008506775, 'learning_rate': 1.0578842315369263e-05, 'epoch': 2.3659347970725215}\n",
            "Captured loss: 2.5537 at step 3556\n",
            "Logs at step 3557: {'loss': 2.5908, 'grad_norm': 1.1447519063949585, 'learning_rate': 1.056775338212464e-05, 'epoch': 2.366600133067199}\n",
            "Captured loss: 2.5908 at step 3557\n",
            "Logs at step 3558: {'loss': 2.5585, 'grad_norm': 1.656315565109253, 'learning_rate': 1.0556664448880018e-05, 'epoch': 2.367265469061876}\n",
            "Captured loss: 2.5585 at step 3558\n",
            "Logs at step 3559: {'loss': 1.9649, 'grad_norm': 1.5610148906707764, 'learning_rate': 1.0545575515635396e-05, 'epoch': 2.3679308050565537}\n",
            "Captured loss: 1.9649 at step 3559\n",
            "Logs at step 3560: {'loss': 2.4487, 'grad_norm': 1.7204524278640747, 'learning_rate': 1.0534486582390775e-05, 'epoch': 2.368596141051231}\n",
            "Captured loss: 2.4487 at step 3560\n",
            "Logs at step 3561: {'loss': 2.6897, 'grad_norm': 1.4479142427444458, 'learning_rate': 1.0523397649146152e-05, 'epoch': 2.3692614770459084}\n",
            "Captured loss: 2.6897 at step 3561\n",
            "Logs at step 3562: {'loss': 2.3502, 'grad_norm': 2.063427686691284, 'learning_rate': 1.0512308715901532e-05, 'epoch': 2.3699268130405855}\n",
            "Captured loss: 2.3502 at step 3562\n",
            "Logs at step 3563: {'loss': 2.5001, 'grad_norm': 1.181868553161621, 'learning_rate': 1.0501219782656909e-05, 'epoch': 2.3705921490352626}\n",
            "Captured loss: 2.5001 at step 3563\n",
            "Logs at step 3564: {'loss': 1.6741, 'grad_norm': 2.1407766342163086, 'learning_rate': 1.0490130849412287e-05, 'epoch': 2.37125748502994}\n",
            "Captured loss: 1.6741 at step 3564\n",
            "Logs at step 3565: {'loss': 2.3164, 'grad_norm': 1.5393171310424805, 'learning_rate': 1.0479041916167664e-05, 'epoch': 2.3719228210246173}\n",
            "Captured loss: 2.3164 at step 3565\n",
            "Logs at step 3566: {'loss': 2.5023, 'grad_norm': 1.168217420578003, 'learning_rate': 1.0467952982923043e-05, 'epoch': 2.372588157019295}\n",
            "Captured loss: 2.5023 at step 3566\n",
            "Logs at step 3567: {'loss': 2.359, 'grad_norm': 2.4782376289367676, 'learning_rate': 1.0456864049678423e-05, 'epoch': 2.373253493013972}\n",
            "Captured loss: 2.359 at step 3567\n",
            "Logs at step 3568: {'loss': 2.0659, 'grad_norm': 1.7500419616699219, 'learning_rate': 1.04457751164338e-05, 'epoch': 2.3739188290086495}\n",
            "Captured loss: 2.0659 at step 3568\n",
            "Logs at step 3569: {'loss': 2.627, 'grad_norm': 1.389107346534729, 'learning_rate': 1.0434686183189178e-05, 'epoch': 2.3745841650033266}\n",
            "Captured loss: 2.627 at step 3569\n",
            "Logs at step 3570: {'loss': 2.5577, 'grad_norm': 2.353149175643921, 'learning_rate': 1.0423597249944555e-05, 'epoch': 2.375249500998004}\n",
            "Captured loss: 2.5577 at step 3570\n",
            "Logs at step 3571: {'loss': 2.5087, 'grad_norm': 1.4322887659072876, 'learning_rate': 1.0412508316699935e-05, 'epoch': 2.3759148369926812}\n",
            "Captured loss: 2.5087 at step 3571\n",
            "Logs at step 3572: {'loss': 2.0954, 'grad_norm': 1.9011675119400024, 'learning_rate': 1.0401419383455312e-05, 'epoch': 2.376580172987359}\n",
            "Captured loss: 2.0954 at step 3572\n",
            "Logs at step 3573: {'loss': 2.5565, 'grad_norm': 1.1986117362976074, 'learning_rate': 1.039033045021069e-05, 'epoch': 2.377245508982036}\n",
            "Captured loss: 2.5565 at step 3573\n",
            "Logs at step 3574: {'loss': 2.4775, 'grad_norm': 1.1478527784347534, 'learning_rate': 1.0379241516966067e-05, 'epoch': 2.3779108449767135}\n",
            "Captured loss: 2.4775 at step 3574\n",
            "Logs at step 3575: {'loss': 2.5725, 'grad_norm': 1.31696617603302, 'learning_rate': 1.0368152583721446e-05, 'epoch': 2.3785761809713906}\n",
            "Captured loss: 2.5725 at step 3575\n",
            "Logs at step 3576: {'loss': 2.3364, 'grad_norm': 1.8803112506866455, 'learning_rate': 1.0357063650476824e-05, 'epoch': 2.3792415169660677}\n",
            "Captured loss: 2.3364 at step 3576\n",
            "Logs at step 3577: {'loss': 2.2656, 'grad_norm': 1.6469939947128296, 'learning_rate': 1.0345974717232203e-05, 'epoch': 2.3799068529607452}\n",
            "Captured loss: 2.2656 at step 3577\n",
            "Logs at step 3578: {'loss': 2.3023, 'grad_norm': 1.8183010816574097, 'learning_rate': 1.033488578398758e-05, 'epoch': 2.3805721889554223}\n",
            "Captured loss: 2.3023 at step 3578\n",
            "Logs at step 3579: {'loss': 2.6, 'grad_norm': 2.034083366394043, 'learning_rate': 1.0323796850742958e-05, 'epoch': 2.3812375249501}\n",
            "Captured loss: 2.6 at step 3579\n",
            "Logs at step 3580: {'loss': 2.386, 'grad_norm': 1.3707386255264282, 'learning_rate': 1.0312707917498338e-05, 'epoch': 2.381902860944777}\n",
            "Captured loss: 2.386 at step 3580\n",
            "Logs at step 3581: {'loss': 2.4287, 'grad_norm': 2.291365623474121, 'learning_rate': 1.0301618984253715e-05, 'epoch': 2.3825681969394545}\n",
            "Captured loss: 2.4287 at step 3581\n",
            "Logs at step 3582: {'loss': 1.9981, 'grad_norm': 1.8356759548187256, 'learning_rate': 1.0290530051009094e-05, 'epoch': 2.3832335329341316}\n",
            "Captured loss: 1.9981 at step 3582\n",
            "Logs at step 3583: {'loss': 2.3627, 'grad_norm': 1.6503186225891113, 'learning_rate': 1.0279441117764472e-05, 'epoch': 2.383898868928809}\n",
            "Captured loss: 2.3627 at step 3583\n",
            "Logs at step 3584: {'loss': 2.6099, 'grad_norm': 2.197089433670044, 'learning_rate': 1.026835218451985e-05, 'epoch': 2.3845642049234863}\n",
            "Captured loss: 2.6099 at step 3584\n",
            "Logs at step 3585: {'loss': 2.3539, 'grad_norm': 2.057926654815674, 'learning_rate': 1.0257263251275227e-05, 'epoch': 2.385229540918164}\n",
            "Captured loss: 2.3539 at step 3585\n",
            "Logs at step 3586: {'loss': 1.9306, 'grad_norm': 2.3636069297790527, 'learning_rate': 1.0246174318030606e-05, 'epoch': 2.385894876912841}\n",
            "Captured loss: 1.9306 at step 3586\n",
            "Logs at step 3587: {'loss': 2.2107, 'grad_norm': 1.6833416223526, 'learning_rate': 1.0235085384785984e-05, 'epoch': 2.3865602129075185}\n",
            "Captured loss: 2.2107 at step 3587\n",
            "Logs at step 3588: {'loss': 2.4975, 'grad_norm': 1.0291199684143066, 'learning_rate': 1.0223996451541361e-05, 'epoch': 2.3872255489021956}\n",
            "Captured loss: 2.4975 at step 3588\n",
            "Logs at step 3589: {'loss': 2.4371, 'grad_norm': 1.176945447921753, 'learning_rate': 1.021290751829674e-05, 'epoch': 2.3878908848968727}\n",
            "Captured loss: 2.4371 at step 3589\n",
            "Logs at step 3590: {'loss': 2.5898, 'grad_norm': 1.1796143054962158, 'learning_rate': 1.0201818585052118e-05, 'epoch': 2.3885562208915503}\n",
            "Captured loss: 2.5898 at step 3590\n",
            "Logs at step 3591: {'loss': 2.7223, 'grad_norm': 1.9905146360397339, 'learning_rate': 1.0190729651807497e-05, 'epoch': 2.3892215568862274}\n",
            "Captured loss: 2.7223 at step 3591\n",
            "Logs at step 3592: {'loss': 2.0621, 'grad_norm': 1.7863010168075562, 'learning_rate': 1.0179640718562875e-05, 'epoch': 2.389886892880905}\n",
            "Captured loss: 2.0621 at step 3592\n",
            "Logs at step 3593: {'loss': 1.9731, 'grad_norm': 1.5025392770767212, 'learning_rate': 1.0168551785318253e-05, 'epoch': 2.390552228875582}\n",
            "Captured loss: 1.9731 at step 3593\n",
            "Logs at step 3594: {'loss': 2.1693, 'grad_norm': 1.8992643356323242, 'learning_rate': 1.015746285207363e-05, 'epoch': 2.3912175648702596}\n",
            "Captured loss: 2.1693 at step 3594\n",
            "Logs at step 3595: {'loss': 2.4861, 'grad_norm': 1.3566539287567139, 'learning_rate': 1.014637391882901e-05, 'epoch': 2.3918829008649367}\n",
            "Captured loss: 2.4861 at step 3595\n",
            "Logs at step 3596: {'loss': 2.3189, 'grad_norm': 1.7954005002975464, 'learning_rate': 1.0135284985584387e-05, 'epoch': 2.3925482368596143}\n",
            "Captured loss: 2.3189 at step 3596\n",
            "Logs at step 3597: {'loss': 1.8856, 'grad_norm': 1.692898154258728, 'learning_rate': 1.0124196052339766e-05, 'epoch': 2.3932135728542914}\n",
            "Captured loss: 1.8856 at step 3597\n",
            "Logs at step 3598: {'loss': 2.406, 'grad_norm': 1.398153305053711, 'learning_rate': 1.0113107119095144e-05, 'epoch': 2.393878908848969}\n",
            "Captured loss: 2.406 at step 3598\n",
            "Logs at step 3599: {'loss': 2.3899, 'grad_norm': 1.112269639968872, 'learning_rate': 1.0102018185850521e-05, 'epoch': 2.394544244843646}\n",
            "Captured loss: 2.3899 at step 3599\n",
            "Logs at step 3600: {'loss': 2.1799, 'grad_norm': 1.8324209451675415, 'learning_rate': 1.00909292526059e-05, 'epoch': 2.3952095808383236}\n",
            "Captured loss: 2.1799 at step 3600\n",
            "Logs at step 3601: {'loss': 2.5197, 'grad_norm': 1.1929124593734741, 'learning_rate': 1.0079840319361278e-05, 'epoch': 2.3958749168330007}\n",
            "Captured loss: 2.5197 at step 3601\n",
            "Logs at step 3602: {'loss': 2.4648, 'grad_norm': 1.2717782258987427, 'learning_rate': 1.0068751386116657e-05, 'epoch': 2.396540252827678}\n",
            "Captured loss: 2.4648 at step 3602\n",
            "Logs at step 3603: {'loss': 2.4204, 'grad_norm': 1.1135331392288208, 'learning_rate': 1.0057662452872033e-05, 'epoch': 2.3972055888223553}\n",
            "Captured loss: 2.4204 at step 3603\n",
            "Logs at step 3604: {'loss': 2.4115, 'grad_norm': 1.0423433780670166, 'learning_rate': 1.0046573519627412e-05, 'epoch': 2.3978709248170325}\n",
            "Captured loss: 2.4115 at step 3604\n",
            "Logs at step 3605: {'loss': 2.3482, 'grad_norm': 1.1730461120605469, 'learning_rate': 1.003548458638279e-05, 'epoch': 2.39853626081171}\n",
            "Captured loss: 2.3482 at step 3605\n",
            "Logs at step 3606: {'loss': 2.0353, 'grad_norm': 1.4343798160552979, 'learning_rate': 1.002439565313817e-05, 'epoch': 2.399201596806387}\n",
            "Captured loss: 2.0353 at step 3606\n",
            "Logs at step 3607: {'loss': 2.5179, 'grad_norm': 1.4800399541854858, 'learning_rate': 1.0013306719893547e-05, 'epoch': 2.3998669328010647}\n",
            "Captured loss: 2.5179 at step 3607\n",
            "Logs at step 3608: {'loss': 2.4619, 'grad_norm': 1.6671693325042725, 'learning_rate': 1.0002217786648924e-05, 'epoch': 2.4005322687957418}\n",
            "Captured loss: 2.4619 at step 3608\n",
            "Logs at step 3609: {'loss': 2.0423, 'grad_norm': 1.6587296724319458, 'learning_rate': 9.991128853404302e-06, 'epoch': 2.4011976047904193}\n",
            "Captured loss: 2.0423 at step 3609\n",
            "Logs at step 3610: {'loss': 2.355, 'grad_norm': 2.120112657546997, 'learning_rate': 9.980039920159681e-06, 'epoch': 2.4018629407850964}\n",
            "Captured loss: 2.355 at step 3610\n",
            "Logs at step 3611: {'loss': 2.0967, 'grad_norm': 1.7093003988265991, 'learning_rate': 9.96895098691506e-06, 'epoch': 2.4025282767797735}\n",
            "Captured loss: 2.0967 at step 3611\n",
            "Logs at step 3612: {'loss': 2.0318, 'grad_norm': 1.5867093801498413, 'learning_rate': 9.957862053670438e-06, 'epoch': 2.403193612774451}\n",
            "Captured loss: 2.0318 at step 3612\n",
            "Logs at step 3613: {'loss': 2.3394, 'grad_norm': 1.2118110656738281, 'learning_rate': 9.946773120425815e-06, 'epoch': 2.4038589487691286}\n",
            "Captured loss: 2.3394 at step 3613\n",
            "Logs at step 3614: {'loss': 2.1932, 'grad_norm': 2.7606682777404785, 'learning_rate': 9.935684187181193e-06, 'epoch': 2.4045242847638058}\n",
            "Captured loss: 2.1932 at step 3614\n",
            "Logs at step 3615: {'loss': 2.5129, 'grad_norm': 1.2107856273651123, 'learning_rate': 9.924595253936572e-06, 'epoch': 2.405189620758483}\n",
            "Captured loss: 2.5129 at step 3615\n",
            "Logs at step 3616: {'loss': 2.2356, 'grad_norm': 1.831861138343811, 'learning_rate': 9.91350632069195e-06, 'epoch': 2.4058549567531604}\n",
            "Captured loss: 2.2356 at step 3616\n",
            "Logs at step 3617: {'loss': 2.0529, 'grad_norm': 1.8811670541763306, 'learning_rate': 9.902417387447329e-06, 'epoch': 2.4065202927478375}\n",
            "Captured loss: 2.0529 at step 3617\n",
            "Logs at step 3618: {'loss': 2.5361, 'grad_norm': 1.3326754570007324, 'learning_rate': 9.891328454202705e-06, 'epoch': 2.407185628742515}\n",
            "Captured loss: 2.5361 at step 3618\n",
            "Logs at step 3619: {'loss': 2.0063, 'grad_norm': 1.4612901210784912, 'learning_rate': 9.880239520958084e-06, 'epoch': 2.407850964737192}\n",
            "Captured loss: 2.0063 at step 3619\n",
            "Logs at step 3620: {'loss': 1.9588, 'grad_norm': 1.5519803762435913, 'learning_rate': 9.869150587713462e-06, 'epoch': 2.4085163007318697}\n",
            "Captured loss: 1.9588 at step 3620\n",
            "Logs at step 3621: {'loss': 2.46, 'grad_norm': 1.4613738059997559, 'learning_rate': 9.858061654468841e-06, 'epoch': 2.409181636726547}\n",
            "Captured loss: 2.46 at step 3621\n",
            "Logs at step 3622: {'loss': 2.4034, 'grad_norm': 1.5487134456634521, 'learning_rate': 9.84697272122422e-06, 'epoch': 2.4098469727212244}\n",
            "Captured loss: 2.4034 at step 3622\n",
            "Logs at step 3623: {'loss': 2.5466, 'grad_norm': 1.213858723640442, 'learning_rate': 9.835883787979596e-06, 'epoch': 2.4105123087159015}\n",
            "Captured loss: 2.5466 at step 3623\n",
            "Logs at step 3624: {'loss': 1.8963, 'grad_norm': 2.1004788875579834, 'learning_rate': 9.824794854734975e-06, 'epoch': 2.4111776447105786}\n",
            "Captured loss: 1.8963 at step 3624\n",
            "Logs at step 3625: {'loss': 2.3192, 'grad_norm': 1.6296497583389282, 'learning_rate': 9.813705921490353e-06, 'epoch': 2.411842980705256}\n",
            "Captured loss: 2.3192 at step 3625\n",
            "Logs at step 3626: {'loss': 2.4631, 'grad_norm': 1.22853684425354, 'learning_rate': 9.802616988245732e-06, 'epoch': 2.4125083166999337}\n",
            "Captured loss: 2.4631 at step 3626\n",
            "Logs at step 3627: {'loss': 2.3171, 'grad_norm': 1.7404533624649048, 'learning_rate': 9.79152805500111e-06, 'epoch': 2.413173652694611}\n",
            "Captured loss: 2.3171 at step 3627\n",
            "Logs at step 3628: {'loss': 1.9324, 'grad_norm': 2.318942070007324, 'learning_rate': 9.780439121756487e-06, 'epoch': 2.413838988689288}\n",
            "Captured loss: 1.9324 at step 3628\n",
            "Logs at step 3629: {'loss': 2.0829, 'grad_norm': 1.7654320001602173, 'learning_rate': 9.769350188511865e-06, 'epoch': 2.4145043246839655}\n",
            "Captured loss: 2.0829 at step 3629\n",
            "Logs at step 3630: {'loss': 2.4696, 'grad_norm': 1.3117396831512451, 'learning_rate': 9.758261255267244e-06, 'epoch': 2.4151696606786426}\n",
            "Captured loss: 2.4696 at step 3630\n",
            "Logs at step 3631: {'loss': 2.4994, 'grad_norm': 1.694933533668518, 'learning_rate': 9.747172322022622e-06, 'epoch': 2.41583499667332}\n",
            "Captured loss: 2.4994 at step 3631\n",
            "Logs at step 3632: {'loss': 2.5223, 'grad_norm': 1.5500094890594482, 'learning_rate': 9.736083388778e-06, 'epoch': 2.4165003326679972}\n",
            "Captured loss: 2.5223 at step 3632\n",
            "Logs at step 3633: {'loss': 2.5279, 'grad_norm': 1.2668230533599854, 'learning_rate': 9.724994455533378e-06, 'epoch': 2.417165668662675}\n",
            "Captured loss: 2.5279 at step 3633\n",
            "Logs at step 3634: {'loss': 2.5221, 'grad_norm': 1.9444053173065186, 'learning_rate': 9.713905522288756e-06, 'epoch': 2.417831004657352}\n",
            "Captured loss: 2.5221 at step 3634\n",
            "Logs at step 3635: {'loss': 2.4535, 'grad_norm': 1.461976170539856, 'learning_rate': 9.702816589044135e-06, 'epoch': 2.4184963406520295}\n",
            "Captured loss: 2.4535 at step 3635\n",
            "Logs at step 3636: {'loss': 2.5094, 'grad_norm': 1.5983175039291382, 'learning_rate': 9.691727655799513e-06, 'epoch': 2.4191616766467066}\n",
            "Captured loss: 2.5094 at step 3636\n",
            "Logs at step 3637: {'loss': 2.4383, 'grad_norm': 2.1711175441741943, 'learning_rate': 9.680638722554892e-06, 'epoch': 2.4198270126413837}\n",
            "Captured loss: 2.4383 at step 3637\n",
            "Logs at step 3638: {'loss': 2.5383, 'grad_norm': 1.1731648445129395, 'learning_rate': 9.669549789310268e-06, 'epoch': 2.420492348636061}\n",
            "Captured loss: 2.5383 at step 3638\n",
            "Logs at step 3639: {'loss': 2.0608, 'grad_norm': 2.054140329360962, 'learning_rate': 9.658460856065647e-06, 'epoch': 2.4211576846307388}\n",
            "Captured loss: 2.0608 at step 3639\n",
            "Logs at step 3640: {'loss': 2.3788, 'grad_norm': 1.0596445798873901, 'learning_rate': 9.647371922821025e-06, 'epoch': 2.421823020625416}\n",
            "Captured loss: 2.3788 at step 3640\n",
            "Logs at step 3641: {'loss': 2.0269, 'grad_norm': 1.8677364587783813, 'learning_rate': 9.636282989576404e-06, 'epoch': 2.422488356620093}\n",
            "Captured loss: 2.0269 at step 3641\n",
            "Logs at step 3642: {'loss': 2.2785, 'grad_norm': 1.3024030923843384, 'learning_rate': 9.625194056331781e-06, 'epoch': 2.4231536926147705}\n",
            "Captured loss: 2.2785 at step 3642\n",
            "Logs at step 3643: {'loss': 2.4316, 'grad_norm': 1.1696772575378418, 'learning_rate': 9.614105123087159e-06, 'epoch': 2.4238190286094476}\n",
            "Captured loss: 2.4316 at step 3643\n",
            "Logs at step 3644: {'loss': 2.533, 'grad_norm': 1.4722750186920166, 'learning_rate': 9.603016189842536e-06, 'epoch': 2.424484364604125}\n",
            "Captured loss: 2.533 at step 3644\n",
            "Logs at step 3645: {'loss': 2.0404, 'grad_norm': 2.0360496044158936, 'learning_rate': 9.591927256597916e-06, 'epoch': 2.4251497005988023}\n",
            "Captured loss: 2.0404 at step 3645\n",
            "Logs at step 3646: {'loss': 2.3952, 'grad_norm': 1.0501816272735596, 'learning_rate': 9.580838323353295e-06, 'epoch': 2.42581503659348}\n",
            "Captured loss: 2.3952 at step 3646\n",
            "Logs at step 3647: {'loss': 2.4847, 'grad_norm': 1.9645808935165405, 'learning_rate': 9.569749390108672e-06, 'epoch': 2.426480372588157}\n",
            "Captured loss: 2.4847 at step 3647\n",
            "Logs at step 3648: {'loss': 1.9801, 'grad_norm': 2.1426045894622803, 'learning_rate': 9.55866045686405e-06, 'epoch': 2.4271457085828345}\n",
            "Captured loss: 1.9801 at step 3648\n",
            "Logs at step 3649: {'loss': 2.3026, 'grad_norm': 1.322754979133606, 'learning_rate': 9.547571523619428e-06, 'epoch': 2.4278110445775116}\n",
            "Captured loss: 2.3026 at step 3649\n",
            "Logs at step 3650: {'loss': 2.6258, 'grad_norm': 1.9038608074188232, 'learning_rate': 9.536482590374807e-06, 'epoch': 2.4284763805721887}\n",
            "Captured loss: 2.6258 at step 3650\n",
            "Logs at step 3651: {'loss': 2.3666, 'grad_norm': 1.821811556816101, 'learning_rate': 9.525393657130184e-06, 'epoch': 2.4291417165668663}\n",
            "Captured loss: 2.3666 at step 3651\n",
            "Logs at step 3652: {'loss': 2.4122, 'grad_norm': 1.7515852451324463, 'learning_rate': 9.514304723885564e-06, 'epoch': 2.4298070525615434}\n",
            "Captured loss: 2.4122 at step 3652\n",
            "Logs at step 3653: {'loss': 2.4582, 'grad_norm': 1.688688039779663, 'learning_rate': 9.50321579064094e-06, 'epoch': 2.430472388556221}\n",
            "Captured loss: 2.4582 at step 3653\n",
            "Logs at step 3654: {'loss': 2.5599, 'grad_norm': 1.3904799222946167, 'learning_rate': 9.492126857396319e-06, 'epoch': 2.431137724550898}\n",
            "Captured loss: 2.5599 at step 3654\n",
            "Logs at step 3655: {'loss': 2.4707, 'grad_norm': 1.4604281187057495, 'learning_rate': 9.481037924151696e-06, 'epoch': 2.4318030605455756}\n",
            "Captured loss: 2.4707 at step 3655\n",
            "Logs at step 3656: {'loss': 2.508, 'grad_norm': 1.4768872261047363, 'learning_rate': 9.469948990907076e-06, 'epoch': 2.4324683965402527}\n",
            "Captured loss: 2.508 at step 3656\n",
            "Logs at step 3657: {'loss': 2.3584, 'grad_norm': 1.5067745447158813, 'learning_rate': 9.458860057662455e-06, 'epoch': 2.4331337325349303}\n",
            "Captured loss: 2.3584 at step 3657\n",
            "Logs at step 3658: {'loss': 2.515, 'grad_norm': 1.465829610824585, 'learning_rate': 9.44777112441783e-06, 'epoch': 2.4337990685296074}\n",
            "Captured loss: 2.515 at step 3658\n",
            "Logs at step 3659: {'loss': 2.1078, 'grad_norm': 1.7133703231811523, 'learning_rate': 9.43668219117321e-06, 'epoch': 2.434464404524285}\n",
            "Captured loss: 2.1078 at step 3659\n",
            "Logs at step 3660: {'loss': 2.0628, 'grad_norm': 1.632839560508728, 'learning_rate': 9.425593257928587e-06, 'epoch': 2.435129740518962}\n",
            "Captured loss: 2.0628 at step 3660\n",
            "Logs at step 3661: {'loss': 2.4205, 'grad_norm': 1.917250633239746, 'learning_rate': 9.414504324683967e-06, 'epoch': 2.4357950765136396}\n",
            "Captured loss: 2.4205 at step 3661\n",
            "Logs at step 3662: {'loss': 2.3585, 'grad_norm': 1.1693170070648193, 'learning_rate': 9.403415391439344e-06, 'epoch': 2.4364604125083167}\n",
            "Captured loss: 2.3585 at step 3662\n",
            "Logs at step 3663: {'loss': 2.2784, 'grad_norm': 2.0493104457855225, 'learning_rate': 9.392326458194722e-06, 'epoch': 2.437125748502994}\n",
            "Captured loss: 2.2784 at step 3663\n",
            "Logs at step 3664: {'loss': 2.4985, 'grad_norm': 1.4264010190963745, 'learning_rate': 9.3812375249501e-06, 'epoch': 2.4377910844976713}\n",
            "Captured loss: 2.4985 at step 3664\n",
            "Logs at step 3665: {'loss': 2.2163, 'grad_norm': 1.790573000907898, 'learning_rate': 9.370148591705479e-06, 'epoch': 2.4384564204923485}\n",
            "Captured loss: 2.2163 at step 3665\n",
            "Logs at step 3666: {'loss': 2.4267, 'grad_norm': 1.1078935861587524, 'learning_rate': 9.359059658460856e-06, 'epoch': 2.439121756487026}\n",
            "Captured loss: 2.4267 at step 3666\n",
            "Logs at step 3667: {'loss': 2.3744, 'grad_norm': 1.17799711227417, 'learning_rate': 9.347970725216235e-06, 'epoch': 2.439787092481703}\n",
            "Captured loss: 2.3744 at step 3667\n",
            "Logs at step 3668: {'loss': 2.5013, 'grad_norm': 1.5950536727905273, 'learning_rate': 9.336881791971613e-06, 'epoch': 2.4404524284763807}\n",
            "Captured loss: 2.5013 at step 3668\n",
            "Logs at step 3669: {'loss': 2.1582, 'grad_norm': 1.8195520639419556, 'learning_rate': 9.32579285872699e-06, 'epoch': 2.4411177644710578}\n",
            "Captured loss: 2.1582 at step 3669\n",
            "Logs at step 3670: {'loss': 2.5651, 'grad_norm': 1.988011360168457, 'learning_rate': 9.31470392548237e-06, 'epoch': 2.4417831004657353}\n",
            "Captured loss: 2.5651 at step 3670\n",
            "Logs at step 3671: {'loss': 2.5081, 'grad_norm': 1.3686445951461792, 'learning_rate': 9.303614992237747e-06, 'epoch': 2.4424484364604124}\n",
            "Captured loss: 2.5081 at step 3671\n",
            "Logs at step 3672: {'loss': 2.0822, 'grad_norm': 2.0048937797546387, 'learning_rate': 9.292526058993126e-06, 'epoch': 2.44311377245509}\n",
            "Captured loss: 2.0822 at step 3672\n",
            "Logs at step 3673: {'loss': 2.5879, 'grad_norm': 1.8145787715911865, 'learning_rate': 9.281437125748502e-06, 'epoch': 2.443779108449767}\n",
            "Captured loss: 2.5879 at step 3673\n",
            "Logs at step 3674: {'loss': 2.4032, 'grad_norm': 1.0033317804336548, 'learning_rate': 9.270348192503882e-06, 'epoch': 2.4444444444444446}\n",
            "Captured loss: 2.4032 at step 3674\n",
            "Logs at step 3675: {'loss': 2.2354, 'grad_norm': 1.4645742177963257, 'learning_rate': 9.259259259259259e-06, 'epoch': 2.4451097804391217}\n",
            "Captured loss: 2.2354 at step 3675\n",
            "Logs at step 3676: {'loss': 2.3288, 'grad_norm': 1.1761592626571655, 'learning_rate': 9.248170326014638e-06, 'epoch': 2.445775116433799}\n",
            "Captured loss: 2.3288 at step 3676\n",
            "Logs at step 3677: {'loss': 1.9461, 'grad_norm': 1.8606892824172974, 'learning_rate': 9.237081392770016e-06, 'epoch': 2.4464404524284764}\n",
            "Captured loss: 1.9461 at step 3677\n",
            "Logs at step 3678: {'loss': 1.6022, 'grad_norm': 2.084092617034912, 'learning_rate': 9.225992459525393e-06, 'epoch': 2.4471057884231535}\n",
            "Captured loss: 1.6022 at step 3678\n",
            "Logs at step 3679: {'loss': 2.0842, 'grad_norm': 1.7276151180267334, 'learning_rate': 9.214903526280773e-06, 'epoch': 2.447771124417831}\n",
            "Captured loss: 2.0842 at step 3679\n",
            "Logs at step 3680: {'loss': 2.0038, 'grad_norm': 2.25669002532959, 'learning_rate': 9.20381459303615e-06, 'epoch': 2.448436460412508}\n",
            "Captured loss: 2.0038 at step 3680\n",
            "Logs at step 3681: {'loss': 2.2141, 'grad_norm': 1.6102029085159302, 'learning_rate': 9.19272565979153e-06, 'epoch': 2.4491017964071857}\n",
            "Captured loss: 2.2141 at step 3681\n",
            "Logs at step 3682: {'loss': 2.3771, 'grad_norm': 1.5757780075073242, 'learning_rate': 9.181636726546907e-06, 'epoch': 2.449767132401863}\n",
            "Captured loss: 2.3771 at step 3682\n",
            "Logs at step 3683: {'loss': 2.3914, 'grad_norm': 2.2215752601623535, 'learning_rate': 9.170547793302285e-06, 'epoch': 2.4504324683965404}\n",
            "Captured loss: 2.3914 at step 3683\n",
            "Logs at step 3684: {'loss': 2.4255, 'grad_norm': 1.6201457977294922, 'learning_rate': 9.159458860057662e-06, 'epoch': 2.4510978043912175}\n",
            "Captured loss: 2.4255 at step 3684\n",
            "Logs at step 3685: {'loss': 2.2975, 'grad_norm': 2.7931692600250244, 'learning_rate': 9.148369926813041e-06, 'epoch': 2.451763140385895}\n",
            "Captured loss: 2.2975 at step 3685\n",
            "Logs at step 3686: {'loss': 2.2898, 'grad_norm': 1.7071377038955688, 'learning_rate': 9.137280993568419e-06, 'epoch': 2.452428476380572}\n",
            "Captured loss: 2.2898 at step 3686\n",
            "Logs at step 3687: {'loss': 2.2737, 'grad_norm': 1.8099340200424194, 'learning_rate': 9.126192060323798e-06, 'epoch': 2.4530938123752497}\n",
            "Captured loss: 2.2737 at step 3687\n",
            "Logs at step 3688: {'loss': 2.0918, 'grad_norm': 1.6901566982269287, 'learning_rate': 9.115103127079174e-06, 'epoch': 2.453759148369927}\n",
            "Captured loss: 2.0918 at step 3688\n",
            "Logs at step 3689: {'loss': 2.4649, 'grad_norm': 1.659485101699829, 'learning_rate': 9.104014193834553e-06, 'epoch': 2.454424484364604}\n",
            "Captured loss: 2.4649 at step 3689\n",
            "Logs at step 3690: {'loss': 2.0766, 'grad_norm': 2.0412638187408447, 'learning_rate': 9.092925260589933e-06, 'epoch': 2.4550898203592815}\n",
            "Captured loss: 2.0766 at step 3690\n",
            "Logs at step 3691: {'loss': 1.9125, 'grad_norm': 2.3270914554595947, 'learning_rate': 9.08183632734531e-06, 'epoch': 2.4557551563539586}\n",
            "Captured loss: 1.9125 at step 3691\n",
            "Logs at step 3692: {'loss': 2.6575, 'grad_norm': 1.4814300537109375, 'learning_rate': 9.070747394100688e-06, 'epoch': 2.456420492348636}\n",
            "Captured loss: 2.6575 at step 3692\n",
            "Logs at step 3693: {'loss': 2.579, 'grad_norm': 1.5152615308761597, 'learning_rate': 9.059658460856065e-06, 'epoch': 2.4570858283433132}\n",
            "Captured loss: 2.579 at step 3693\n",
            "Logs at step 3694: {'loss': 2.4452, 'grad_norm': 1.546108603477478, 'learning_rate': 9.048569527611444e-06, 'epoch': 2.457751164337991}\n",
            "Captured loss: 2.4452 at step 3694\n",
            "Logs at step 3695: {'loss': 2.3916, 'grad_norm': 1.7685381174087524, 'learning_rate': 9.037480594366822e-06, 'epoch': 2.458416500332668}\n",
            "Captured loss: 2.3916 at step 3695\n",
            "Logs at step 3696: {'loss': 2.2497, 'grad_norm': 1.4458131790161133, 'learning_rate': 9.026391661122201e-06, 'epoch': 2.4590818363273454}\n",
            "Captured loss: 2.2497 at step 3696\n",
            "Logs at step 3697: {'loss': 2.4436, 'grad_norm': 1.198980689048767, 'learning_rate': 9.015302727877579e-06, 'epoch': 2.4597471723220226}\n",
            "Captured loss: 2.4436 at step 3697\n",
            "Logs at step 3698: {'loss': 2.47, 'grad_norm': 1.2820823192596436, 'learning_rate': 9.004213794632956e-06, 'epoch': 2.4604125083167}\n",
            "Captured loss: 2.47 at step 3698\n",
            "Logs at step 3699: {'loss': 2.5337, 'grad_norm': 1.311790108680725, 'learning_rate': 8.993124861388334e-06, 'epoch': 2.461077844311377}\n",
            "Captured loss: 2.5337 at step 3699\n",
            "Logs at step 3700: {'loss': 2.4843, 'grad_norm': 1.1655970811843872, 'learning_rate': 8.982035928143713e-06, 'epoch': 2.4617431803060548}\n",
            "Captured loss: 2.4843 at step 3700\n",
            "Logs at step 3701: {'loss': 2.4411, 'grad_norm': 1.0147830247879028, 'learning_rate': 8.970946994899092e-06, 'epoch': 2.462408516300732}\n",
            "Captured loss: 2.4411 at step 3701\n",
            "Logs at step 3702: {'loss': 2.2668, 'grad_norm': 1.6183557510375977, 'learning_rate': 8.95985806165447e-06, 'epoch': 2.463073852295409}\n",
            "Captured loss: 2.2668 at step 3702\n",
            "Logs at step 3703: {'loss': 2.462, 'grad_norm': 1.2590569257736206, 'learning_rate': 8.948769128409847e-06, 'epoch': 2.4637391882900865}\n",
            "Captured loss: 2.462 at step 3703\n",
            "Logs at step 3704: {'loss': 2.2015, 'grad_norm': 1.6971608400344849, 'learning_rate': 8.937680195165225e-06, 'epoch': 2.4644045242847636}\n",
            "Captured loss: 2.2015 at step 3704\n",
            "Logs at step 3705: {'loss': 2.2688, 'grad_norm': 1.4315675497055054, 'learning_rate': 8.926591261920604e-06, 'epoch': 2.465069860279441}\n",
            "Captured loss: 2.2688 at step 3705\n",
            "Logs at step 3706: {'loss': 2.0523, 'grad_norm': 1.8375831842422485, 'learning_rate': 8.915502328675982e-06, 'epoch': 2.4657351962741183}\n",
            "Captured loss: 2.0523 at step 3706\n",
            "Logs at step 3707: {'loss': 2.2388, 'grad_norm': 1.800653100013733, 'learning_rate': 8.90441339543136e-06, 'epoch': 2.466400532268796}\n",
            "Captured loss: 2.2388 at step 3707\n",
            "Logs at step 3708: {'loss': 2.1264, 'grad_norm': 2.357879638671875, 'learning_rate': 8.893324462186737e-06, 'epoch': 2.467065868263473}\n",
            "Captured loss: 2.1264 at step 3708\n",
            "Logs at step 3709: {'loss': 2.0027, 'grad_norm': 2.152656316757202, 'learning_rate': 8.882235528942116e-06, 'epoch': 2.4677312042581505}\n",
            "Captured loss: 2.0027 at step 3709\n",
            "Logs at step 3710: {'loss': 1.9687, 'grad_norm': 1.5559688806533813, 'learning_rate': 8.871146595697494e-06, 'epoch': 2.4683965402528276}\n",
            "Captured loss: 1.9687 at step 3710\n",
            "Logs at step 3711: {'loss': 2.2354, 'grad_norm': 3.4475595951080322, 'learning_rate': 8.860057662452873e-06, 'epoch': 2.469061876247505}\n",
            "Captured loss: 2.2354 at step 3711\n",
            "Logs at step 3712: {'loss': 2.2556, 'grad_norm': 1.537020206451416, 'learning_rate': 8.84896872920825e-06, 'epoch': 2.4697272122421823}\n",
            "Captured loss: 2.2556 at step 3712\n",
            "Logs at step 3713: {'loss': 2.4323, 'grad_norm': 2.3119683265686035, 'learning_rate': 8.837879795963628e-06, 'epoch': 2.47039254823686}\n",
            "Captured loss: 2.4323 at step 3713\n",
            "Logs at step 3714: {'loss': 2.5352, 'grad_norm': 1.4721897840499878, 'learning_rate': 8.826790862719007e-06, 'epoch': 2.471057884231537}\n",
            "Captured loss: 2.5352 at step 3714\n",
            "Logs at step 3715: {'loss': 2.4796, 'grad_norm': 1.226263403892517, 'learning_rate': 8.815701929474385e-06, 'epoch': 2.471723220226214}\n",
            "Captured loss: 2.4796 at step 3715\n",
            "Logs at step 3716: {'loss': 1.9027, 'grad_norm': 2.320167064666748, 'learning_rate': 8.804612996229764e-06, 'epoch': 2.4723885562208916}\n",
            "Captured loss: 1.9027 at step 3716\n",
            "Logs at step 3717: {'loss': 2.5264, 'grad_norm': 1.884817123413086, 'learning_rate': 8.793524062985142e-06, 'epoch': 2.4730538922155687}\n",
            "Captured loss: 2.5264 at step 3717\n",
            "Logs at step 3718: {'loss': 2.3986, 'grad_norm': 1.7312254905700684, 'learning_rate': 8.78243512974052e-06, 'epoch': 2.4737192282102463}\n",
            "Captured loss: 2.3986 at step 3718\n",
            "Logs at step 3719: {'loss': 2.2093, 'grad_norm': 1.5296062231063843, 'learning_rate': 8.771346196495897e-06, 'epoch': 2.4743845642049234}\n",
            "Captured loss: 2.2093 at step 3719\n",
            "Logs at step 3720: {'loss': 2.3317, 'grad_norm': 1.5377724170684814, 'learning_rate': 8.760257263251276e-06, 'epoch': 2.475049900199601}\n",
            "Captured loss: 2.3317 at step 3720\n",
            "Logs at step 3721: {'loss': 2.0075, 'grad_norm': 1.728684902191162, 'learning_rate': 8.749168330006654e-06, 'epoch': 2.475715236194278}\n",
            "Captured loss: 2.0075 at step 3721\n",
            "Logs at step 3722: {'loss': 1.865, 'grad_norm': 1.6770615577697754, 'learning_rate': 8.738079396762033e-06, 'epoch': 2.4763805721889556}\n",
            "Captured loss: 1.865 at step 3722\n",
            "Logs at step 3723: {'loss': 2.2451, 'grad_norm': 1.522746205329895, 'learning_rate': 8.72699046351741e-06, 'epoch': 2.4770459081836327}\n",
            "Captured loss: 2.2451 at step 3723\n",
            "Logs at step 3724: {'loss': 2.427, 'grad_norm': 1.1417983770370483, 'learning_rate': 8.715901530272788e-06, 'epoch': 2.4777112441783102}\n",
            "Captured loss: 2.427 at step 3724\n",
            "Logs at step 3725: {'loss': 2.4018, 'grad_norm': 1.7250653505325317, 'learning_rate': 8.704812597028167e-06, 'epoch': 2.4783765801729873}\n",
            "Captured loss: 2.4018 at step 3725\n",
            "Logs at step 3726: {'loss': 2.4637, 'grad_norm': 1.4762964248657227, 'learning_rate': 8.693723663783545e-06, 'epoch': 2.479041916167665}\n",
            "Captured loss: 2.4637 at step 3726\n",
            "Logs at step 3727: {'loss': 2.5355, 'grad_norm': 1.4301338195800781, 'learning_rate': 8.682634730538922e-06, 'epoch': 2.479707252162342}\n",
            "Captured loss: 2.5355 at step 3727\n",
            "Logs at step 3728: {'loss': 2.2898, 'grad_norm': 1.6485384702682495, 'learning_rate': 8.6715457972943e-06, 'epoch': 2.480372588157019}\n",
            "Captured loss: 2.2898 at step 3728\n",
            "Logs at step 3729: {'loss': 2.495, 'grad_norm': 1.2536840438842773, 'learning_rate': 8.660456864049679e-06, 'epoch': 2.4810379241516967}\n",
            "Captured loss: 2.495 at step 3729\n",
            "Logs at step 3730: {'loss': 2.5078, 'grad_norm': 1.203321099281311, 'learning_rate': 8.649367930805057e-06, 'epoch': 2.4817032601463738}\n",
            "Captured loss: 2.5078 at step 3730\n",
            "Logs at step 3731: {'loss': 2.4694, 'grad_norm': 3.541067361831665, 'learning_rate': 8.638278997560436e-06, 'epoch': 2.4823685961410513}\n",
            "Captured loss: 2.4694 at step 3731\n",
            "Logs at step 3732: {'loss': 2.5164, 'grad_norm': 1.1531248092651367, 'learning_rate': 8.627190064315813e-06, 'epoch': 2.4830339321357284}\n",
            "Captured loss: 2.5164 at step 3732\n",
            "Logs at step 3733: {'loss': 1.9842, 'grad_norm': 3.1111364364624023, 'learning_rate': 8.616101131071191e-06, 'epoch': 2.483699268130406}\n",
            "Captured loss: 1.9842 at step 3733\n",
            "Logs at step 3734: {'loss': 2.5376, 'grad_norm': 1.3912452459335327, 'learning_rate': 8.605012197826568e-06, 'epoch': 2.484364604125083}\n",
            "Captured loss: 2.5376 at step 3734\n",
            "Logs at step 3735: {'loss': 2.0219, 'grad_norm': 1.9885951280593872, 'learning_rate': 8.593923264581948e-06, 'epoch': 2.4850299401197606}\n",
            "Captured loss: 2.0219 at step 3735\n",
            "Logs at step 3736: {'loss': 2.3905, 'grad_norm': 1.3745348453521729, 'learning_rate': 8.582834331337327e-06, 'epoch': 2.4856952761144377}\n",
            "Captured loss: 2.3905 at step 3736\n",
            "Logs at step 3737: {'loss': 1.7374, 'grad_norm': 1.8884532451629639, 'learning_rate': 8.571745398092705e-06, 'epoch': 2.486360612109115}\n",
            "Captured loss: 1.7374 at step 3737\n",
            "Logs at step 3738: {'loss': 2.4354, 'grad_norm': 1.382158637046814, 'learning_rate': 8.560656464848082e-06, 'epoch': 2.4870259481037924}\n",
            "Captured loss: 2.4354 at step 3738\n",
            "Logs at step 3739: {'loss': 2.0352, 'grad_norm': 1.9048525094985962, 'learning_rate': 8.54956753160346e-06, 'epoch': 2.48769128409847}\n",
            "Captured loss: 2.0352 at step 3739\n",
            "Logs at step 3740: {'loss': 2.3351, 'grad_norm': 1.676131248474121, 'learning_rate': 8.538478598358839e-06, 'epoch': 2.488356620093147}\n",
            "Captured loss: 2.3351 at step 3740\n",
            "Logs at step 3741: {'loss': 2.5089, 'grad_norm': 1.8088616132736206, 'learning_rate': 8.527389665114216e-06, 'epoch': 2.489021956087824}\n",
            "Captured loss: 2.5089 at step 3741\n",
            "Logs at step 3742: {'loss': 2.3943, 'grad_norm': 1.542300820350647, 'learning_rate': 8.516300731869594e-06, 'epoch': 2.4896872920825017}\n",
            "Captured loss: 2.3943 at step 3742\n",
            "Logs at step 3743: {'loss': 2.2207, 'grad_norm': 1.5322657823562622, 'learning_rate': 8.505211798624972e-06, 'epoch': 2.490352628077179}\n",
            "Captured loss: 2.2207 at step 3743\n",
            "Logs at step 3744: {'loss': 2.3615, 'grad_norm': 1.332758903503418, 'learning_rate': 8.49412286538035e-06, 'epoch': 2.4910179640718564}\n",
            "Captured loss: 2.3615 at step 3744\n",
            "Logs at step 3745: {'loss': 2.5326, 'grad_norm': 1.7714970111846924, 'learning_rate': 8.483033932135728e-06, 'epoch': 2.4916833000665335}\n",
            "Captured loss: 2.5326 at step 3745\n",
            "Logs at step 3746: {'loss': 2.5139, 'grad_norm': 1.2065943479537964, 'learning_rate': 8.471944998891108e-06, 'epoch': 2.492348636061211}\n",
            "Captured loss: 2.5139 at step 3746\n",
            "Logs at step 3747: {'loss': 2.5212, 'grad_norm': 1.164370059967041, 'learning_rate': 8.460856065646485e-06, 'epoch': 2.493013972055888}\n",
            "Captured loss: 2.5212 at step 3747\n",
            "Logs at step 3748: {'loss': 2.3786, 'grad_norm': 1.8868845701217651, 'learning_rate': 8.449767132401863e-06, 'epoch': 2.4936793080505657}\n",
            "Captured loss: 2.3786 at step 3748\n",
            "Logs at step 3749: {'loss': 2.2558, 'grad_norm': 1.8819386959075928, 'learning_rate': 8.438678199157242e-06, 'epoch': 2.494344644045243}\n",
            "Captured loss: 2.2558 at step 3749\n",
            "Logs at step 3750: {'loss': 2.3861, 'grad_norm': 1.3633249998092651, 'learning_rate': 8.42758926591262e-06, 'epoch': 2.49500998003992}\n",
            "Captured loss: 2.3861 at step 3750\n",
            "Logs at step 3751: {'loss': 2.4063, 'grad_norm': 1.5049920082092285, 'learning_rate': 8.416500332667999e-06, 'epoch': 2.4956753160345975}\n",
            "Captured loss: 2.4063 at step 3751\n",
            "Logs at step 3752: {'loss': 2.254, 'grad_norm': 1.943982481956482, 'learning_rate': 8.405411399423376e-06, 'epoch': 2.496340652029275}\n",
            "Captured loss: 2.254 at step 3752\n",
            "Logs at step 3753: {'loss': 2.3118, 'grad_norm': 1.6620136499404907, 'learning_rate': 8.394322466178754e-06, 'epoch': 2.497005988023952}\n",
            "Captured loss: 2.3118 at step 3753\n",
            "Logs at step 3754: {'loss': 2.5651, 'grad_norm': 1.551735758781433, 'learning_rate': 8.383233532934131e-06, 'epoch': 2.4976713240186292}\n",
            "Captured loss: 2.5651 at step 3754\n",
            "Logs at step 3755: {'loss': 1.9302, 'grad_norm': 1.6191486120224, 'learning_rate': 8.37214459968951e-06, 'epoch': 2.498336660013307}\n",
            "Captured loss: 1.9302 at step 3755\n",
            "Logs at step 3756: {'loss': 2.3746, 'grad_norm': 1.4299254417419434, 'learning_rate': 8.361055666444888e-06, 'epoch': 2.499001996007984}\n",
            "Captured loss: 2.3746 at step 3756\n",
            "Logs at step 3757: {'loss': 2.2968, 'grad_norm': 1.2481346130371094, 'learning_rate': 8.349966733200266e-06, 'epoch': 2.4996673320026614}\n",
            "Captured loss: 2.2968 at step 3757\n",
            "Logs at step 3758: {'loss': 2.2306, 'grad_norm': 1.8151148557662964, 'learning_rate': 8.338877799955645e-06, 'epoch': 2.5003326679973386}\n",
            "Captured loss: 2.2306 at step 3758\n",
            "Logs at step 3759: {'loss': 2.4286, 'grad_norm': 1.5942085981369019, 'learning_rate': 8.327788866711023e-06, 'epoch': 2.500998003992016}\n",
            "Captured loss: 2.4286 at step 3759\n",
            "Logs at step 3760: {'loss': 2.1198, 'grad_norm': 2.115837574005127, 'learning_rate': 8.316699933466402e-06, 'epoch': 2.501663339986693}\n",
            "Captured loss: 2.1198 at step 3760\n",
            "Logs at step 3761: {'loss': 2.1828, 'grad_norm': 1.7550275325775146, 'learning_rate': 8.30561100022178e-06, 'epoch': 2.5023286759813708}\n",
            "Captured loss: 2.1828 at step 3761\n",
            "Logs at step 3762: {'loss': 2.2764, 'grad_norm': 1.7968076467514038, 'learning_rate': 8.294522066977157e-06, 'epoch': 2.502994011976048}\n",
            "Captured loss: 2.2764 at step 3762\n",
            "Logs at step 3763: {'loss': 2.4548, 'grad_norm': 1.4792301654815674, 'learning_rate': 8.283433133732534e-06, 'epoch': 2.503659347970725}\n",
            "Captured loss: 2.4548 at step 3763\n",
            "Logs at step 3764: {'loss': 2.5398, 'grad_norm': 1.2535243034362793, 'learning_rate': 8.272344200487914e-06, 'epoch': 2.5043246839654025}\n",
            "Captured loss: 2.5398 at step 3764\n",
            "Logs at step 3765: {'loss': 2.5102, 'grad_norm': 1.6216342449188232, 'learning_rate': 8.261255267243291e-06, 'epoch': 2.50499001996008}\n",
            "Captured loss: 2.5102 at step 3765\n",
            "Logs at step 3766: {'loss': 1.984, 'grad_norm': 1.5039863586425781, 'learning_rate': 8.25016633399867e-06, 'epoch': 2.505655355954757}\n",
            "Captured loss: 1.984 at step 3766\n",
            "Logs at step 3767: {'loss': 2.4982, 'grad_norm': 1.3434438705444336, 'learning_rate': 8.239077400754048e-06, 'epoch': 2.5063206919494343}\n",
            "Captured loss: 2.4982 at step 3767\n",
            "Logs at step 3768: {'loss': 2.5539, 'grad_norm': 1.671338677406311, 'learning_rate': 8.227988467509426e-06, 'epoch': 2.506986027944112}\n",
            "Captured loss: 2.5539 at step 3768\n",
            "Logs at step 3769: {'loss': 2.2234, 'grad_norm': 1.4126012325286865, 'learning_rate': 8.216899534264805e-06, 'epoch': 2.507651363938789}\n",
            "Captured loss: 2.2234 at step 3769\n",
            "Logs at step 3770: {'loss': 2.251, 'grad_norm': 1.886772632598877, 'learning_rate': 8.205810601020182e-06, 'epoch': 2.5083166999334665}\n",
            "Captured loss: 2.251 at step 3770\n",
            "Logs at step 3771: {'loss': 2.2882, 'grad_norm': 1.4264748096466064, 'learning_rate': 8.194721667775562e-06, 'epoch': 2.5089820359281436}\n",
            "Captured loss: 2.2882 at step 3771\n",
            "Logs at step 3772: {'loss': 2.074, 'grad_norm': 1.8793970346450806, 'learning_rate': 8.183632734530937e-06, 'epoch': 2.509647371922821}\n",
            "Captured loss: 2.074 at step 3772\n",
            "Logs at step 3773: {'loss': 2.1421, 'grad_norm': 1.5293537378311157, 'learning_rate': 8.172543801286317e-06, 'epoch': 2.5103127079174983}\n",
            "Captured loss: 2.1421 at step 3773\n",
            "Logs at step 3774: {'loss': 2.0637, 'grad_norm': 1.596996545791626, 'learning_rate': 8.161454868041694e-06, 'epoch': 2.510978043912176}\n",
            "Captured loss: 2.0637 at step 3774\n",
            "Logs at step 3775: {'loss': 2.4093, 'grad_norm': 1.7993669509887695, 'learning_rate': 8.150365934797073e-06, 'epoch': 2.511643379906853}\n",
            "Captured loss: 2.4093 at step 3775\n",
            "Logs at step 3776: {'loss': 2.3195, 'grad_norm': 1.4785985946655273, 'learning_rate': 8.139277001552451e-06, 'epoch': 2.51230871590153}\n",
            "Captured loss: 2.3195 at step 3776\n",
            "Logs at step 3777: {'loss': 2.2382, 'grad_norm': 1.4623360633850098, 'learning_rate': 8.128188068307829e-06, 'epoch': 2.5129740518962076}\n",
            "Captured loss: 2.2382 at step 3777\n",
            "Logs at step 3778: {'loss': 2.5365, 'grad_norm': 2.0870261192321777, 'learning_rate': 8.117099135063206e-06, 'epoch': 2.513639387890885}\n",
            "Captured loss: 2.5365 at step 3778\n",
            "Logs at step 3779: {'loss': 2.0494, 'grad_norm': 2.2367398738861084, 'learning_rate': 8.106010201818585e-06, 'epoch': 2.5143047238855623}\n",
            "Captured loss: 2.0494 at step 3779\n",
            "Logs at step 3780: {'loss': 2.4125, 'grad_norm': 1.3177146911621094, 'learning_rate': 8.094921268573965e-06, 'epoch': 2.5149700598802394}\n",
            "Captured loss: 2.4125 at step 3780\n",
            "Logs at step 3781: {'loss': 1.95, 'grad_norm': 1.808481216430664, 'learning_rate': 8.083832335329342e-06, 'epoch': 2.515635395874917}\n",
            "Captured loss: 1.95 at step 3781\n",
            "Logs at step 3782: {'loss': 2.1119, 'grad_norm': 1.66790771484375, 'learning_rate': 8.07274340208472e-06, 'epoch': 2.516300731869594}\n",
            "Captured loss: 2.1119 at step 3782\n",
            "Logs at step 3783: {'loss': 1.9817, 'grad_norm': 1.4202100038528442, 'learning_rate': 8.061654468840097e-06, 'epoch': 2.5169660678642716}\n",
            "Captured loss: 1.9817 at step 3783\n",
            "Logs at step 3784: {'loss': 2.4952, 'grad_norm': 1.4178130626678467, 'learning_rate': 8.050565535595477e-06, 'epoch': 2.5176314038589487}\n",
            "Captured loss: 2.4952 at step 3784\n",
            "Logs at step 3785: {'loss': 2.2978, 'grad_norm': 1.4374380111694336, 'learning_rate': 8.039476602350854e-06, 'epoch': 2.5182967398536262}\n",
            "Captured loss: 2.2978 at step 3785\n",
            "Logs at step 3786: {'loss': 2.389, 'grad_norm': 1.5377954244613647, 'learning_rate': 8.028387669106233e-06, 'epoch': 2.5189620758483033}\n",
            "Captured loss: 2.389 at step 3786\n",
            "Logs at step 3787: {'loss': 2.3336, 'grad_norm': 1.456536889076233, 'learning_rate': 8.01729873586161e-06, 'epoch': 2.519627411842981}\n",
            "Captured loss: 2.3336 at step 3787\n",
            "Logs at step 3788: {'loss': 2.6022, 'grad_norm': 1.50483238697052, 'learning_rate': 8.006209802616988e-06, 'epoch': 2.520292747837658}\n",
            "Captured loss: 2.6022 at step 3788\n",
            "Logs at step 3789: {'loss': 2.4633, 'grad_norm': 1.826179027557373, 'learning_rate': 7.995120869372366e-06, 'epoch': 2.520958083832335}\n",
            "Captured loss: 2.4633 at step 3789\n",
            "Logs at step 3790: {'loss': 1.5895, 'grad_norm': 2.190687417984009, 'learning_rate': 7.984031936127745e-06, 'epoch': 2.5216234198270127}\n",
            "Captured loss: 1.5895 at step 3790\n",
            "Logs at step 3791: {'loss': 1.7775, 'grad_norm': 2.065727710723877, 'learning_rate': 7.972943002883124e-06, 'epoch': 2.52228875582169}\n",
            "Captured loss: 1.7775 at step 3791\n",
            "Logs at step 3792: {'loss': 2.4186, 'grad_norm': 1.8567324876785278, 'learning_rate': 7.9618540696385e-06, 'epoch': 2.5229540918163673}\n",
            "Captured loss: 2.4186 at step 3792\n",
            "Logs at step 3793: {'loss': 2.5152, 'grad_norm': 1.2984750270843506, 'learning_rate': 7.95076513639388e-06, 'epoch': 2.5236194278110444}\n",
            "Captured loss: 2.5152 at step 3793\n",
            "Logs at step 3794: {'loss': 2.6526, 'grad_norm': 1.8517847061157227, 'learning_rate': 7.939676203149257e-06, 'epoch': 2.524284763805722}\n",
            "Captured loss: 2.6526 at step 3794\n",
            "Logs at step 3795: {'loss': 2.4794, 'grad_norm': 1.2556469440460205, 'learning_rate': 7.928587269904636e-06, 'epoch': 2.524950099800399}\n",
            "Captured loss: 2.4794 at step 3795\n",
            "Logs at step 3796: {'loss': 2.1717, 'grad_norm': 1.8820521831512451, 'learning_rate': 7.917498336660014e-06, 'epoch': 2.5256154357950766}\n",
            "Captured loss: 2.1717 at step 3796\n",
            "Logs at step 3797: {'loss': 2.347, 'grad_norm': 1.3231662511825562, 'learning_rate': 7.906409403415391e-06, 'epoch': 2.5262807717897537}\n",
            "Captured loss: 2.347 at step 3797\n",
            "Logs at step 3798: {'loss': 2.4587, 'grad_norm': 1.4694550037384033, 'learning_rate': 7.895320470170769e-06, 'epoch': 2.5269461077844313}\n",
            "Captured loss: 2.4587 at step 3798\n",
            "Logs at step 3799: {'loss': 2.3737, 'grad_norm': 1.2964763641357422, 'learning_rate': 7.884231536926148e-06, 'epoch': 2.5276114437791084}\n",
            "Captured loss: 2.3737 at step 3799\n",
            "Logs at step 3800: {'loss': 2.4308, 'grad_norm': 1.1780427694320679, 'learning_rate': 7.873142603681526e-06, 'epoch': 2.528276779773786}\n",
            "Captured loss: 2.4308 at step 3800\n",
            "Logs at step 3801: {'loss': 2.4101, 'grad_norm': 1.2847092151641846, 'learning_rate': 7.862053670436905e-06, 'epoch': 2.528942115768463}\n",
            "Captured loss: 2.4101 at step 3801\n",
            "Logs at step 3802: {'loss': 2.3535, 'grad_norm': 1.2343066930770874, 'learning_rate': 7.850964737192283e-06, 'epoch': 2.52960745176314}\n",
            "Captured loss: 2.3535 at step 3802\n",
            "Logs at step 3803: {'loss': 2.1018, 'grad_norm': 1.652397871017456, 'learning_rate': 7.83987580394766e-06, 'epoch': 2.5302727877578177}\n",
            "Captured loss: 2.1018 at step 3803\n",
            "Logs at step 3804: {'loss': 2.2236, 'grad_norm': 1.4411470890045166, 'learning_rate': 7.82878687070304e-06, 'epoch': 2.5309381237524953}\n",
            "Captured loss: 2.2236 at step 3804\n",
            "Logs at step 3805: {'loss': 1.9263, 'grad_norm': 1.7342588901519775, 'learning_rate': 7.817697937458417e-06, 'epoch': 2.5316034597471724}\n",
            "Captured loss: 1.9263 at step 3805\n",
            "Logs at step 3806: {'loss': 1.9684, 'grad_norm': 1.707531213760376, 'learning_rate': 7.806609004213796e-06, 'epoch': 2.5322687957418495}\n",
            "Captured loss: 1.9684 at step 3806\n",
            "Logs at step 3807: {'loss': 2.1446, 'grad_norm': 2.003960371017456, 'learning_rate': 7.795520070969172e-06, 'epoch': 2.532934131736527}\n",
            "Captured loss: 2.1446 at step 3807\n",
            "Logs at step 3808: {'loss': 2.316, 'grad_norm': 1.644770860671997, 'learning_rate': 7.784431137724551e-06, 'epoch': 2.533599467731204}\n",
            "Captured loss: 2.316 at step 3808\n",
            "Logs at step 3809: {'loss': 2.4365, 'grad_norm': 1.1213140487670898, 'learning_rate': 7.773342204479929e-06, 'epoch': 2.5342648037258817}\n",
            "Captured loss: 2.4365 at step 3809\n",
            "Logs at step 3810: {'loss': 2.2871, 'grad_norm': 1.7747973203659058, 'learning_rate': 7.762253271235308e-06, 'epoch': 2.534930139720559}\n",
            "Captured loss: 2.2871 at step 3810\n",
            "Logs at step 3811: {'loss': 2.5416, 'grad_norm': 1.4880468845367432, 'learning_rate': 7.751164337990686e-06, 'epoch': 2.5355954757152364}\n",
            "Captured loss: 2.5416 at step 3811\n",
            "Logs at step 3812: {'loss': 2.0627, 'grad_norm': 2.0770740509033203, 'learning_rate': 7.740075404746063e-06, 'epoch': 2.5362608117099135}\n",
            "Captured loss: 2.0627 at step 3812\n",
            "Logs at step 3813: {'loss': 2.0935, 'grad_norm': 1.711852788925171, 'learning_rate': 7.728986471501442e-06, 'epoch': 2.536926147704591}\n",
            "Captured loss: 2.0935 at step 3813\n",
            "Logs at step 3814: {'loss': 2.3805, 'grad_norm': 1.545977234840393, 'learning_rate': 7.71789753825682e-06, 'epoch': 2.537591483699268}\n",
            "Captured loss: 2.3805 at step 3814\n",
            "Logs at step 3815: {'loss': 2.3805, 'grad_norm': 1.4645557403564453, 'learning_rate': 7.7068086050122e-06, 'epoch': 2.5382568196939452}\n",
            "Captured loss: 2.3805 at step 3815\n",
            "Logs at step 3816: {'loss': 2.6008, 'grad_norm': 2.066100835800171, 'learning_rate': 7.695719671767577e-06, 'epoch': 2.538922155688623}\n",
            "Captured loss: 2.6008 at step 3816\n",
            "Logs at step 3817: {'loss': 2.2607, 'grad_norm': 1.9418779611587524, 'learning_rate': 7.684630738522954e-06, 'epoch': 2.5395874916833003}\n",
            "Captured loss: 2.2607 at step 3817\n",
            "Logs at step 3818: {'loss': 2.4852, 'grad_norm': 1.489259123802185, 'learning_rate': 7.673541805278332e-06, 'epoch': 2.5402528276779774}\n",
            "Captured loss: 2.4852 at step 3818\n",
            "Logs at step 3819: {'loss': 2.7406, 'grad_norm': 1.8421847820281982, 'learning_rate': 7.662452872033711e-06, 'epoch': 2.5409181636726546}\n",
            "Captured loss: 2.7406 at step 3819\n",
            "Logs at step 3820: {'loss': 2.4007, 'grad_norm': 1.1511998176574707, 'learning_rate': 7.651363938789089e-06, 'epoch': 2.541583499667332}\n",
            "Captured loss: 2.4007 at step 3820\n",
            "Logs at step 3821: {'loss': 2.0051, 'grad_norm': 2.4938948154449463, 'learning_rate': 7.640275005544468e-06, 'epoch': 2.542248835662009}\n",
            "Captured loss: 2.0051 at step 3821\n",
            "Logs at step 3822: {'loss': 2.5205, 'grad_norm': 1.364307165145874, 'learning_rate': 7.629186072299845e-06, 'epoch': 2.5429141716566868}\n",
            "Captured loss: 2.5205 at step 3822\n",
            "Logs at step 3823: {'loss': 2.5664, 'grad_norm': 1.2595930099487305, 'learning_rate': 7.618097139055223e-06, 'epoch': 2.543579507651364}\n",
            "Captured loss: 2.5664 at step 3823\n",
            "Logs at step 3824: {'loss': 2.099, 'grad_norm': 1.8057270050048828, 'learning_rate': 7.6070082058106006e-06, 'epoch': 2.544244843646041}\n",
            "Captured loss: 2.099 at step 3824\n",
            "Logs at step 3825: {'loss': 2.3461, 'grad_norm': 1.8400737047195435, 'learning_rate': 7.59591927256598e-06, 'epoch': 2.5449101796407185}\n",
            "Captured loss: 2.3461 at step 3825\n",
            "Logs at step 3826: {'loss': 2.0183, 'grad_norm': 1.7081713676452637, 'learning_rate': 7.584830339321358e-06, 'epoch': 2.545575515635396}\n",
            "Captured loss: 2.0183 at step 3826\n",
            "Logs at step 3827: {'loss': 2.5059, 'grad_norm': 1.695544958114624, 'learning_rate': 7.573741406076736e-06, 'epoch': 2.546240851630073}\n",
            "Captured loss: 2.5059 at step 3827\n",
            "Logs at step 3828: {'loss': 2.0888, 'grad_norm': 1.5592440366744995, 'learning_rate': 7.562652472832114e-06, 'epoch': 2.5469061876247503}\n",
            "Captured loss: 2.0888 at step 3828\n",
            "Logs at step 3829: {'loss': 2.0937, 'grad_norm': 1.79318368434906, 'learning_rate': 7.551563539587492e-06, 'epoch': 2.547571523619428}\n",
            "Captured loss: 2.0937 at step 3829\n",
            "Logs at step 3830: {'loss': 2.4125, 'grad_norm': 1.5008997917175293, 'learning_rate': 7.54047460634287e-06, 'epoch': 2.5482368596141054}\n",
            "Captured loss: 2.4125 at step 3830\n",
            "Logs at step 3831: {'loss': 2.4326, 'grad_norm': 1.3227393627166748, 'learning_rate': 7.529385673098248e-06, 'epoch': 2.5489021956087825}\n",
            "Captured loss: 2.4326 at step 3831\n",
            "Logs at step 3832: {'loss': 2.4263, 'grad_norm': 2.1464366912841797, 'learning_rate': 7.518296739853627e-06, 'epoch': 2.5495675316034596}\n",
            "Captured loss: 2.4263 at step 3832\n",
            "Logs at step 3833: {'loss': 2.3506, 'grad_norm': 1.5417380332946777, 'learning_rate': 7.5072078066090044e-06, 'epoch': 2.550232867598137}\n",
            "Captured loss: 2.3506 at step 3833\n",
            "Logs at step 3834: {'loss': 2.1439, 'grad_norm': 1.8980284929275513, 'learning_rate': 7.496118873364383e-06, 'epoch': 2.5508982035928143}\n",
            "Captured loss: 2.1439 at step 3834\n",
            "Logs at step 3835: {'loss': 2.4822, 'grad_norm': 1.3199669122695923, 'learning_rate': 7.48502994011976e-06, 'epoch': 2.551563539587492}\n",
            "Captured loss: 2.4822 at step 3835\n",
            "Logs at step 3836: {'loss': 2.3993, 'grad_norm': 1.1008230447769165, 'learning_rate': 7.473941006875139e-06, 'epoch': 2.552228875582169}\n",
            "Captured loss: 2.3993 at step 3836\n",
            "Logs at step 3837: {'loss': 2.1021, 'grad_norm': 1.8197060823440552, 'learning_rate': 7.462852073630518e-06, 'epoch': 2.552894211576846}\n",
            "Captured loss: 2.1021 at step 3837\n",
            "Logs at step 3838: {'loss': 1.9648, 'grad_norm': 1.6366746425628662, 'learning_rate': 7.451763140385895e-06, 'epoch': 2.5535595475715236}\n",
            "Captured loss: 1.9648 at step 3838\n",
            "Logs at step 3839: {'loss': 2.3395, 'grad_norm': 2.488563060760498, 'learning_rate': 7.440674207141274e-06, 'epoch': 2.554224883566201}\n",
            "Captured loss: 2.3395 at step 3839\n",
            "Logs at step 3840: {'loss': 2.2223, 'grad_norm': 1.9787495136260986, 'learning_rate': 7.4295852738966515e-06, 'epoch': 2.5548902195608783}\n",
            "Captured loss: 2.2223 at step 3840\n",
            "Logs at step 3841: {'loss': 2.0838, 'grad_norm': 1.611847162246704, 'learning_rate': 7.41849634065203e-06, 'epoch': 2.5555555555555554}\n",
            "Captured loss: 2.0838 at step 3841\n",
            "Logs at step 3842: {'loss': 1.909, 'grad_norm': 2.114065170288086, 'learning_rate': 7.4074074074074075e-06, 'epoch': 2.556220891550233}\n",
            "Captured loss: 1.909 at step 3842\n",
            "Logs at step 3843: {'loss': 2.6465, 'grad_norm': 1.9273627996444702, 'learning_rate': 7.396318474162786e-06, 'epoch': 2.55688622754491}\n",
            "Captured loss: 2.6465 at step 3843\n",
            "Logs at step 3844: {'loss': 2.4052, 'grad_norm': 2.3183064460754395, 'learning_rate': 7.3852295409181634e-06, 'epoch': 2.5575515635395876}\n",
            "Captured loss: 2.4052 at step 3844\n",
            "Logs at step 3845: {'loss': 2.3419, 'grad_norm': 1.724837064743042, 'learning_rate': 7.374140607673543e-06, 'epoch': 2.5582168995342647}\n",
            "Captured loss: 2.3419 at step 3845\n",
            "Logs at step 3846: {'loss': 2.5266, 'grad_norm': 2.2202537059783936, 'learning_rate': 7.363051674428919e-06, 'epoch': 2.5588822355289422}\n",
            "Captured loss: 2.5266 at step 3846\n",
            "Logs at step 3847: {'loss': 2.3975, 'grad_norm': 1.767706036567688, 'learning_rate': 7.351962741184299e-06, 'epoch': 2.5595475715236193}\n",
            "Captured loss: 2.3975 at step 3847\n",
            "Logs at step 3848: {'loss': 2.5797, 'grad_norm': 2.508439540863037, 'learning_rate': 7.340873807939677e-06, 'epoch': 2.560212907518297}\n",
            "Captured loss: 2.5797 at step 3848\n",
            "Logs at step 3849: {'loss': 2.3793, 'grad_norm': 1.2886077165603638, 'learning_rate': 7.3297848746950546e-06, 'epoch': 2.560878243512974}\n",
            "Captured loss: 2.3793 at step 3849\n",
            "Logs at step 3850: {'loss': 2.3637, 'grad_norm': 1.7095303535461426, 'learning_rate': 7.318695941450433e-06, 'epoch': 2.561543579507651}\n",
            "Captured loss: 2.3637 at step 3850\n",
            "Logs at step 3851: {'loss': 2.53, 'grad_norm': 1.8343342542648315, 'learning_rate': 7.3076070082058105e-06, 'epoch': 2.5622089155023287}\n",
            "Captured loss: 2.53 at step 3851\n",
            "Logs at step 3852: {'loss': 2.4348, 'grad_norm': 1.7185418605804443, 'learning_rate': 7.29651807496119e-06, 'epoch': 2.562874251497006}\n",
            "Captured loss: 2.4348 at step 3852\n",
            "Logs at step 3853: {'loss': 2.424, 'grad_norm': 1.4209675788879395, 'learning_rate': 7.2854291417165665e-06, 'epoch': 2.5635395874916833}\n",
            "Captured loss: 2.424 at step 3853\n",
            "Logs at step 3854: {'loss': 2.3354, 'grad_norm': 1.6636584997177124, 'learning_rate': 7.274340208471946e-06, 'epoch': 2.5642049234863604}\n",
            "Captured loss: 2.3354 at step 3854\n",
            "Logs at step 3855: {'loss': 2.3387, 'grad_norm': 2.2997405529022217, 'learning_rate': 7.263251275227323e-06, 'epoch': 2.564870259481038}\n",
            "Captured loss: 2.3387 at step 3855\n",
            "Logs at step 3856: {'loss': 2.4418, 'grad_norm': 1.6030107736587524, 'learning_rate': 7.252162341982702e-06, 'epoch': 2.565535595475715}\n",
            "Captured loss: 2.4418 at step 3856\n",
            "Logs at step 3857: {'loss': 2.2832, 'grad_norm': 1.8205039501190186, 'learning_rate': 7.241073408738079e-06, 'epoch': 2.5662009314703926}\n",
            "Captured loss: 2.2832 at step 3857\n",
            "Logs at step 3858: {'loss': 2.41, 'grad_norm': 1.240384578704834, 'learning_rate': 7.229984475493458e-06, 'epoch': 2.5668662674650697}\n",
            "Captured loss: 2.41 at step 3858\n",
            "Logs at step 3859: {'loss': 2.3043, 'grad_norm': 2.047985792160034, 'learning_rate': 7.218895542248837e-06, 'epoch': 2.5675316034597473}\n",
            "Captured loss: 2.3043 at step 3859\n",
            "Logs at step 3860: {'loss': 2.2934, 'grad_norm': 1.5604963302612305, 'learning_rate': 7.207806609004214e-06, 'epoch': 2.5681969394544244}\n",
            "Captured loss: 2.2934 at step 3860\n",
            "Logs at step 3861: {'loss': 2.0298, 'grad_norm': 1.9076553583145142, 'learning_rate': 7.196717675759593e-06, 'epoch': 2.568862275449102}\n",
            "Captured loss: 2.0298 at step 3861\n",
            "Logs at step 3862: {'loss': 2.3381, 'grad_norm': 1.5253379344940186, 'learning_rate': 7.18562874251497e-06, 'epoch': 2.569527611443779}\n",
            "Captured loss: 2.3381 at step 3862\n",
            "Logs at step 3863: {'loss': 2.091, 'grad_norm': 1.8364911079406738, 'learning_rate': 7.174539809270349e-06, 'epoch': 2.570192947438456}\n",
            "Captured loss: 2.091 at step 3863\n",
            "Logs at step 3864: {'loss': 2.0924, 'grad_norm': 1.8023828268051147, 'learning_rate': 7.163450876025726e-06, 'epoch': 2.5708582834331337}\n",
            "Captured loss: 2.0924 at step 3864\n",
            "Logs at step 3865: {'loss': 2.3166, 'grad_norm': 1.7899576425552368, 'learning_rate': 7.152361942781105e-06, 'epoch': 2.5715236194278113}\n",
            "Captured loss: 2.3166 at step 3865\n",
            "Logs at step 3866: {'loss': 2.4626, 'grad_norm': 1.3735686540603638, 'learning_rate': 7.141273009536482e-06, 'epoch': 2.5721889554224884}\n",
            "Captured loss: 2.4626 at step 3866\n",
            "Logs at step 3867: {'loss': 2.1005, 'grad_norm': 1.6846370697021484, 'learning_rate': 7.1301840762918615e-06, 'epoch': 2.5728542914171655}\n",
            "Captured loss: 2.1005 at step 3867\n",
            "Logs at step 3868: {'loss': 2.309, 'grad_norm': 2.3516554832458496, 'learning_rate': 7.119095143047238e-06, 'epoch': 2.573519627411843}\n",
            "Captured loss: 2.309 at step 3868\n",
            "Logs at step 3869: {'loss': 2.3952, 'grad_norm': 1.9460400342941284, 'learning_rate': 7.1080062098026174e-06, 'epoch': 2.57418496340652}\n",
            "Captured loss: 2.3952 at step 3869\n",
            "Logs at step 3870: {'loss': 2.58, 'grad_norm': 1.8668853044509888, 'learning_rate': 7.096917276557996e-06, 'epoch': 2.5748502994011977}\n",
            "Captured loss: 2.58 at step 3870\n",
            "Logs at step 3871: {'loss': 2.3856, 'grad_norm': 1.4311211109161377, 'learning_rate': 7.085828343313373e-06, 'epoch': 2.575515635395875}\n",
            "Captured loss: 2.3856 at step 3871\n",
            "Logs at step 3872: {'loss': 1.8334, 'grad_norm': 2.1645348072052, 'learning_rate': 7.074739410068753e-06, 'epoch': 2.5761809713905524}\n",
            "Captured loss: 1.8334 at step 3872\n",
            "Logs at step 3873: {'loss': 2.5154, 'grad_norm': 1.7600030899047852, 'learning_rate': 7.063650476824129e-06, 'epoch': 2.5768463073852295}\n",
            "Captured loss: 2.5154 at step 3873\n",
            "Logs at step 3874: {'loss': 2.4291, 'grad_norm': 2.180098295211792, 'learning_rate': 7.052561543579509e-06, 'epoch': 2.577511643379907}\n",
            "Captured loss: 2.4291 at step 3874\n",
            "Logs at step 3875: {'loss': 2.5562, 'grad_norm': 1.641786813735962, 'learning_rate': 7.041472610334886e-06, 'epoch': 2.578176979374584}\n",
            "Captured loss: 2.5562 at step 3875\n",
            "Logs at step 3876: {'loss': 2.4319, 'grad_norm': 1.2510672807693481, 'learning_rate': 7.0303836770902645e-06, 'epoch': 2.5788423153692612}\n",
            "Captured loss: 2.4319 at step 3876\n",
            "Logs at step 3877: {'loss': 2.382, 'grad_norm': 2.977815628051758, 'learning_rate': 7.019294743845642e-06, 'epoch': 2.579507651363939}\n",
            "Captured loss: 2.382 at step 3877\n",
            "Logs at step 3878: {'loss': 2.4106, 'grad_norm': 1.1355820894241333, 'learning_rate': 7.0082058106010205e-06, 'epoch': 2.5801729873586163}\n",
            "Captured loss: 2.4106 at step 3878\n",
            "Logs at step 3879: {'loss': 2.314, 'grad_norm': 1.3459001779556274, 'learning_rate': 6.997116877356398e-06, 'epoch': 2.5808383233532934}\n",
            "Captured loss: 2.314 at step 3879\n",
            "Logs at step 3880: {'loss': 2.4607, 'grad_norm': 1.2880580425262451, 'learning_rate': 6.986027944111776e-06, 'epoch': 2.5815036593479705}\n",
            "Captured loss: 2.4607 at step 3880\n",
            "Logs at step 3881: {'loss': 1.6592, 'grad_norm': 2.3588736057281494, 'learning_rate': 6.974939010867156e-06, 'epoch': 2.582168995342648}\n",
            "Captured loss: 1.6592 at step 3881\n",
            "Logs at step 3882: {'loss': 2.4016, 'grad_norm': 1.6263666152954102, 'learning_rate': 6.963850077622533e-06, 'epoch': 2.582834331337325}\n",
            "Captured loss: 2.4016 at step 3882\n",
            "Logs at step 3883: {'loss': 2.3088, 'grad_norm': 1.8430285453796387, 'learning_rate': 6.952761144377912e-06, 'epoch': 2.5834996673320028}\n",
            "Captured loss: 2.3088 at step 3883\n",
            "Logs at step 3884: {'loss': 2.281, 'grad_norm': 1.3791019916534424, 'learning_rate': 6.941672211133289e-06, 'epoch': 2.58416500332668}\n",
            "Captured loss: 2.281 at step 3884\n",
            "Logs at step 3885: {'loss': 2.3307, 'grad_norm': 2.2841105461120605, 'learning_rate': 6.9305832778886676e-06, 'epoch': 2.5848303393213574}\n",
            "Captured loss: 2.3307 at step 3885\n",
            "Logs at step 3886: {'loss': 1.7958, 'grad_norm': 2.0194530487060547, 'learning_rate': 6.919494344644045e-06, 'epoch': 2.5854956753160345}\n",
            "Captured loss: 1.7958 at step 3886\n",
            "Logs at step 3887: {'loss': 2.2783, 'grad_norm': 2.143568754196167, 'learning_rate': 6.908405411399424e-06, 'epoch': 2.586161011310712}\n",
            "Captured loss: 2.2783 at step 3887\n",
            "Logs at step 3888: {'loss': 1.9739, 'grad_norm': 1.6814072132110596, 'learning_rate': 6.897316478154801e-06, 'epoch': 2.586826347305389}\n",
            "Captured loss: 1.9739 at step 3888\n",
            "Logs at step 3889: {'loss': 2.1864, 'grad_norm': 2.2971277236938477, 'learning_rate': 6.88622754491018e-06, 'epoch': 2.5874916833000663}\n",
            "Captured loss: 2.1864 at step 3889\n",
            "Logs at step 3890: {'loss': 2.6539, 'grad_norm': 1.6369532346725464, 'learning_rate': 6.875138611665558e-06, 'epoch': 2.588157019294744}\n",
            "Captured loss: 2.6539 at step 3890\n",
            "Logs at step 3891: {'loss': 1.7046, 'grad_norm': 2.6143970489501953, 'learning_rate': 6.864049678420936e-06, 'epoch': 2.5888223552894214}\n",
            "Captured loss: 1.7046 at step 3891\n",
            "Logs at step 3892: {'loss': 2.4267, 'grad_norm': 1.202311396598816, 'learning_rate': 6.852960745176315e-06, 'epoch': 2.5894876912840985}\n",
            "Captured loss: 2.4267 at step 3892\n",
            "Logs at step 3893: {'loss': 2.4247, 'grad_norm': 1.4338161945343018, 'learning_rate': 6.841871811931692e-06, 'epoch': 2.5901530272787756}\n",
            "Captured loss: 2.4247 at step 3893\n",
            "Logs at step 3894: {'loss': 1.5425, 'grad_norm': 2.0231382846832275, 'learning_rate': 6.8307828786870714e-06, 'epoch': 2.590818363273453}\n",
            "Captured loss: 1.5425 at step 3894\n",
            "Logs at step 3895: {'loss': 2.5003, 'grad_norm': 1.2154910564422607, 'learning_rate': 6.819693945442448e-06, 'epoch': 2.5914836992681303}\n",
            "Captured loss: 2.5003 at step 3895\n",
            "Logs at step 3896: {'loss': 1.8017, 'grad_norm': 1.8052514791488647, 'learning_rate': 6.808605012197827e-06, 'epoch': 2.592149035262808}\n",
            "Captured loss: 1.8017 at step 3896\n",
            "Logs at step 3897: {'loss': 2.4917, 'grad_norm': 1.0771033763885498, 'learning_rate': 6.797516078953205e-06, 'epoch': 2.592814371257485}\n",
            "Captured loss: 2.4917 at step 3897\n",
            "Logs at step 3898: {'loss': 2.6039, 'grad_norm': 2.8641817569732666, 'learning_rate': 6.786427145708583e-06, 'epoch': 2.5934797072521625}\n",
            "Captured loss: 2.6039 at step 3898\n",
            "Logs at step 3899: {'loss': 1.5314, 'grad_norm': 1.9736300706863403, 'learning_rate': 6.775338212463961e-06, 'epoch': 2.5941450432468396}\n",
            "Captured loss: 1.5314 at step 3899\n",
            "Logs at step 3900: {'loss': 2.2728, 'grad_norm': 2.07319974899292, 'learning_rate': 6.764249279219339e-06, 'epoch': 2.594810379241517}\n",
            "Captured loss: 2.2728 at step 3900\n",
            "Logs at step 3901: {'loss': 2.4194, 'grad_norm': 1.5202339887619019, 'learning_rate': 6.753160345974717e-06, 'epoch': 2.5954757152361942}\n",
            "Captured loss: 2.4194 at step 3901\n",
            "Logs at step 3902: {'loss': 2.1054, 'grad_norm': 1.9307743310928345, 'learning_rate': 6.742071412730096e-06, 'epoch': 2.5961410512308714}\n",
            "Captured loss: 2.1054 at step 3902\n",
            "Logs at step 3903: {'loss': 2.5203, 'grad_norm': 1.7211453914642334, 'learning_rate': 6.7309824794854745e-06, 'epoch': 2.596806387225549}\n",
            "Captured loss: 2.5203 at step 3903\n",
            "Logs at step 3904: {'loss': 2.1794, 'grad_norm': 1.7517880201339722, 'learning_rate': 6.719893546240852e-06, 'epoch': 2.5974717232202265}\n",
            "Captured loss: 2.1794 at step 3904\n",
            "Logs at step 3905: {'loss': 2.1823, 'grad_norm': 1.7996745109558105, 'learning_rate': 6.7088046129962304e-06, 'epoch': 2.5981370592149036}\n",
            "Captured loss: 2.1823 at step 3905\n",
            "Logs at step 3906: {'loss': 2.5022, 'grad_norm': 1.2773693799972534, 'learning_rate': 6.697715679751608e-06, 'epoch': 2.5988023952095807}\n",
            "Captured loss: 2.5022 at step 3906\n",
            "Logs at step 3907: {'loss': 2.297, 'grad_norm': 1.411215901374817, 'learning_rate': 6.686626746506986e-06, 'epoch': 2.5994677312042582}\n",
            "Captured loss: 2.297 at step 3907\n",
            "Logs at step 3908: {'loss': 2.1382, 'grad_norm': 1.580898642539978, 'learning_rate': 6.675537813262364e-06, 'epoch': 2.6001330671989353}\n",
            "Captured loss: 2.1382 at step 3908\n",
            "Logs at step 3909: {'loss': 2.4838, 'grad_norm': 1.425696849822998, 'learning_rate': 6.664448880017743e-06, 'epoch': 2.600798403193613}\n",
            "Captured loss: 2.4838 at step 3909\n",
            "Logs at step 3910: {'loss': 2.3905, 'grad_norm': 2.49617075920105, 'learning_rate': 6.65335994677312e-06, 'epoch': 2.60146373918829}\n",
            "Captured loss: 2.3905 at step 3910\n",
            "Logs at step 3911: {'loss': 2.5698, 'grad_norm': 1.4869290590286255, 'learning_rate': 6.642271013528499e-06, 'epoch': 2.6021290751829675}\n",
            "Captured loss: 2.5698 at step 3911\n",
            "Logs at step 3912: {'loss': 2.5503, 'grad_norm': 1.7653905153274536, 'learning_rate': 6.631182080283877e-06, 'epoch': 2.6027944111776447}\n",
            "Captured loss: 2.5503 at step 3912\n",
            "Logs at step 3913: {'loss': 2.263, 'grad_norm': 2.0033130645751953, 'learning_rate': 6.620093147039255e-06, 'epoch': 2.603459747172322}\n",
            "Captured loss: 2.263 at step 3913\n",
            "Logs at step 3914: {'loss': 2.5694, 'grad_norm': 1.2776943445205688, 'learning_rate': 6.609004213794633e-06, 'epoch': 2.6041250831669993}\n",
            "Captured loss: 2.5694 at step 3914\n",
            "Logs at step 3915: {'loss': 2.3015, 'grad_norm': 2.38883113861084, 'learning_rate': 6.597915280550011e-06, 'epoch': 2.6047904191616764}\n",
            "Captured loss: 2.3015 at step 3915\n",
            "Logs at step 3916: {'loss': 2.3229, 'grad_norm': 1.421047329902649, 'learning_rate': 6.58682634730539e-06, 'epoch': 2.605455755156354}\n",
            "Captured loss: 2.3229 at step 3916\n",
            "Logs at step 3917: {'loss': 2.0364, 'grad_norm': 1.8592084646224976, 'learning_rate': 6.575737414060768e-06, 'epoch': 2.6061210911510315}\n",
            "Captured loss: 2.0364 at step 3917\n",
            "Logs at step 3918: {'loss': 2.3127, 'grad_norm': 1.199476718902588, 'learning_rate': 6.564648480816146e-06, 'epoch': 2.6067864271457086}\n",
            "Captured loss: 2.3127 at step 3918\n",
            "Logs at step 3919: {'loss': 2.5056, 'grad_norm': 1.6489225625991821, 'learning_rate': 6.553559547571524e-06, 'epoch': 2.6074517631403857}\n",
            "Captured loss: 2.5056 at step 3919\n",
            "Logs at step 3920: {'loss': 2.4927, 'grad_norm': 1.3452695608139038, 'learning_rate': 6.542470614326902e-06, 'epoch': 2.6081170991350633}\n",
            "Captured loss: 2.4927 at step 3920\n",
            "Logs at step 3921: {'loss': 2.5228, 'grad_norm': 1.4802298545837402, 'learning_rate': 6.53138168108228e-06, 'epoch': 2.6087824351297404}\n",
            "Captured loss: 2.5228 at step 3921\n",
            "Logs at step 3922: {'loss': 2.4671, 'grad_norm': 1.1894781589508057, 'learning_rate': 6.520292747837659e-06, 'epoch': 2.609447771124418}\n",
            "Captured loss: 2.4671 at step 3922\n",
            "Logs at step 3923: {'loss': 2.5474, 'grad_norm': 1.3510417938232422, 'learning_rate': 6.509203814593036e-06, 'epoch': 2.610113107119095}\n",
            "Captured loss: 2.5474 at step 3923\n",
            "Logs at step 3924: {'loss': 2.3612, 'grad_norm': 1.9096994400024414, 'learning_rate': 6.498114881348415e-06, 'epoch': 2.6107784431137726}\n",
            "Captured loss: 2.3612 at step 3924\n",
            "Logs at step 3925: {'loss': 2.443, 'grad_norm': 1.087565302848816, 'learning_rate': 6.4870259481037925e-06, 'epoch': 2.6114437791084497}\n",
            "Captured loss: 2.443 at step 3925\n",
            "Logs at step 3926: {'loss': 2.5877, 'grad_norm': 1.33684241771698, 'learning_rate': 6.475937014859171e-06, 'epoch': 2.6121091151031273}\n",
            "Captured loss: 2.5877 at step 3926\n",
            "Logs at step 3927: {'loss': 2.5067, 'grad_norm': 1.55803644657135, 'learning_rate': 6.464848081614549e-06, 'epoch': 2.6127744510978044}\n",
            "Captured loss: 2.5067 at step 3927\n",
            "Logs at step 3928: {'loss': 1.6081, 'grad_norm': 2.428906202316284, 'learning_rate': 6.453759148369927e-06, 'epoch': 2.6134397870924815}\n",
            "Captured loss: 1.6081 at step 3928\n",
            "Logs at step 3929: {'loss': 1.5802, 'grad_norm': 2.5933287143707275, 'learning_rate': 6.442670215125306e-06, 'epoch': 2.614105123087159}\n",
            "Captured loss: 1.5802 at step 3929\n",
            "Logs at step 3930: {'loss': 2.351, 'grad_norm': 1.6574361324310303, 'learning_rate': 6.431581281880683e-06, 'epoch': 2.6147704590818366}\n",
            "Captured loss: 2.351 at step 3930\n",
            "Logs at step 3931: {'loss': 2.4609, 'grad_norm': 1.2804653644561768, 'learning_rate': 6.420492348636062e-06, 'epoch': 2.6154357950765137}\n",
            "Captured loss: 2.4609 at step 3931\n",
            "Logs at step 3932: {'loss': 2.2949, 'grad_norm': 1.465285062789917, 'learning_rate': 6.4094034153914395e-06, 'epoch': 2.616101131071191}\n",
            "Captured loss: 2.2949 at step 3932\n",
            "Logs at step 3933: {'loss': 2.1592, 'grad_norm': 1.8187514543533325, 'learning_rate': 6.398314482146818e-06, 'epoch': 2.6167664670658684}\n",
            "Captured loss: 2.1592 at step 3933\n",
            "Logs at step 3934: {'loss': 2.3964, 'grad_norm': 1.5751101970672607, 'learning_rate': 6.3872255489021955e-06, 'epoch': 2.6174318030605455}\n",
            "Captured loss: 2.3964 at step 3934\n",
            "Logs at step 3935: {'loss': 2.55, 'grad_norm': 1.7534716129302979, 'learning_rate': 6.376136615657574e-06, 'epoch': 2.618097139055223}\n",
            "Captured loss: 2.55 at step 3935\n",
            "Logs at step 3936: {'loss': 2.5143, 'grad_norm': 1.8151849508285522, 'learning_rate': 6.3650476824129514e-06, 'epoch': 2.6187624750499}\n",
            "Captured loss: 2.5143 at step 3936\n",
            "Logs at step 3937: {'loss': 2.6553, 'grad_norm': 1.7198576927185059, 'learning_rate': 6.353958749168331e-06, 'epoch': 2.6194278110445777}\n",
            "Captured loss: 2.6553 at step 3937\n",
            "Logs at step 3938: {'loss': 2.0749, 'grad_norm': 2.5021097660064697, 'learning_rate': 6.342869815923709e-06, 'epoch': 2.6200931470392548}\n",
            "Captured loss: 2.0749 at step 3938\n",
            "Logs at step 3939: {'loss': 2.4528, 'grad_norm': 1.376874566078186, 'learning_rate': 6.331780882679087e-06, 'epoch': 2.6207584830339323}\n",
            "Captured loss: 2.4528 at step 3939\n",
            "Logs at step 3940: {'loss': 2.3601, 'grad_norm': 1.2318707704544067, 'learning_rate': 6.320691949434465e-06, 'epoch': 2.6214238190286094}\n",
            "Captured loss: 2.3601 at step 3940\n",
            "Logs at step 3941: {'loss': 2.6511, 'grad_norm': 1.8805044889450073, 'learning_rate': 6.309603016189843e-06, 'epoch': 2.6220891550232865}\n",
            "Captured loss: 2.6511 at step 3941\n",
            "Logs at step 3942: {'loss': 1.8842, 'grad_norm': 2.0046393871307373, 'learning_rate': 6.298514082945221e-06, 'epoch': 2.622754491017964}\n",
            "Captured loss: 1.8842 at step 3942\n",
            "Logs at step 3943: {'loss': 2.0235, 'grad_norm': 1.6452308893203735, 'learning_rate': 6.2874251497005985e-06, 'epoch': 2.6234198270126416}\n",
            "Captured loss: 2.0235 at step 3943\n",
            "Logs at step 3944: {'loss': 2.34, 'grad_norm': 1.832263469696045, 'learning_rate': 6.276336216455978e-06, 'epoch': 2.6240851630073188}\n",
            "Captured loss: 2.34 at step 3944\n",
            "Logs at step 3945: {'loss': 2.4115, 'grad_norm': 1.3982552289962769, 'learning_rate': 6.2652472832113545e-06, 'epoch': 2.624750499001996}\n",
            "Captured loss: 2.4115 at step 3945\n",
            "Logs at step 3946: {'loss': 2.454, 'grad_norm': 1.2068817615509033, 'learning_rate': 6.254158349966734e-06, 'epoch': 2.6254158349966734}\n",
            "Captured loss: 2.454 at step 3946\n",
            "Logs at step 3947: {'loss': 2.2152, 'grad_norm': 1.7658171653747559, 'learning_rate': 6.243069416722111e-06, 'epoch': 2.6260811709913505}\n",
            "Captured loss: 2.2152 at step 3947\n",
            "Logs at step 3948: {'loss': 2.3128, 'grad_norm': 1.803990364074707, 'learning_rate': 6.23198048347749e-06, 'epoch': 2.626746506986028}\n",
            "Captured loss: 2.3128 at step 3948\n",
            "Logs at step 3949: {'loss': 2.2864, 'grad_norm': 1.4609307050704956, 'learning_rate': 6.220891550232868e-06, 'epoch': 2.627411842980705}\n",
            "Captured loss: 2.2864 at step 3949\n",
            "Logs at step 3950: {'loss': 2.3008, 'grad_norm': 2.2034196853637695, 'learning_rate': 6.209802616988246e-06, 'epoch': 2.6280771789753827}\n",
            "Captured loss: 2.3008 at step 3950\n",
            "Logs at step 3951: {'loss': 1.8362, 'grad_norm': 2.6931300163269043, 'learning_rate': 6.198713683743624e-06, 'epoch': 2.62874251497006}\n",
            "Captured loss: 1.8362 at step 3951\n",
            "Logs at step 3952: {'loss': 2.5293, 'grad_norm': 2.1785454750061035, 'learning_rate': 6.187624750499002e-06, 'epoch': 2.6294078509647374}\n",
            "Captured loss: 2.5293 at step 3952\n",
            "Logs at step 3953: {'loss': 2.3694, 'grad_norm': 1.6956144571304321, 'learning_rate': 6.17653581725438e-06, 'epoch': 2.6300731869594145}\n",
            "Captured loss: 2.3694 at step 3953\n",
            "Logs at step 3954: {'loss': 2.3948, 'grad_norm': 1.808261513710022, 'learning_rate': 6.165446884009759e-06, 'epoch': 2.6307385229540916}\n",
            "Captured loss: 2.3948 at step 3954\n",
            "Logs at step 3955: {'loss': 2.3977, 'grad_norm': 2.5792858600616455, 'learning_rate': 6.154357950765137e-06, 'epoch': 2.631403858948769}\n",
            "Captured loss: 2.3977 at step 3955\n",
            "Logs at step 3956: {'loss': 2.4771, 'grad_norm': 1.1464087963104248, 'learning_rate': 6.143269017520515e-06, 'epoch': 2.6320691949434467}\n",
            "Captured loss: 2.4771 at step 3956\n",
            "Logs at step 3957: {'loss': 2.4737, 'grad_norm': 1.6653308868408203, 'learning_rate': 6.132180084275893e-06, 'epoch': 2.632734530938124}\n",
            "Captured loss: 2.4737 at step 3957\n",
            "Logs at step 3958: {'loss': 2.0766, 'grad_norm': 1.8232544660568237, 'learning_rate': 6.121091151031271e-06, 'epoch': 2.633399866932801}\n",
            "Captured loss: 2.0766 at step 3958\n",
            "Logs at step 3959: {'loss': 2.0147, 'grad_norm': 1.6635160446166992, 'learning_rate': 6.1100022177866495e-06, 'epoch': 2.6340652029274785}\n",
            "Captured loss: 2.0147 at step 3959\n",
            "Logs at step 3960: {'loss': 2.6079, 'grad_norm': 1.7804657220840454, 'learning_rate': 6.098913284542027e-06, 'epoch': 2.6347305389221556}\n",
            "Captured loss: 2.6079 at step 3960\n",
            "Logs at step 3961: {'loss': 2.3492, 'grad_norm': 2.0572993755340576, 'learning_rate': 6.0878243512974054e-06, 'epoch': 2.635395874916833}\n",
            "Captured loss: 2.3492 at step 3961\n",
            "Logs at step 3962: {'loss': 1.9751, 'grad_norm': 2.2219200134277344, 'learning_rate': 6.076735418052784e-06, 'epoch': 2.6360612109115102}\n",
            "Captured loss: 1.9751 at step 3962\n",
            "Logs at step 3963: {'loss': 2.122, 'grad_norm': 1.6807732582092285, 'learning_rate': 6.065646484808161e-06, 'epoch': 2.6367265469061874}\n",
            "Captured loss: 2.122 at step 3963\n",
            "Logs at step 3964: {'loss': 1.9739, 'grad_norm': 1.4442012310028076, 'learning_rate': 6.05455755156354e-06, 'epoch': 2.637391882900865}\n",
            "Captured loss: 1.9739 at step 3964\n",
            "Logs at step 3965: {'loss': 2.4393, 'grad_norm': 1.8729965686798096, 'learning_rate': 6.043468618318918e-06, 'epoch': 2.6380572188955425}\n",
            "Captured loss: 2.4393 at step 3965\n",
            "Logs at step 3966: {'loss': 2.5618, 'grad_norm': 1.6969085931777954, 'learning_rate': 6.032379685074297e-06, 'epoch': 2.6387225548902196}\n",
            "Captured loss: 2.5618 at step 3966\n",
            "Logs at step 3967: {'loss': 2.4396, 'grad_norm': 2.455203056335449, 'learning_rate': 6.021290751829674e-06, 'epoch': 2.6393878908848967}\n",
            "Captured loss: 2.4396 at step 3967\n",
            "Logs at step 3968: {'loss': 2.4686, 'grad_norm': 1.4193569421768188, 'learning_rate': 6.0102018185850525e-06, 'epoch': 2.640053226879574}\n",
            "Captured loss: 2.4686 at step 3968\n",
            "Logs at step 3969: {'loss': 2.3736, 'grad_norm': 1.4878602027893066, 'learning_rate': 5.999112885340431e-06, 'epoch': 2.6407185628742518}\n",
            "Captured loss: 2.3736 at step 3969\n",
            "Logs at step 3970: {'loss': 2.6802, 'grad_norm': 1.714682936668396, 'learning_rate': 5.9880239520958085e-06, 'epoch': 2.641383898868929}\n",
            "Captured loss: 2.6802 at step 3970\n",
            "Logs at step 3971: {'loss': 2.4191, 'grad_norm': 1.2507352828979492, 'learning_rate': 5.976935018851187e-06, 'epoch': 2.642049234863606}\n",
            "Captured loss: 2.4191 at step 3971\n",
            "Logs at step 3972: {'loss': 2.3624, 'grad_norm': 1.1433216333389282, 'learning_rate': 5.9658460856065644e-06, 'epoch': 2.6427145708582835}\n",
            "Captured loss: 2.3624 at step 3972\n",
            "Logs at step 3973: {'loss': 2.2412, 'grad_norm': 1.6913175582885742, 'learning_rate': 5.954757152361943e-06, 'epoch': 2.6433799068529606}\n",
            "Captured loss: 2.2412 at step 3973\n",
            "Logs at step 3974: {'loss': 2.5539, 'grad_norm': 1.5463250875473022, 'learning_rate': 5.943668219117321e-06, 'epoch': 2.644045242847638}\n",
            "Captured loss: 2.5539 at step 3974\n",
            "Logs at step 3975: {'loss': 1.6351, 'grad_norm': 2.2581748962402344, 'learning_rate': 5.932579285872699e-06, 'epoch': 2.6447105788423153}\n",
            "Captured loss: 1.6351 at step 3975\n",
            "Logs at step 3976: {'loss': 2.2595, 'grad_norm': 1.6798782348632812, 'learning_rate': 5.921490352628078e-06, 'epoch': 2.6453759148369924}\n",
            "Captured loss: 2.2595 at step 3976\n",
            "Logs at step 3977: {'loss': 2.3066, 'grad_norm': 1.6619967222213745, 'learning_rate': 5.9104014193834556e-06, 'epoch': 2.64604125083167}\n",
            "Captured loss: 2.3066 at step 3977\n",
            "Logs at step 3978: {'loss': 2.1377, 'grad_norm': 2.4254066944122314, 'learning_rate': 5.899312486138834e-06, 'epoch': 2.6467065868263475}\n",
            "Captured loss: 2.1377 at step 3978\n",
            "Logs at step 3979: {'loss': 2.4346, 'grad_norm': 1.5839526653289795, 'learning_rate': 5.888223552894212e-06, 'epoch': 2.6473719228210246}\n",
            "Captured loss: 2.4346 at step 3979\n",
            "Logs at step 3980: {'loss': 2.4578, 'grad_norm': 1.205525517463684, 'learning_rate': 5.87713461964959e-06, 'epoch': 2.6480372588157017}\n",
            "Captured loss: 2.4578 at step 3980\n",
            "Logs at step 3981: {'loss': 2.4314, 'grad_norm': 1.6816093921661377, 'learning_rate': 5.866045686404968e-06, 'epoch': 2.6487025948103793}\n",
            "Captured loss: 2.4314 at step 3981\n",
            "Logs at step 3982: {'loss': 2.5174, 'grad_norm': 1.7903780937194824, 'learning_rate': 5.854956753160346e-06, 'epoch': 2.6493679308050564}\n",
            "Captured loss: 2.5174 at step 3982\n",
            "Logs at step 3983: {'loss': 2.189, 'grad_norm': 1.8508524894714355, 'learning_rate': 5.843867819915724e-06, 'epoch': 2.650033266799734}\n",
            "Captured loss: 2.189 at step 3983\n",
            "Logs at step 3984: {'loss': 2.2203, 'grad_norm': 1.3840749263763428, 'learning_rate': 5.832778886671103e-06, 'epoch': 2.650698602794411}\n",
            "Captured loss: 2.2203 at step 3984\n",
            "Logs at step 3985: {'loss': 2.2186, 'grad_norm': 2.613673448562622, 'learning_rate': 5.82168995342648e-06, 'epoch': 2.6513639387890886}\n",
            "Captured loss: 2.2186 at step 3985\n",
            "Logs at step 3986: {'loss': 2.3101, 'grad_norm': 1.9457659721374512, 'learning_rate': 5.810601020181859e-06, 'epoch': 2.6520292747837657}\n",
            "Captured loss: 2.3101 at step 3986\n",
            "Logs at step 3987: {'loss': 2.3044, 'grad_norm': 1.6896952390670776, 'learning_rate': 5.799512086937236e-06, 'epoch': 2.6526946107784433}\n",
            "Captured loss: 2.3044 at step 3987\n",
            "Logs at step 3988: {'loss': 2.5321, 'grad_norm': 2.2201905250549316, 'learning_rate': 5.788423153692615e-06, 'epoch': 2.6533599467731204}\n",
            "Captured loss: 2.5321 at step 3988\n",
            "Logs at step 3989: {'loss': 2.2374, 'grad_norm': 1.459743857383728, 'learning_rate': 5.777334220447994e-06, 'epoch': 2.6540252827677975}\n",
            "Captured loss: 2.2374 at step 3989\n",
            "Logs at step 3990: {'loss': 2.3818, 'grad_norm': 2.1587977409362793, 'learning_rate': 5.766245287203371e-06, 'epoch': 2.654690618762475}\n",
            "Captured loss: 2.3818 at step 3990\n",
            "Logs at step 3991: {'loss': 2.221, 'grad_norm': 2.3176329135894775, 'learning_rate': 5.75515635395875e-06, 'epoch': 2.6553559547571526}\n",
            "Captured loss: 2.221 at step 3991\n",
            "Logs at step 3992: {'loss': 2.5959, 'grad_norm': 2.1517415046691895, 'learning_rate': 5.744067420714127e-06, 'epoch': 2.6560212907518297}\n",
            "Captured loss: 2.5959 at step 3992\n",
            "Logs at step 3993: {'loss': 2.5709, 'grad_norm': 1.5037263631820679, 'learning_rate': 5.732978487469506e-06, 'epoch': 2.656686626746507}\n",
            "Captured loss: 2.5709 at step 3993\n",
            "Logs at step 3994: {'loss': 1.791, 'grad_norm': 1.708228588104248, 'learning_rate': 5.721889554224884e-06, 'epoch': 2.6573519627411843}\n",
            "Captured loss: 1.791 at step 3994\n",
            "Logs at step 3995: {'loss': 2.5091, 'grad_norm': 1.3530396223068237, 'learning_rate': 5.710800620980262e-06, 'epoch': 2.6580172987358615}\n",
            "Captured loss: 2.5091 at step 3995\n",
            "Logs at step 3996: {'loss': 1.9524, 'grad_norm': 1.8056137561798096, 'learning_rate': 5.69971168773564e-06, 'epoch': 2.658682634730539}\n",
            "Captured loss: 1.9524 at step 3996\n",
            "Logs at step 3997: {'loss': 2.516, 'grad_norm': 1.0509965419769287, 'learning_rate': 5.688622754491018e-06, 'epoch': 2.659347970725216}\n",
            "Captured loss: 2.516 at step 3997\n",
            "Logs at step 3998: {'loss': 2.5493, 'grad_norm': 1.3737305402755737, 'learning_rate': 5.677533821246396e-06, 'epoch': 2.6600133067198937}\n",
            "Captured loss: 2.5493 at step 3998\n",
            "Logs at step 3999: {'loss': 2.5325, 'grad_norm': 1.592939019203186, 'learning_rate': 5.666444888001774e-06, 'epoch': 2.6606786427145708}\n",
            "Captured loss: 2.5325 at step 3999\n",
            "Logs at step 4000: {'loss': 2.1487, 'grad_norm': 2.6638906002044678, 'learning_rate': 5.655355954757153e-06, 'epoch': 2.6613439787092483}\n",
            "Captured loss: 2.1487 at step 4000\n",
            "Logs at step 4001: {'loss': 2.2556, 'grad_norm': 1.5900938510894775, 'learning_rate': 5.644267021512531e-06, 'epoch': 2.6620093147039254}\n",
            "Captured loss: 2.2556 at step 4001\n",
            "Logs at step 4002: {'loss': 2.2773, 'grad_norm': 1.5341567993164062, 'learning_rate': 5.633178088267909e-06, 'epoch': 2.6626746506986025}\n",
            "Captured loss: 2.2773 at step 4002\n",
            "Logs at step 4003: {'loss': 2.6178, 'grad_norm': 1.3428226709365845, 'learning_rate': 5.622089155023287e-06, 'epoch': 2.66333998669328}\n",
            "Captured loss: 2.6178 at step 4003\n",
            "Logs at step 4004: {'loss': 2.3811, 'grad_norm': 1.8395785093307495, 'learning_rate': 5.6110002217786655e-06, 'epoch': 2.6640053226879576}\n",
            "Captured loss: 2.3811 at step 4004\n",
            "Logs at step 4005: {'loss': 2.4253, 'grad_norm': 1.1945538520812988, 'learning_rate': 5.599911288534043e-06, 'epoch': 2.6646706586826348}\n",
            "Captured loss: 2.4253 at step 4005\n",
            "Logs at step 4006: {'loss': 2.5589, 'grad_norm': 2.053122043609619, 'learning_rate': 5.5888223552894215e-06, 'epoch': 2.665335994677312}\n",
            "Captured loss: 2.5589 at step 4006\n",
            "Logs at step 4007: {'loss': 2.382, 'grad_norm': 1.3338987827301025, 'learning_rate': 5.577733422044799e-06, 'epoch': 2.6660013306719894}\n",
            "Captured loss: 2.382 at step 4007\n",
            "Logs at step 4008: {'loss': 1.9812, 'grad_norm': 1.7195720672607422, 'learning_rate': 5.5666444888001774e-06, 'epoch': 2.6666666666666665}\n",
            "Captured loss: 1.9812 at step 4008\n",
            "Logs at step 4009: {'loss': 2.377, 'grad_norm': 1.1263468265533447, 'learning_rate': 5.555555555555556e-06, 'epoch': 2.667332002661344}\n",
            "Captured loss: 2.377 at step 4009\n",
            "Logs at step 4010: {'loss': 2.0085, 'grad_norm': 2.5234642028808594, 'learning_rate': 5.544466622310934e-06, 'epoch': 2.667997338656021}\n",
            "Captured loss: 2.0085 at step 4010\n",
            "Logs at step 4011: {'loss': 1.9517, 'grad_norm': 2.337423086166382, 'learning_rate': 5.533377689066313e-06, 'epoch': 2.6686626746506987}\n",
            "Captured loss: 1.9517 at step 4011\n",
            "Logs at step 4012: {'loss': 1.9047, 'grad_norm': 1.925931692123413, 'learning_rate': 5.52228875582169e-06, 'epoch': 2.669328010645376}\n",
            "Captured loss: 1.9047 at step 4012\n",
            "Logs at step 4013: {'loss': 2.3838, 'grad_norm': 1.40105402469635, 'learning_rate': 5.5111998225770686e-06, 'epoch': 2.6699933466400534}\n",
            "Captured loss: 2.3838 at step 4013\n",
            "Logs at step 4014: {'loss': 2.3961, 'grad_norm': 1.1942957639694214, 'learning_rate': 5.500110889332447e-06, 'epoch': 2.6706586826347305}\n",
            "Captured loss: 2.3961 at step 4014\n",
            "Logs at step 4015: {'loss': 2.269, 'grad_norm': 1.579809308052063, 'learning_rate': 5.4890219560878245e-06, 'epoch': 2.6713240186294076}\n",
            "Captured loss: 2.269 at step 4015\n",
            "Logs at step 4016: {'loss': 2.3651, 'grad_norm': 2.1837027072906494, 'learning_rate': 5.477933022843203e-06, 'epoch': 2.671989354624085}\n",
            "Captured loss: 2.3651 at step 4016\n",
            "Logs at step 4017: {'loss': 2.3404, 'grad_norm': 1.9367815256118774, 'learning_rate': 5.4668440895985805e-06, 'epoch': 2.6726546906187627}\n",
            "Captured loss: 2.3404 at step 4017\n",
            "Logs at step 4018: {'loss': 1.9536, 'grad_norm': 2.102290391921997, 'learning_rate': 5.455755156353959e-06, 'epoch': 2.67332002661344}\n",
            "Captured loss: 1.9536 at step 4018\n",
            "Logs at step 4019: {'loss': 1.9115, 'grad_norm': 2.513381004333496, 'learning_rate': 5.444666223109337e-06, 'epoch': 2.673985362608117}\n",
            "Captured loss: 1.9115 at step 4019\n",
            "Logs at step 4020: {'loss': 2.2515, 'grad_norm': 1.2827200889587402, 'learning_rate': 5.433577289864715e-06, 'epoch': 2.6746506986027945}\n",
            "Captured loss: 2.2515 at step 4020\n",
            "Logs at step 4021: {'loss': 2.7328, 'grad_norm': 3.263031482696533, 'learning_rate': 5.422488356620094e-06, 'epoch': 2.6753160345974716}\n",
            "Captured loss: 2.7328 at step 4021\n",
            "Logs at step 4022: {'loss': 2.5133, 'grad_norm': 1.1921899318695068, 'learning_rate': 5.411399423375472e-06, 'epoch': 2.675981370592149}\n",
            "Captured loss: 2.5133 at step 4022\n",
            "Logs at step 4023: {'loss': 2.4768, 'grad_norm': 1.9397029876708984, 'learning_rate': 5.40031049013085e-06, 'epoch': 2.6766467065868262}\n",
            "Captured loss: 2.4768 at step 4023\n",
            "Logs at step 4024: {'loss': 2.4163, 'grad_norm': 2.240995407104492, 'learning_rate': 5.3892215568862275e-06, 'epoch': 2.677312042581504}\n",
            "Captured loss: 2.4163 at step 4024\n",
            "Logs at step 4025: {'loss': 2.2842, 'grad_norm': 2.1585538387298584, 'learning_rate': 5.378132623641606e-06, 'epoch': 2.677977378576181}\n",
            "Captured loss: 2.2842 at step 4025\n",
            "Logs at step 4026: {'loss': 2.358, 'grad_norm': 3.4943788051605225, 'learning_rate': 5.367043690396984e-06, 'epoch': 2.6786427145708585}\n",
            "Captured loss: 2.358 at step 4026\n",
            "Logs at step 4027: {'loss': 2.3991, 'grad_norm': 1.343693494796753, 'learning_rate': 5.355954757152362e-06, 'epoch': 2.6793080505655356}\n",
            "Captured loss: 2.3991 at step 4027\n",
            "Logs at step 4028: {'loss': 2.526, 'grad_norm': 1.5145472288131714, 'learning_rate': 5.34486582390774e-06, 'epoch': 2.6799733865602127}\n",
            "Captured loss: 2.526 at step 4028\n",
            "Logs at step 4029: {'loss': 1.9418, 'grad_norm': 1.695888876914978, 'learning_rate': 5.333776890663119e-06, 'epoch': 2.68063872255489}\n",
            "Captured loss: 1.9418 at step 4029\n",
            "Logs at step 4030: {'loss': 1.8842, 'grad_norm': 1.8956001996994019, 'learning_rate': 5.322687957418496e-06, 'epoch': 2.6813040585495678}\n",
            "Captured loss: 1.8842 at step 4030\n",
            "Logs at step 4031: {'loss': 2.2353, 'grad_norm': 1.6451504230499268, 'learning_rate': 5.311599024173875e-06, 'epoch': 2.681969394544245}\n",
            "Captured loss: 2.2353 at step 4031\n",
            "Logs at step 4032: {'loss': 2.4453, 'grad_norm': 1.1299126148223877, 'learning_rate': 5.300510090929252e-06, 'epoch': 2.682634730538922}\n",
            "Captured loss: 2.4453 at step 4032\n",
            "Logs at step 4033: {'loss': 2.598, 'grad_norm': 1.7831188440322876, 'learning_rate': 5.2894211576846314e-06, 'epoch': 2.6833000665335995}\n",
            "Captured loss: 2.598 at step 4033\n",
            "Logs at step 4034: {'loss': 2.1155, 'grad_norm': 2.0278754234313965, 'learning_rate': 5.278332224440009e-06, 'epoch': 2.6839654025282766}\n",
            "Captured loss: 2.1155 at step 4034\n",
            "Logs at step 4035: {'loss': 2.3939, 'grad_norm': 2.2456421852111816, 'learning_rate': 5.267243291195387e-06, 'epoch': 2.684630738522954}\n",
            "Captured loss: 2.3939 at step 4035\n",
            "Logs at step 4036: {'loss': 1.5527, 'grad_norm': 1.9718562364578247, 'learning_rate': 5.256154357950766e-06, 'epoch': 2.6852960745176313}\n",
            "Captured loss: 1.5527 at step 4036\n",
            "Logs at step 4037: {'loss': 2.2394, 'grad_norm': 1.6656197309494019, 'learning_rate': 5.245065424706143e-06, 'epoch': 2.685961410512309}\n",
            "Captured loss: 2.2394 at step 4037\n",
            "Logs at step 4038: {'loss': 2.4257, 'grad_norm': 1.3806095123291016, 'learning_rate': 5.233976491461522e-06, 'epoch': 2.686626746506986}\n",
            "Captured loss: 2.4257 at step 4038\n",
            "Logs at step 4039: {'loss': 2.428, 'grad_norm': 1.796118974685669, 'learning_rate': 5.2228875582169e-06, 'epoch': 2.6872920825016635}\n",
            "Captured loss: 2.428 at step 4039\n",
            "Logs at step 4040: {'loss': 2.3401, 'grad_norm': 1.4122364521026611, 'learning_rate': 5.211798624972278e-06, 'epoch': 2.6879574184963406}\n",
            "Captured loss: 2.3401 at step 4040\n",
            "Logs at step 4041: {'loss': 2.3807, 'grad_norm': 1.9831639528274536, 'learning_rate': 5.200709691727656e-06, 'epoch': 2.6886227544910177}\n",
            "Captured loss: 2.3807 at step 4041\n",
            "Logs at step 4042: {'loss': 2.4056, 'grad_norm': 1.743915319442749, 'learning_rate': 5.189620758483034e-06, 'epoch': 2.6892880904856953}\n",
            "Captured loss: 2.4056 at step 4042\n",
            "Logs at step 4043: {'loss': 1.8727, 'grad_norm': 2.1055333614349365, 'learning_rate': 5.178531825238412e-06, 'epoch': 2.689953426480373}\n",
            "Captured loss: 1.8727 at step 4043\n",
            "Logs at step 4044: {'loss': 2.5444, 'grad_norm': 1.1742315292358398, 'learning_rate': 5.16744289199379e-06, 'epoch': 2.69061876247505}\n",
            "Captured loss: 2.5444 at step 4044\n",
            "Logs at step 4045: {'loss': 2.4916, 'grad_norm': 2.103349208831787, 'learning_rate': 5.156353958749169e-06, 'epoch': 2.691284098469727}\n",
            "Captured loss: 2.4916 at step 4045\n",
            "Logs at step 4046: {'loss': 2.4063, 'grad_norm': 1.356040358543396, 'learning_rate': 5.145265025504547e-06, 'epoch': 2.6919494344644046}\n",
            "Captured loss: 2.4063 at step 4046\n",
            "Logs at step 4047: {'loss': 2.5476, 'grad_norm': 1.432961106300354, 'learning_rate': 5.134176092259925e-06, 'epoch': 2.6926147704590817}\n",
            "Captured loss: 2.5476 at step 4047\n",
            "Logs at step 4048: {'loss': 2.0474, 'grad_norm': 1.9160747528076172, 'learning_rate': 5.123087159015303e-06, 'epoch': 2.6932801064537593}\n",
            "Captured loss: 2.0474 at step 4048\n",
            "Logs at step 4049: {'loss': 2.3693, 'grad_norm': 1.6496213674545288, 'learning_rate': 5.111998225770681e-06, 'epoch': 2.6939454424484364}\n",
            "Captured loss: 2.3693 at step 4049\n",
            "Logs at step 4050: {'loss': 1.9657, 'grad_norm': 2.6765079498291016, 'learning_rate': 5.100909292526059e-06, 'epoch': 2.694610778443114}\n",
            "Captured loss: 1.9657 at step 4050\n",
            "Logs at step 4051: {'loss': 2.0756, 'grad_norm': 2.0156917572021484, 'learning_rate': 5.0898203592814375e-06, 'epoch': 2.695276114437791}\n",
            "Captured loss: 2.0756 at step 4051\n",
            "Logs at step 4052: {'loss': 2.018, 'grad_norm': 1.9838485717773438, 'learning_rate': 5.078731426036815e-06, 'epoch': 2.6959414504324686}\n",
            "Captured loss: 2.018 at step 4052\n",
            "Logs at step 4053: {'loss': 2.3477, 'grad_norm': 1.8161064386367798, 'learning_rate': 5.0676424927921935e-06, 'epoch': 2.6966067864271457}\n",
            "Captured loss: 2.3477 at step 4053\n",
            "Logs at step 4054: {'loss': 2.1275, 'grad_norm': 2.0068483352661133, 'learning_rate': 5.056553559547572e-06, 'epoch': 2.697272122421823}\n",
            "Captured loss: 2.1275 at step 4054\n",
            "Logs at step 4055: {'loss': 2.5052, 'grad_norm': 1.3495545387268066, 'learning_rate': 5.04546462630295e-06, 'epoch': 2.6979374584165003}\n",
            "Captured loss: 2.5052 at step 4055\n",
            "Logs at step 4056: {'loss': 2.1747, 'grad_norm': 1.7327630519866943, 'learning_rate': 5.034375693058329e-06, 'epoch': 2.698602794411178}\n",
            "Captured loss: 2.1747 at step 4056\n",
            "Logs at step 4057: {'loss': 2.3927, 'grad_norm': 1.8429781198501587, 'learning_rate': 5.023286759813706e-06, 'epoch': 2.699268130405855}\n",
            "Captured loss: 2.3927 at step 4057\n",
            "Logs at step 4058: {'loss': 2.4428, 'grad_norm': 1.2068454027175903, 'learning_rate': 5.012197826569085e-06, 'epoch': 2.699933466400532}\n",
            "Captured loss: 2.4428 at step 4058\n",
            "Logs at step 4059: {'loss': 2.3204, 'grad_norm': 1.5507495403289795, 'learning_rate': 5.001108893324462e-06, 'epoch': 2.7005988023952097}\n",
            "Captured loss: 2.3204 at step 4059\n",
            "Logs at step 4060: {'loss': 2.48, 'grad_norm': 1.4767569303512573, 'learning_rate': 4.9900199600798405e-06, 'epoch': 2.7012641383898868}\n",
            "Captured loss: 2.48 at step 4060\n",
            "Logs at step 4061: {'loss': 2.3799, 'grad_norm': 2.035404920578003, 'learning_rate': 4.978931026835219e-06, 'epoch': 2.7019294743845643}\n",
            "Captured loss: 2.3799 at step 4061\n",
            "Logs at step 4062: {'loss': 2.5457, 'grad_norm': 1.9098235368728638, 'learning_rate': 4.9678420935905965e-06, 'epoch': 2.7025948103792414}\n",
            "Captured loss: 2.5457 at step 4062\n",
            "Logs at step 4063: {'loss': 1.788, 'grad_norm': 2.1495401859283447, 'learning_rate': 4.956753160345975e-06, 'epoch': 2.703260146373919}\n",
            "Captured loss: 1.788 at step 4063\n",
            "Logs at step 4064: {'loss': 2.0195, 'grad_norm': 2.141798257827759, 'learning_rate': 4.9456642271013524e-06, 'epoch': 2.703925482368596}\n",
            "Captured loss: 2.0195 at step 4064\n",
            "Logs at step 4065: {'loss': 2.243, 'grad_norm': 1.5081791877746582, 'learning_rate': 4.934575293856731e-06, 'epoch': 2.7045908183632736}\n",
            "Captured loss: 2.243 at step 4065\n",
            "Logs at step 4066: {'loss': 2.5639, 'grad_norm': 1.6236240863800049, 'learning_rate': 4.92348636061211e-06, 'epoch': 2.7052561543579507}\n",
            "Captured loss: 2.5639 at step 4066\n",
            "Logs at step 4067: {'loss': 2.3298, 'grad_norm': 1.6351203918457031, 'learning_rate': 4.912397427367488e-06, 'epoch': 2.705921490352628}\n",
            "Captured loss: 2.3298 at step 4067\n",
            "Logs at step 4068: {'loss': 2.2742, 'grad_norm': 2.604376792907715, 'learning_rate': 4.901308494122866e-06, 'epoch': 2.7065868263473054}\n",
            "Captured loss: 2.2742 at step 4068\n",
            "Logs at step 4069: {'loss': 2.3006, 'grad_norm': 1.6618225574493408, 'learning_rate': 4.890219560878244e-06, 'epoch': 2.707252162341983}\n",
            "Captured loss: 2.3006 at step 4069\n",
            "Logs at step 4070: {'loss': 2.377, 'grad_norm': 1.5657001733779907, 'learning_rate': 4.879130627633622e-06, 'epoch': 2.70791749833666}\n",
            "Captured loss: 2.377 at step 4070\n",
            "Logs at step 4071: {'loss': 2.5139, 'grad_norm': 1.2022589445114136, 'learning_rate': 4.868041694389e-06, 'epoch': 2.708582834331337}\n",
            "Captured loss: 2.5139 at step 4071\n",
            "Logs at step 4072: {'loss': 1.9821, 'grad_norm': 1.8500230312347412, 'learning_rate': 4.856952761144378e-06, 'epoch': 2.7092481703260147}\n",
            "Captured loss: 1.9821 at step 4072\n",
            "Logs at step 4073: {'loss': 2.3341, 'grad_norm': 1.9666656255722046, 'learning_rate': 4.845863827899756e-06, 'epoch': 2.709913506320692}\n",
            "Captured loss: 2.3341 at step 4073\n",
            "Logs at step 4074: {'loss': 2.0307, 'grad_norm': 1.6814706325531006, 'learning_rate': 4.834774894655134e-06, 'epoch': 2.7105788423153694}\n",
            "Captured loss: 2.0307 at step 4074\n",
            "Logs at step 4075: {'loss': 2.132, 'grad_norm': 1.4040591716766357, 'learning_rate': 4.823685961410512e-06, 'epoch': 2.7112441783100465}\n",
            "Captured loss: 2.132 at step 4075\n",
            "Logs at step 4076: {'loss': 1.9207, 'grad_norm': 2.285299777984619, 'learning_rate': 4.812597028165891e-06, 'epoch': 2.711909514304724}\n",
            "Captured loss: 1.9207 at step 4076\n",
            "Logs at step 4077: {'loss': 2.5217, 'grad_norm': 1.4706709384918213, 'learning_rate': 4.801508094921268e-06, 'epoch': 2.712574850299401}\n",
            "Captured loss: 2.5217 at step 4077\n",
            "Logs at step 4078: {'loss': 2.5595, 'grad_norm': 1.3290579319000244, 'learning_rate': 4.7904191616766475e-06, 'epoch': 2.7132401862940787}\n",
            "Captured loss: 2.5595 at step 4078\n",
            "Logs at step 4079: {'loss': 2.1439, 'grad_norm': 2.605860948562622, 'learning_rate': 4.779330228432025e-06, 'epoch': 2.713905522288756}\n",
            "Captured loss: 2.1439 at step 4079\n",
            "Logs at step 4080: {'loss': 1.684, 'grad_norm': 1.7449355125427246, 'learning_rate': 4.768241295187403e-06, 'epoch': 2.714570858283433}\n",
            "Captured loss: 1.684 at step 4080\n",
            "Logs at step 4081: {'loss': 2.2022, 'grad_norm': 1.8336073160171509, 'learning_rate': 4.757152361942782e-06, 'epoch': 2.7152361942781105}\n",
            "Captured loss: 2.2022 at step 4081\n",
            "Logs at step 4082: {'loss': 2.3659, 'grad_norm': 1.1238924264907837, 'learning_rate': 4.746063428698159e-06, 'epoch': 2.715901530272788}\n",
            "Captured loss: 2.3659 at step 4082\n",
            "Logs at step 4083: {'loss': 2.1586, 'grad_norm': 1.5015898942947388, 'learning_rate': 4.734974495453538e-06, 'epoch': 2.716566866267465}\n",
            "Captured loss: 2.1586 at step 4083\n",
            "Logs at step 4084: {'loss': 2.3567, 'grad_norm': 1.1679471731185913, 'learning_rate': 4.723885562208915e-06, 'epoch': 2.7172322022621422}\n",
            "Captured loss: 2.3567 at step 4084\n",
            "Logs at step 4085: {'loss': 2.5849, 'grad_norm': 1.5453722476959229, 'learning_rate': 4.712796628964294e-06, 'epoch': 2.71789753825682}\n",
            "Captured loss: 2.5849 at step 4085\n",
            "Logs at step 4086: {'loss': 2.3626, 'grad_norm': 3.0309391021728516, 'learning_rate': 4.701707695719672e-06, 'epoch': 2.718562874251497}\n",
            "Captured loss: 2.3626 at step 4086\n",
            "Logs at step 4087: {'loss': 2.2827, 'grad_norm': 2.1105194091796875, 'learning_rate': 4.69061876247505e-06, 'epoch': 2.7192282102461744}\n",
            "Captured loss: 2.2827 at step 4087\n",
            "Logs at step 4088: {'loss': 2.4519, 'grad_norm': 1.7249231338500977, 'learning_rate': 4.679529829230428e-06, 'epoch': 2.7198935462408516}\n",
            "Captured loss: 2.4519 at step 4088\n",
            "Logs at step 4089: {'loss': 2.4526, 'grad_norm': 1.2569761276245117, 'learning_rate': 4.6684408959858064e-06, 'epoch': 2.720558882235529}\n",
            "Captured loss: 2.4526 at step 4089\n",
            "Logs at step 4090: {'loss': 2.0633, 'grad_norm': 1.889545202255249, 'learning_rate': 4.657351962741185e-06, 'epoch': 2.721224218230206}\n",
            "Captured loss: 2.0633 at step 4090\n",
            "Logs at step 4091: {'loss': 2.6439, 'grad_norm': 1.6395822763442993, 'learning_rate': 4.646263029496563e-06, 'epoch': 2.7218895542248838}\n",
            "Captured loss: 2.6439 at step 4091\n",
            "Logs at step 4092: {'loss': 2.5021, 'grad_norm': 1.3226709365844727, 'learning_rate': 4.635174096251941e-06, 'epoch': 2.722554890219561}\n",
            "Captured loss: 2.5021 at step 4092\n",
            "Logs at step 4093: {'loss': 2.4516, 'grad_norm': 1.3251230716705322, 'learning_rate': 4.624085163007319e-06, 'epoch': 2.723220226214238}\n",
            "Captured loss: 2.4516 at step 4093\n",
            "Logs at step 4094: {'loss': 2.4367, 'grad_norm': 1.309421181678772, 'learning_rate': 4.612996229762697e-06, 'epoch': 2.7238855622089155}\n",
            "Captured loss: 2.4367 at step 4094\n",
            "Logs at step 4095: {'loss': 1.5709, 'grad_norm': 2.2977447509765625, 'learning_rate': 4.601907296518075e-06, 'epoch': 2.724550898203593}\n",
            "Captured loss: 1.5709 at step 4095\n",
            "Logs at step 4096: {'loss': 1.8484, 'grad_norm': 1.8688548803329468, 'learning_rate': 4.5908183632734535e-06, 'epoch': 2.72521623419827}\n",
            "Captured loss: 1.8484 at step 4096\n",
            "Logs at step 4097: {'loss': 2.524, 'grad_norm': 1.3460675477981567, 'learning_rate': 4.579729430028831e-06, 'epoch': 2.7258815701929473}\n",
            "Captured loss: 2.524 at step 4097\n",
            "Logs at step 4098: {'loss': 2.4857, 'grad_norm': 2.272444009780884, 'learning_rate': 4.5686404967842095e-06, 'epoch': 2.726546906187625}\n",
            "Captured loss: 2.4857 at step 4098\n",
            "Logs at step 4099: {'loss': 2.3472, 'grad_norm': 1.4570295810699463, 'learning_rate': 4.557551563539587e-06, 'epoch': 2.727212242182302}\n",
            "Captured loss: 2.3472 at step 4099\n",
            "Logs at step 4100: {'loss': 2.3743, 'grad_norm': 1.764211893081665, 'learning_rate': 4.546462630294966e-06, 'epoch': 2.7278775781769795}\n",
            "Captured loss: 2.3743 at step 4100\n",
            "Logs at step 4101: {'loss': 1.8124, 'grad_norm': 1.9247385263442993, 'learning_rate': 4.535373697050344e-06, 'epoch': 2.7285429141716566}\n",
            "Captured loss: 1.8124 at step 4101\n",
            "Logs at step 4102: {'loss': 2.1501, 'grad_norm': 1.5940911769866943, 'learning_rate': 4.524284763805722e-06, 'epoch': 2.7292082501663337}\n",
            "Captured loss: 2.1501 at step 4102\n",
            "Logs at step 4103: {'loss': 2.1438, 'grad_norm': 1.6200567483901978, 'learning_rate': 4.513195830561101e-06, 'epoch': 2.7298735861610113}\n",
            "Captured loss: 2.1438 at step 4103\n",
            "Logs at step 4104: {'loss': 2.2817, 'grad_norm': 1.6029934883117676, 'learning_rate': 4.502106897316478e-06, 'epoch': 2.730538922155689}\n",
            "Captured loss: 2.2817 at step 4104\n",
            "Logs at step 4105: {'loss': 2.4706, 'grad_norm': 1.1995466947555542, 'learning_rate': 4.4910179640718566e-06, 'epoch': 2.731204258150366}\n",
            "Captured loss: 2.4706 at step 4105\n",
            "Logs at step 4106: {'loss': 2.1033, 'grad_norm': 1.8037811517715454, 'learning_rate': 4.479929030827235e-06, 'epoch': 2.731869594145043}\n",
            "Captured loss: 2.1033 at step 4106\n",
            "Logs at step 4107: {'loss': 2.3116, 'grad_norm': 1.8008157014846802, 'learning_rate': 4.4688400975826125e-06, 'epoch': 2.7325349301397206}\n",
            "Captured loss: 2.3116 at step 4107\n",
            "Logs at step 4108: {'loss': 1.8791, 'grad_norm': 1.6856639385223389, 'learning_rate': 4.457751164337991e-06, 'epoch': 2.7332002661343977}\n",
            "Captured loss: 1.8791 at step 4108\n",
            "Logs at step 4109: {'loss': 1.6668, 'grad_norm': 2.05242919921875, 'learning_rate': 4.4466622310933685e-06, 'epoch': 2.7338656021290753}\n",
            "Captured loss: 1.6668 at step 4109\n",
            "Logs at step 4110: {'loss': 2.434, 'grad_norm': 1.4582237005233765, 'learning_rate': 4.435573297848747e-06, 'epoch': 2.7345309381237524}\n",
            "Captured loss: 2.434 at step 4110\n",
            "Logs at step 4111: {'loss': 2.448, 'grad_norm': 1.299651861190796, 'learning_rate': 4.424484364604125e-06, 'epoch': 2.73519627411843}\n",
            "Captured loss: 2.448 at step 4111\n",
            "Logs at step 4112: {'loss': 1.6059, 'grad_norm': 2.2652909755706787, 'learning_rate': 4.413395431359504e-06, 'epoch': 2.735861610113107}\n",
            "Captured loss: 1.6059 at step 4112\n",
            "Logs at step 4113: {'loss': 2.0038, 'grad_norm': 2.7371225357055664, 'learning_rate': 4.402306498114882e-06, 'epoch': 2.7365269461077846}\n",
            "Captured loss: 2.0038 at step 4113\n",
            "Logs at step 4114: {'loss': 2.3339, 'grad_norm': 1.9616661071777344, 'learning_rate': 4.39121756487026e-06, 'epoch': 2.7371922821024617}\n",
            "Captured loss: 2.3339 at step 4114\n",
            "Logs at step 4115: {'loss': 2.4487, 'grad_norm': 1.405137538909912, 'learning_rate': 4.380128631625638e-06, 'epoch': 2.737857618097139}\n",
            "Captured loss: 2.4487 at step 4115\n",
            "Logs at step 4116: {'loss': 1.8245, 'grad_norm': 1.926949143409729, 'learning_rate': 4.369039698381016e-06, 'epoch': 2.7385229540918163}\n",
            "Captured loss: 1.8245 at step 4116\n",
            "Logs at step 4117: {'loss': 2.0038, 'grad_norm': 2.047933340072632, 'learning_rate': 4.357950765136394e-06, 'epoch': 2.739188290086494}\n",
            "Captured loss: 2.0038 at step 4117\n",
            "Logs at step 4118: {'loss': 2.2862, 'grad_norm': 1.2446916103363037, 'learning_rate': 4.346861831891772e-06, 'epoch': 2.739853626081171}\n",
            "Captured loss: 2.2862 at step 4118\n",
            "Logs at step 4119: {'loss': 2.3981, 'grad_norm': 1.416896939277649, 'learning_rate': 4.33577289864715e-06, 'epoch': 2.740518962075848}\n",
            "Captured loss: 2.3981 at step 4119\n",
            "Logs at step 4120: {'loss': 1.9918, 'grad_norm': 1.599682331085205, 'learning_rate': 4.324683965402528e-06, 'epoch': 2.7411842980705257}\n",
            "Captured loss: 1.9918 at step 4120\n",
            "Logs at step 4121: {'loss': 2.1162, 'grad_norm': 1.8647234439849854, 'learning_rate': 4.313595032157907e-06, 'epoch': 2.7418496340652028}\n",
            "Captured loss: 2.1162 at step 4121\n",
            "Logs at step 4122: {'loss': 2.3567, 'grad_norm': 1.3881800174713135, 'learning_rate': 4.302506098913284e-06, 'epoch': 2.7425149700598803}\n",
            "Captured loss: 2.3567 at step 4122\n",
            "Logs at step 4123: {'loss': 2.2988, 'grad_norm': 1.5933716297149658, 'learning_rate': 4.2914171656686635e-06, 'epoch': 2.7431803060545574}\n",
            "Captured loss: 2.2988 at step 4123\n",
            "Logs at step 4124: {'loss': 2.059, 'grad_norm': 2.3989450931549072, 'learning_rate': 4.280328232424041e-06, 'epoch': 2.743845642049235}\n",
            "Captured loss: 2.059 at step 4124\n",
            "Logs at step 4125: {'loss': 2.2973, 'grad_norm': 1.5806351900100708, 'learning_rate': 4.2692392991794194e-06, 'epoch': 2.744510978043912}\n",
            "Captured loss: 2.2973 at step 4125\n",
            "Logs at step 4126: {'loss': 2.0059, 'grad_norm': 1.6755160093307495, 'learning_rate': 4.258150365934797e-06, 'epoch': 2.7451763140385896}\n",
            "Captured loss: 2.0059 at step 4126\n",
            "Logs at step 4127: {'loss': 1.8038, 'grad_norm': 1.9608439207077026, 'learning_rate': 4.247061432690175e-06, 'epoch': 2.7458416500332667}\n",
            "Captured loss: 1.8038 at step 4127\n",
            "Logs at step 4128: {'loss': 2.1333, 'grad_norm': 1.984951376914978, 'learning_rate': 4.235972499445554e-06, 'epoch': 2.746506986027944}\n",
            "Captured loss: 2.1333 at step 4128\n",
            "Logs at step 4129: {'loss': 1.7968, 'grad_norm': 2.038353443145752, 'learning_rate': 4.224883566200931e-06, 'epoch': 2.7471723220226214}\n",
            "Captured loss: 1.7968 at step 4129\n",
            "Logs at step 4130: {'loss': 2.5974, 'grad_norm': 1.638830542564392, 'learning_rate': 4.21379463295631e-06, 'epoch': 2.747837658017299}\n",
            "Captured loss: 2.5974 at step 4130\n",
            "Logs at step 4131: {'loss': 2.3542, 'grad_norm': 1.2635014057159424, 'learning_rate': 4.202705699711688e-06, 'epoch': 2.748502994011976}\n",
            "Captured loss: 2.3542 at step 4131\n",
            "Logs at step 4132: {'loss': 2.2487, 'grad_norm': 1.5922513008117676, 'learning_rate': 4.191616766467066e-06, 'epoch': 2.749168330006653}\n",
            "Captured loss: 2.2487 at step 4132\n",
            "Logs at step 4133: {'loss': 2.4594, 'grad_norm': 1.1191128492355347, 'learning_rate': 4.180527833222444e-06, 'epoch': 2.7498336660013307}\n",
            "Captured loss: 2.4594 at step 4133\n",
            "Logs at step 4134: {'loss': 2.5943, 'grad_norm': 1.6815261840820312, 'learning_rate': 4.1694388999778225e-06, 'epoch': 2.750499001996008}\n",
            "Captured loss: 2.5943 at step 4134\n",
            "Logs at step 4135: {'loss': 1.718, 'grad_norm': 2.2293331623077393, 'learning_rate': 4.158349966733201e-06, 'epoch': 2.7511643379906854}\n",
            "Captured loss: 1.718 at step 4135\n",
            "Logs at step 4136: {'loss': 1.914, 'grad_norm': 2.0189149379730225, 'learning_rate': 4.1472610334885784e-06, 'epoch': 2.7518296739853625}\n",
            "Captured loss: 1.914 at step 4136\n",
            "Logs at step 4137: {'loss': 2.5472, 'grad_norm': 1.4009917974472046, 'learning_rate': 4.136172100243957e-06, 'epoch': 2.75249500998004}\n",
            "Captured loss: 2.5472 at step 4137\n",
            "Logs at step 4138: {'loss': 2.4632, 'grad_norm': 1.4505324363708496, 'learning_rate': 4.125083166999335e-06, 'epoch': 2.753160345974717}\n",
            "Captured loss: 2.4632 at step 4138\n",
            "Logs at step 4139: {'loss': 2.4397, 'grad_norm': 1.6730446815490723, 'learning_rate': 4.113994233754713e-06, 'epoch': 2.7538256819693947}\n",
            "Captured loss: 2.4397 at step 4139\n",
            "Logs at step 4140: {'loss': 2.4071, 'grad_norm': 1.22895085811615, 'learning_rate': 4.102905300510091e-06, 'epoch': 2.754491017964072}\n",
            "Captured loss: 2.4071 at step 4140\n",
            "Logs at step 4141: {'loss': 2.1286, 'grad_norm': 2.223696708679199, 'learning_rate': 4.091816367265469e-06, 'epoch': 2.755156353958749}\n",
            "Captured loss: 2.1286 at step 4141\n",
            "Logs at step 4142: {'loss': 1.8251, 'grad_norm': 1.7248729467391968, 'learning_rate': 4.080727434020847e-06, 'epoch': 2.7558216899534265}\n",
            "Captured loss: 1.8251 at step 4142\n",
            "Logs at step 4143: {'loss': 2.3546, 'grad_norm': 1.597341537475586, 'learning_rate': 4.0696385007762255e-06, 'epoch': 2.756487025948104}\n",
            "Captured loss: 2.3546 at step 4143\n",
            "Logs at step 4144: {'loss': 2.1084, 'grad_norm': 1.4881024360656738, 'learning_rate': 4.058549567531603e-06, 'epoch': 2.757152361942781}\n",
            "Captured loss: 2.1084 at step 4144\n",
            "Logs at step 4145: {'loss': 2.3725, 'grad_norm': 1.3418182134628296, 'learning_rate': 4.047460634286982e-06, 'epoch': 2.7578176979374582}\n",
            "Captured loss: 2.3725 at step 4145\n",
            "Logs at step 4146: {'loss': 2.0724, 'grad_norm': 1.656936764717102, 'learning_rate': 4.03637170104236e-06, 'epoch': 2.758483033932136}\n",
            "Captured loss: 2.0724 at step 4146\n",
            "Logs at step 4147: {'loss': 2.4289, 'grad_norm': 2.082831859588623, 'learning_rate': 4.025282767797738e-06, 'epoch': 2.759148369926813}\n",
            "Captured loss: 2.4289 at step 4147\n",
            "Logs at step 4148: {'loss': 2.5548, 'grad_norm': 1.2104275226593018, 'learning_rate': 4.014193834553117e-06, 'epoch': 2.7598137059214904}\n",
            "Captured loss: 2.5548 at step 4148\n",
            "Logs at step 4149: {'loss': 2.3852, 'grad_norm': 1.3062576055526733, 'learning_rate': 4.003104901308494e-06, 'epoch': 2.7604790419161676}\n",
            "Captured loss: 2.3852 at step 4149\n",
            "Logs at step 4150: {'loss': 2.1649, 'grad_norm': 1.7418016195297241, 'learning_rate': 3.992015968063873e-06, 'epoch': 2.761144377910845}\n",
            "Captured loss: 2.1649 at step 4150\n",
            "Logs at step 4151: {'loss': 2.3737, 'grad_norm': 1.265292763710022, 'learning_rate': 3.98092703481925e-06, 'epoch': 2.761809713905522}\n",
            "Captured loss: 2.3737 at step 4151\n",
            "Logs at step 4152: {'loss': 2.3819, 'grad_norm': 1.620846152305603, 'learning_rate': 3.9698381015746286e-06, 'epoch': 2.7624750499001998}\n",
            "Captured loss: 2.3819 at step 4152\n",
            "Logs at step 4153: {'loss': 2.0772, 'grad_norm': 1.6747230291366577, 'learning_rate': 3.958749168330007e-06, 'epoch': 2.763140385894877}\n",
            "Captured loss: 2.0772 at step 4153\n",
            "Logs at step 4154: {'loss': 2.2822, 'grad_norm': 1.3426563739776611, 'learning_rate': 3.9476602350853845e-06, 'epoch': 2.763805721889554}\n",
            "Captured loss: 2.2822 at step 4154\n",
            "Logs at step 4155: {'loss': 2.1606, 'grad_norm': 1.5564072132110596, 'learning_rate': 3.936571301840763e-06, 'epoch': 2.7644710578842315}\n",
            "Captured loss: 2.1606 at step 4155\n",
            "Logs at step 4156: {'loss': 2.504, 'grad_norm': 2.1957955360412598, 'learning_rate': 3.925482368596141e-06, 'epoch': 2.765136393878909}\n",
            "Captured loss: 2.504 at step 4156\n",
            "Logs at step 4157: {'loss': 2.1088, 'grad_norm': 2.164361000061035, 'learning_rate': 3.91439343535152e-06, 'epoch': 2.765801729873586}\n",
            "Captured loss: 2.1088 at step 4157\n",
            "Logs at step 4158: {'loss': 2.4679, 'grad_norm': 2.6440935134887695, 'learning_rate': 3.903304502106898e-06, 'epoch': 2.7664670658682633}\n",
            "Captured loss: 2.4679 at step 4158\n",
            "Logs at step 4159: {'loss': 1.5954, 'grad_norm': 2.2398500442504883, 'learning_rate': 3.892215568862276e-06, 'epoch': 2.767132401862941}\n",
            "Captured loss: 1.5954 at step 4159\n",
            "Logs at step 4160: {'loss': 2.6304, 'grad_norm': 1.9194146394729614, 'learning_rate': 3.881126635617654e-06, 'epoch': 2.767797737857618}\n",
            "Captured loss: 2.6304 at step 4160\n",
            "Logs at step 4161: {'loss': 1.9429, 'grad_norm': 1.49886155128479, 'learning_rate': 3.870037702373032e-06, 'epoch': 2.7684630738522955}\n",
            "Captured loss: 1.9429 at step 4161\n",
            "Logs at step 4162: {'loss': 2.4314, 'grad_norm': 1.3250871896743774, 'learning_rate': 3.85894876912841e-06, 'epoch': 2.7691284098469726}\n",
            "Captured loss: 2.4314 at step 4162\n",
            "Logs at step 4163: {'loss': 2.421, 'grad_norm': 1.9991998672485352, 'learning_rate': 3.847859835883788e-06, 'epoch': 2.76979374584165}\n",
            "Captured loss: 2.421 at step 4163\n",
            "Logs at step 4164: {'loss': 2.589, 'grad_norm': 1.2677407264709473, 'learning_rate': 3.836770902639166e-06, 'epoch': 2.7704590818363273}\n",
            "Captured loss: 2.589 at step 4164\n",
            "Logs at step 4165: {'loss': 2.1892, 'grad_norm': 1.6996182203292847, 'learning_rate': 3.825681969394544e-06, 'epoch': 2.771124417831005}\n",
            "Captured loss: 2.1892 at step 4165\n",
            "Logs at step 4166: {'loss': 2.4207, 'grad_norm': 1.2990646362304688, 'learning_rate': 3.8145930361499223e-06, 'epoch': 2.771789753825682}\n",
            "Captured loss: 2.4207 at step 4166\n",
            "Logs at step 4167: {'loss': 1.8837, 'grad_norm': 2.2244713306427, 'learning_rate': 3.8035041029053003e-06, 'epoch': 2.772455089820359}\n",
            "Captured loss: 1.8837 at step 4167\n",
            "Logs at step 4168: {'loss': 2.4689, 'grad_norm': 1.0574787855148315, 'learning_rate': 3.792415169660679e-06, 'epoch': 2.7731204258150366}\n",
            "Captured loss: 2.4689 at step 4168\n",
            "Logs at step 4169: {'loss': 2.4085, 'grad_norm': 1.2612906694412231, 'learning_rate': 3.781326236416057e-06, 'epoch': 2.773785761809714}\n",
            "Captured loss: 2.4085 at step 4169\n",
            "Logs at step 4170: {'loss': 2.2736, 'grad_norm': 1.558896541595459, 'learning_rate': 3.770237303171435e-06, 'epoch': 2.7744510978043913}\n",
            "Captured loss: 2.2736 at step 4170\n",
            "Logs at step 4171: {'loss': 2.5142, 'grad_norm': 1.33329439163208, 'learning_rate': 3.7591483699268134e-06, 'epoch': 2.7751164337990684}\n",
            "Captured loss: 2.5142 at step 4171\n",
            "Logs at step 4172: {'loss': 2.3901, 'grad_norm': 1.1841422319412231, 'learning_rate': 3.7480594366821914e-06, 'epoch': 2.775781769793746}\n",
            "Captured loss: 2.3901 at step 4172\n",
            "Logs at step 4173: {'loss': 1.5968, 'grad_norm': 2.0302913188934326, 'learning_rate': 3.7369705034375694e-06, 'epoch': 2.776447105788423}\n",
            "Captured loss: 1.5968 at step 4173\n",
            "Logs at step 4174: {'loss': 2.5272, 'grad_norm': 1.3635330200195312, 'learning_rate': 3.7258815701929474e-06, 'epoch': 2.7771124417831006}\n",
            "Captured loss: 2.5272 at step 4174\n",
            "Logs at step 4175: {'loss': 2.4961, 'grad_norm': 1.1630592346191406, 'learning_rate': 3.7147926369483258e-06, 'epoch': 2.7777777777777777}\n",
            "Captured loss: 2.4961 at step 4175\n",
            "Logs at step 4176: {'loss': 2.231, 'grad_norm': 2.07977032661438, 'learning_rate': 3.7037037037037037e-06, 'epoch': 2.7784431137724552}\n",
            "Captured loss: 2.231 at step 4176\n",
            "Logs at step 4177: {'loss': 2.4074, 'grad_norm': 1.6901134252548218, 'learning_rate': 3.6926147704590817e-06, 'epoch': 2.7791084497671323}\n",
            "Captured loss: 2.4074 at step 4177\n",
            "Logs at step 4178: {'loss': 2.3103, 'grad_norm': 2.098629951477051, 'learning_rate': 3.6815258372144597e-06, 'epoch': 2.77977378576181}\n",
            "Captured loss: 2.3103 at step 4178\n",
            "Logs at step 4179: {'loss': 2.2579, 'grad_norm': 1.227303385734558, 'learning_rate': 3.6704369039698385e-06, 'epoch': 2.780439121756487}\n",
            "Captured loss: 2.2579 at step 4179\n",
            "Logs at step 4180: {'loss': 2.5047, 'grad_norm': 1.3134156465530396, 'learning_rate': 3.6593479707252165e-06, 'epoch': 2.781104457751164}\n",
            "Captured loss: 2.5047 at step 4180\n",
            "Logs at step 4181: {'loss': 2.3929, 'grad_norm': 1.5455684661865234, 'learning_rate': 3.648259037480595e-06, 'epoch': 2.7817697937458417}\n",
            "Captured loss: 2.3929 at step 4181\n",
            "Logs at step 4182: {'loss': 2.4726, 'grad_norm': 1.9392789602279663, 'learning_rate': 3.637170104235973e-06, 'epoch': 2.782435129740519}\n",
            "Captured loss: 2.4726 at step 4182\n",
            "Logs at step 4183: {'loss': 2.234, 'grad_norm': 1.813659906387329, 'learning_rate': 3.626081170991351e-06, 'epoch': 2.7831004657351963}\n",
            "Captured loss: 2.234 at step 4183\n",
            "Logs at step 4184: {'loss': 2.3404, 'grad_norm': 1.4466495513916016, 'learning_rate': 3.614992237746729e-06, 'epoch': 2.7837658017298734}\n",
            "Captured loss: 2.3404 at step 4184\n",
            "Logs at step 4185: {'loss': 2.4067, 'grad_norm': 1.2804151773452759, 'learning_rate': 3.603903304502107e-06, 'epoch': 2.784431137724551}\n",
            "Captured loss: 2.4067 at step 4185\n",
            "Logs at step 4186: {'loss': 2.332, 'grad_norm': 1.7402315139770508, 'learning_rate': 3.592814371257485e-06, 'epoch': 2.785096473719228}\n",
            "Captured loss: 2.332 at step 4186\n",
            "Logs at step 4187: {'loss': 1.8442, 'grad_norm': 1.792205572128296, 'learning_rate': 3.581725438012863e-06, 'epoch': 2.7857618097139056}\n",
            "Captured loss: 1.8442 at step 4187\n",
            "Logs at step 4188: {'loss': 2.4626, 'grad_norm': 1.390032172203064, 'learning_rate': 3.570636504768241e-06, 'epoch': 2.7864271457085827}\n",
            "Captured loss: 2.4626 at step 4188\n",
            "Logs at step 4189: {'loss': 2.3521, 'grad_norm': 1.4996922016143799, 'learning_rate': 3.559547571523619e-06, 'epoch': 2.7870924817032603}\n",
            "Captured loss: 2.3521 at step 4189\n",
            "Logs at step 4190: {'loss': 2.5063, 'grad_norm': 1.2805737257003784, 'learning_rate': 3.548458638278998e-06, 'epoch': 2.7877578176979374}\n",
            "Captured loss: 2.5063 at step 4190\n",
            "Logs at step 4191: {'loss': 2.3523, 'grad_norm': 1.8491300344467163, 'learning_rate': 3.5373697050343763e-06, 'epoch': 2.788423153692615}\n",
            "Captured loss: 2.3523 at step 4191\n",
            "Logs at step 4192: {'loss': 2.3707, 'grad_norm': 1.2362726926803589, 'learning_rate': 3.5262807717897543e-06, 'epoch': 2.789088489687292}\n",
            "Captured loss: 2.3707 at step 4192\n",
            "Logs at step 4193: {'loss': 2.4369, 'grad_norm': 1.0970306396484375, 'learning_rate': 3.5151918385451323e-06, 'epoch': 2.789753825681969}\n",
            "Captured loss: 2.4369 at step 4193\n",
            "Logs at step 4194: {'loss': 2.2183, 'grad_norm': 1.6613941192626953, 'learning_rate': 3.5041029053005102e-06, 'epoch': 2.7904191616766467}\n",
            "Captured loss: 2.2183 at step 4194\n",
            "Logs at step 4195: {'loss': 1.9827, 'grad_norm': 2.0852673053741455, 'learning_rate': 3.493013972055888e-06, 'epoch': 2.7910844976713243}\n",
            "Captured loss: 1.9827 at step 4195\n",
            "Logs at step 4196: {'loss': 1.9796, 'grad_norm': 1.9614633321762085, 'learning_rate': 3.4819250388112666e-06, 'epoch': 2.7917498336660014}\n",
            "Captured loss: 1.9796 at step 4196\n",
            "Logs at step 4197: {'loss': 2.6123, 'grad_norm': 2.565117835998535, 'learning_rate': 3.4708361055666446e-06, 'epoch': 2.7924151696606785}\n",
            "Captured loss: 2.6123 at step 4197\n",
            "Logs at step 4198: {'loss': 2.2932, 'grad_norm': 2.332632303237915, 'learning_rate': 3.4597471723220226e-06, 'epoch': 2.793080505655356}\n",
            "Captured loss: 2.2932 at step 4198\n",
            "Logs at step 4199: {'loss': 2.5052, 'grad_norm': 1.3579776287078857, 'learning_rate': 3.4486582390774005e-06, 'epoch': 2.793745841650033}\n",
            "Captured loss: 2.5052 at step 4199\n",
            "Logs at step 4200: {'loss': 2.5145, 'grad_norm': 1.3988494873046875, 'learning_rate': 3.437569305832779e-06, 'epoch': 2.7944111776447107}\n",
            "Captured loss: 2.5145 at step 4200\n",
            "Logs at step 4201: {'loss': 1.9197, 'grad_norm': 1.5687448978424072, 'learning_rate': 3.4264803725881573e-06, 'epoch': 2.795076513639388}\n",
            "Captured loss: 1.9197 at step 4201\n",
            "Logs at step 4202: {'loss': 1.829, 'grad_norm': 1.531039834022522, 'learning_rate': 3.4153914393435357e-06, 'epoch': 2.7957418496340654}\n",
            "Captured loss: 1.829 at step 4202\n",
            "Logs at step 4203: {'loss': 2.4869, 'grad_norm': 1.615828514099121, 'learning_rate': 3.4043025060989137e-06, 'epoch': 2.7964071856287425}\n",
            "Captured loss: 2.4869 at step 4203\n",
            "Logs at step 4204: {'loss': 2.3578, 'grad_norm': 1.0634775161743164, 'learning_rate': 3.3932135728542917e-06, 'epoch': 2.79707252162342}\n",
            "Captured loss: 2.3578 at step 4204\n",
            "Logs at step 4205: {'loss': 2.415, 'grad_norm': 1.1889516115188599, 'learning_rate': 3.3821246396096696e-06, 'epoch': 2.797737857618097}\n",
            "Captured loss: 2.415 at step 4205\n",
            "Logs at step 4206: {'loss': 2.4313, 'grad_norm': 1.2219265699386597, 'learning_rate': 3.371035706365048e-06, 'epoch': 2.7984031936127742}\n",
            "Captured loss: 2.4313 at step 4206\n",
            "Logs at step 4207: {'loss': 2.4087, 'grad_norm': 1.3631495237350464, 'learning_rate': 3.359946773120426e-06, 'epoch': 2.799068529607452}\n",
            "Captured loss: 2.4087 at step 4207\n",
            "Logs at step 4208: {'loss': 2.3841, 'grad_norm': 1.6998127698898315, 'learning_rate': 3.348857839875804e-06, 'epoch': 2.7997338656021293}\n",
            "Captured loss: 2.3841 at step 4208\n",
            "Logs at step 4209: {'loss': 2.4685, 'grad_norm': 1.546545147895813, 'learning_rate': 3.337768906631182e-06, 'epoch': 2.8003992015968064}\n",
            "Captured loss: 2.4685 at step 4209\n",
            "Logs at step 4210: {'loss': 2.5095, 'grad_norm': 1.6793087720870972, 'learning_rate': 3.32667997338656e-06, 'epoch': 2.8010645375914835}\n",
            "Captured loss: 2.5095 at step 4210\n",
            "Logs at step 4211: {'loss': 2.3869, 'grad_norm': 1.8449393510818481, 'learning_rate': 3.3155910401419383e-06, 'epoch': 2.801729873586161}\n",
            "Captured loss: 2.3869 at step 4211\n",
            "Logs at step 4212: {'loss': 2.0015, 'grad_norm': 1.6072978973388672, 'learning_rate': 3.3045021068973163e-06, 'epoch': 2.802395209580838}\n",
            "Captured loss: 2.0015 at step 4212\n",
            "Logs at step 4213: {'loss': 2.412, 'grad_norm': 1.2339776754379272, 'learning_rate': 3.293413173652695e-06, 'epoch': 2.8030605455755158}\n",
            "Captured loss: 2.412 at step 4213\n",
            "Logs at step 4214: {'loss': 2.3526, 'grad_norm': 1.3065181970596313, 'learning_rate': 3.282324240408073e-06, 'epoch': 2.803725881570193}\n",
            "Captured loss: 2.3526 at step 4214\n",
            "Logs at step 4215: {'loss': 2.3656, 'grad_norm': 2.182955265045166, 'learning_rate': 3.271235307163451e-06, 'epoch': 2.8043912175648704}\n",
            "Captured loss: 2.3656 at step 4215\n",
            "Logs at step 4216: {'loss': 1.8007, 'grad_norm': 2.2087972164154053, 'learning_rate': 3.2601463739188295e-06, 'epoch': 2.8050565535595475}\n",
            "Captured loss: 1.8007 at step 4216\n",
            "Logs at step 4217: {'loss': 2.4156, 'grad_norm': 1.5807700157165527, 'learning_rate': 3.2490574406742075e-06, 'epoch': 2.805721889554225}\n",
            "Captured loss: 2.4156 at step 4217\n",
            "Logs at step 4218: {'loss': 2.249, 'grad_norm': 2.526616334915161, 'learning_rate': 3.2379685074295854e-06, 'epoch': 2.806387225548902}\n",
            "Captured loss: 2.249 at step 4218\n",
            "Logs at step 4219: {'loss': 2.0701, 'grad_norm': 2.151771306991577, 'learning_rate': 3.2268795741849634e-06, 'epoch': 2.8070525615435793}\n",
            "Captured loss: 2.0701 at step 4219\n",
            "Logs at step 4220: {'loss': 1.6678, 'grad_norm': 2.6645119190216064, 'learning_rate': 3.2157906409403414e-06, 'epoch': 2.807717897538257}\n",
            "Captured loss: 1.6678 at step 4220\n",
            "Logs at step 4221: {'loss': 1.9473, 'grad_norm': 1.6183669567108154, 'learning_rate': 3.2047017076957198e-06, 'epoch': 2.8083832335329344}\n",
            "Captured loss: 1.9473 at step 4221\n",
            "Logs at step 4222: {'loss': 2.6198, 'grad_norm': 1.7896580696105957, 'learning_rate': 3.1936127744510977e-06, 'epoch': 2.8090485695276115}\n",
            "Captured loss: 2.6198 at step 4222\n",
            "Logs at step 4223: {'loss': 1.9006, 'grad_norm': 1.9473260641098022, 'learning_rate': 3.1825238412064757e-06, 'epoch': 2.8097139055222886}\n",
            "Captured loss: 1.9006 at step 4223\n",
            "Logs at step 4224: {'loss': 2.3773, 'grad_norm': 1.3473541736602783, 'learning_rate': 3.1714349079618545e-06, 'epoch': 2.810379241516966}\n",
            "Captured loss: 2.3773 at step 4224\n",
            "Logs at step 4225: {'loss': 2.5167, 'grad_norm': 1.4415969848632812, 'learning_rate': 3.1603459747172325e-06, 'epoch': 2.8110445775116433}\n",
            "Captured loss: 2.5167 at step 4225\n",
            "Logs at step 4226: {'loss': 2.2261, 'grad_norm': 1.8441723585128784, 'learning_rate': 3.1492570414726105e-06, 'epoch': 2.811709913506321}\n",
            "Captured loss: 2.2261 at step 4226\n",
            "Logs at step 4227: {'loss': 2.4606, 'grad_norm': 1.3609247207641602, 'learning_rate': 3.138168108227989e-06, 'epoch': 2.812375249500998}\n",
            "Captured loss: 2.4606 at step 4227\n",
            "Logs at step 4228: {'loss': 2.3275, 'grad_norm': 1.499243140220642, 'learning_rate': 3.127079174983367e-06, 'epoch': 2.813040585495675}\n",
            "Captured loss: 2.3275 at step 4228\n",
            "Logs at step 4229: {'loss': 2.5193, 'grad_norm': 1.7218564748764038, 'learning_rate': 3.115990241738745e-06, 'epoch': 2.8137059214903526}\n",
            "Captured loss: 2.5193 at step 4229\n",
            "Logs at step 4230: {'loss': 2.4568, 'grad_norm': 1.1926543712615967, 'learning_rate': 3.104901308494123e-06, 'epoch': 2.81437125748503}\n",
            "Captured loss: 2.4568 at step 4230\n",
            "Logs at step 4231: {'loss': 2.4182, 'grad_norm': 1.4421340227127075, 'learning_rate': 3.093812375249501e-06, 'epoch': 2.8150365934797072}\n",
            "Captured loss: 2.4182 at step 4231\n",
            "Logs at step 4232: {'loss': 2.4145, 'grad_norm': 2.039435863494873, 'learning_rate': 3.0827234420048796e-06, 'epoch': 2.8157019294743844}\n",
            "Captured loss: 2.4145 at step 4232\n",
            "Logs at step 4233: {'loss': 2.495, 'grad_norm': 1.621749997138977, 'learning_rate': 3.0716345087602576e-06, 'epoch': 2.816367265469062}\n",
            "Captured loss: 2.495 at step 4233\n",
            "Logs at step 4234: {'loss': 1.4246, 'grad_norm': 2.2764458656311035, 'learning_rate': 3.0605455755156356e-06, 'epoch': 2.8170326014637395}\n",
            "Captured loss: 1.4246 at step 4234\n",
            "Logs at step 4235: {'loss': 2.5499, 'grad_norm': 1.5744580030441284, 'learning_rate': 3.0494566422710135e-06, 'epoch': 2.8176979374584166}\n",
            "Captured loss: 2.5499 at step 4235\n",
            "Logs at step 4236: {'loss': 1.9105, 'grad_norm': 1.5811798572540283, 'learning_rate': 3.038367709026392e-06, 'epoch': 2.8183632734530937}\n",
            "Captured loss: 1.9105 at step 4236\n",
            "Logs at step 4237: {'loss': 2.3533, 'grad_norm': 2.016664505004883, 'learning_rate': 3.02727877578177e-06, 'epoch': 2.8190286094477712}\n",
            "Captured loss: 2.3533 at step 4237\n",
            "Logs at step 4238: {'loss': 2.3976, 'grad_norm': 1.8229166269302368, 'learning_rate': 3.0161898425371483e-06, 'epoch': 2.8196939454424483}\n",
            "Captured loss: 2.3976 at step 4238\n",
            "Logs at step 4239: {'loss': 2.0495, 'grad_norm': 1.9775571823120117, 'learning_rate': 3.0051009092925263e-06, 'epoch': 2.820359281437126}\n",
            "Captured loss: 2.0495 at step 4239\n",
            "Logs at step 4240: {'loss': 2.3102, 'grad_norm': 2.9496161937713623, 'learning_rate': 2.9940119760479042e-06, 'epoch': 2.821024617431803}\n",
            "Captured loss: 2.3102 at step 4240\n",
            "Logs at step 4241: {'loss': 2.276, 'grad_norm': 2.093902349472046, 'learning_rate': 2.9829230428032822e-06, 'epoch': 2.82168995342648}\n",
            "Captured loss: 2.276 at step 4241\n",
            "Logs at step 4242: {'loss': 2.377, 'grad_norm': 1.4658552408218384, 'learning_rate': 2.9718341095586606e-06, 'epoch': 2.8223552894211577}\n",
            "Captured loss: 2.377 at step 4242\n",
            "Logs at step 4243: {'loss': 2.4537, 'grad_norm': 1.2917054891586304, 'learning_rate': 2.960745176314039e-06, 'epoch': 2.823020625415835}\n",
            "Captured loss: 2.4537 at step 4243\n",
            "Logs at step 4244: {'loss': 2.4426, 'grad_norm': 1.245084285736084, 'learning_rate': 2.949656243069417e-06, 'epoch': 2.8236859614105123}\n",
            "Captured loss: 2.4426 at step 4244\n",
            "Logs at step 4245: {'loss': 2.4263, 'grad_norm': 2.093888282775879, 'learning_rate': 2.938567309824795e-06, 'epoch': 2.8243512974051894}\n",
            "Captured loss: 2.4263 at step 4245\n",
            "Logs at step 4246: {'loss': 2.4493, 'grad_norm': 2.150869607925415, 'learning_rate': 2.927478376580173e-06, 'epoch': 2.825016633399867}\n",
            "Captured loss: 2.4493 at step 4246\n",
            "Logs at step 4247: {'loss': 1.6912, 'grad_norm': 2.471118211746216, 'learning_rate': 2.9163894433355513e-06, 'epoch': 2.825681969394544}\n",
            "Captured loss: 1.6912 at step 4247\n",
            "Logs at step 4248: {'loss': 1.9721, 'grad_norm': 1.7337509393692017, 'learning_rate': 2.9053005100909293e-06, 'epoch': 2.8263473053892216}\n",
            "Captured loss: 1.9721 at step 4248\n",
            "Logs at step 4249: {'loss': 2.5418, 'grad_norm': 1.3672232627868652, 'learning_rate': 2.8942115768463077e-06, 'epoch': 2.8270126413838987}\n",
            "Captured loss: 2.5418 at step 4249\n",
            "Logs at step 4250: {'loss': 1.9353, 'grad_norm': 1.8944337368011475, 'learning_rate': 2.8831226436016857e-06, 'epoch': 2.8276779773785763}\n",
            "Captured loss: 1.9353 at step 4250\n",
            "Logs at step 4251: {'loss': 2.5067, 'grad_norm': 1.3774278163909912, 'learning_rate': 2.8720337103570636e-06, 'epoch': 2.8283433133732534}\n",
            "Captured loss: 2.5067 at step 4251\n",
            "Logs at step 4252: {'loss': 1.7951, 'grad_norm': 1.797477126121521, 'learning_rate': 2.860944777112442e-06, 'epoch': 2.829008649367931}\n",
            "Captured loss: 1.7951 at step 4252\n",
            "Logs at step 4253: {'loss': 1.6923, 'grad_norm': 2.159097194671631, 'learning_rate': 2.84985584386782e-06, 'epoch': 2.829673985362608}\n",
            "Captured loss: 1.6923 at step 4253\n",
            "Logs at step 4254: {'loss': 2.0984, 'grad_norm': 2.019076347351074, 'learning_rate': 2.838766910623198e-06, 'epoch': 2.830339321357285}\n",
            "Captured loss: 2.0984 at step 4254\n",
            "Logs at step 4255: {'loss': 2.0026, 'grad_norm': 1.5682178735733032, 'learning_rate': 2.8276779773785764e-06, 'epoch': 2.8310046573519627}\n",
            "Captured loss: 2.0026 at step 4255\n",
            "Logs at step 4256: {'loss': 2.453, 'grad_norm': 1.994803547859192, 'learning_rate': 2.8165890441339544e-06, 'epoch': 2.8316699933466403}\n",
            "Captured loss: 2.453 at step 4256\n",
            "Logs at step 4257: {'loss': 2.4513, 'grad_norm': 1.1664773225784302, 'learning_rate': 2.8055001108893328e-06, 'epoch': 2.8323353293413174}\n",
            "Captured loss: 2.4513 at step 4257\n",
            "Logs at step 4258: {'loss': 1.5337, 'grad_norm': 1.7898213863372803, 'learning_rate': 2.7944111776447107e-06, 'epoch': 2.8330006653359945}\n",
            "Captured loss: 1.5337 at step 4258\n",
            "Logs at step 4259: {'loss': 2.4578, 'grad_norm': 1.628913402557373, 'learning_rate': 2.7833222444000887e-06, 'epoch': 2.833666001330672}\n",
            "Captured loss: 2.4578 at step 4259\n",
            "Logs at step 4260: {'loss': 2.3704, 'grad_norm': 1.260523796081543, 'learning_rate': 2.772233311155467e-06, 'epoch': 2.834331337325349}\n",
            "Captured loss: 2.3704 at step 4260\n",
            "Logs at step 4261: {'loss': 2.3667, 'grad_norm': 1.4021273851394653, 'learning_rate': 2.761144377910845e-06, 'epoch': 2.8349966733200267}\n",
            "Captured loss: 2.3667 at step 4261\n",
            "Logs at step 4262: {'loss': 2.2136, 'grad_norm': 1.9350467920303345, 'learning_rate': 2.7500554446662235e-06, 'epoch': 2.835662009314704}\n",
            "Captured loss: 2.2136 at step 4262\n",
            "Logs at step 4263: {'loss': 2.4291, 'grad_norm': 1.515777826309204, 'learning_rate': 2.7389665114216015e-06, 'epoch': 2.8363273453093814}\n",
            "Captured loss: 2.4291 at step 4263\n",
            "Logs at step 4264: {'loss': 2.3769, 'grad_norm': 2.256387710571289, 'learning_rate': 2.7278775781769794e-06, 'epoch': 2.8369926813040585}\n",
            "Captured loss: 2.3769 at step 4264\n",
            "Logs at step 4265: {'loss': 2.4977, 'grad_norm': 4.446108818054199, 'learning_rate': 2.7167886449323574e-06, 'epoch': 2.837658017298736}\n",
            "Captured loss: 2.4977 at step 4265\n",
            "Logs at step 4266: {'loss': 2.4214, 'grad_norm': 2.2810781002044678, 'learning_rate': 2.705699711687736e-06, 'epoch': 2.838323353293413}\n",
            "Captured loss: 2.4214 at step 4266\n",
            "Logs at step 4267: {'loss': 2.3647, 'grad_norm': 1.3775588274002075, 'learning_rate': 2.6946107784431138e-06, 'epoch': 2.8389886892880902}\n",
            "Captured loss: 2.3647 at step 4267\n",
            "Logs at step 4268: {'loss': 2.3872, 'grad_norm': 1.890758752822876, 'learning_rate': 2.683521845198492e-06, 'epoch': 2.839654025282768}\n",
            "Captured loss: 2.3872 at step 4268\n",
            "Logs at step 4269: {'loss': 2.2132, 'grad_norm': 1.940704345703125, 'learning_rate': 2.67243291195387e-06, 'epoch': 2.8403193612774453}\n",
            "Captured loss: 2.2132 at step 4269\n",
            "Logs at step 4270: {'loss': 2.3732, 'grad_norm': 2.412440061569214, 'learning_rate': 2.661343978709248e-06, 'epoch': 2.8409846972721224}\n",
            "Captured loss: 2.3732 at step 4270\n",
            "Logs at step 4271: {'loss': 2.0081, 'grad_norm': 1.7879506349563599, 'learning_rate': 2.650255045464626e-06, 'epoch': 2.8416500332667995}\n",
            "Captured loss: 2.0081 at step 4271\n",
            "Logs at step 4272: {'loss': 2.3607, 'grad_norm': 1.6191058158874512, 'learning_rate': 2.6391661122200045e-06, 'epoch': 2.842315369261477}\n",
            "Captured loss: 2.3607 at step 4272\n",
            "Logs at step 4273: {'loss': 1.9264, 'grad_norm': 1.8651984930038452, 'learning_rate': 2.628077178975383e-06, 'epoch': 2.842980705256154}\n",
            "Captured loss: 1.9264 at step 4273\n",
            "Logs at step 4274: {'loss': 2.3962, 'grad_norm': 1.144515872001648, 'learning_rate': 2.616988245730761e-06, 'epoch': 2.8436460412508318}\n",
            "Captured loss: 2.3962 at step 4274\n",
            "Logs at step 4275: {'loss': 2.2067, 'grad_norm': 1.56425142288208, 'learning_rate': 2.605899312486139e-06, 'epoch': 2.844311377245509}\n",
            "Captured loss: 2.2067 at step 4275\n",
            "Logs at step 4276: {'loss': 2.2542, 'grad_norm': 1.4741990566253662, 'learning_rate': 2.594810379241517e-06, 'epoch': 2.8449767132401864}\n",
            "Captured loss: 2.2542 at step 4276\n",
            "Logs at step 4277: {'loss': 2.5795, 'grad_norm': 1.6080390214920044, 'learning_rate': 2.583721445996895e-06, 'epoch': 2.8456420492348635}\n",
            "Captured loss: 2.5795 at step 4277\n",
            "Logs at step 4278: {'loss': 2.5694, 'grad_norm': 2.3344204425811768, 'learning_rate': 2.5726325127522736e-06, 'epoch': 2.846307385229541}\n",
            "Captured loss: 2.5694 at step 4278\n",
            "Logs at step 4279: {'loss': 2.4123, 'grad_norm': 1.5243877172470093, 'learning_rate': 2.5615435795076516e-06, 'epoch': 2.846972721224218}\n",
            "Captured loss: 2.4123 at step 4279\n",
            "Logs at step 4280: {'loss': 2.2225, 'grad_norm': 1.5861551761627197, 'learning_rate': 2.5504546462630296e-06, 'epoch': 2.8476380572188953}\n",
            "Captured loss: 2.2225 at step 4280\n",
            "Logs at step 4281: {'loss': 1.9311, 'grad_norm': 1.8632540702819824, 'learning_rate': 2.5393657130184075e-06, 'epoch': 2.848303393213573}\n",
            "Captured loss: 1.9311 at step 4281\n",
            "Logs at step 4282: {'loss': 2.5225, 'grad_norm': 1.2211248874664307, 'learning_rate': 2.528276779773786e-06, 'epoch': 2.8489687292082504}\n",
            "Captured loss: 2.5225 at step 4282\n",
            "Logs at step 4283: {'loss': 2.3456, 'grad_norm': 1.9174178838729858, 'learning_rate': 2.5171878465291643e-06, 'epoch': 2.8496340652029275}\n",
            "Captured loss: 2.3456 at step 4283\n",
            "Logs at step 4284: {'loss': 1.9375, 'grad_norm': 1.6904010772705078, 'learning_rate': 2.5060989132845423e-06, 'epoch': 2.8502994011976046}\n",
            "Captured loss: 1.9375 at step 4284\n",
            "Logs at step 4285: {'loss': 2.5436, 'grad_norm': 1.9571433067321777, 'learning_rate': 2.4950099800399203e-06, 'epoch': 2.850964737192282}\n",
            "Captured loss: 2.5436 at step 4285\n",
            "Logs at step 4286: {'loss': 2.3971, 'grad_norm': 1.5463647842407227, 'learning_rate': 2.4839210467952982e-06, 'epoch': 2.8516300731869593}\n",
            "Captured loss: 2.3971 at step 4286\n",
            "Logs at step 4287: {'loss': 2.513, 'grad_norm': 1.2323607206344604, 'learning_rate': 2.4728321135506762e-06, 'epoch': 2.852295409181637}\n",
            "Captured loss: 2.513 at step 4287\n",
            "Logs at step 4288: {'loss': 1.9612, 'grad_norm': 1.8425159454345703, 'learning_rate': 2.461743180306055e-06, 'epoch': 2.852960745176314}\n",
            "Captured loss: 1.9612 at step 4288\n",
            "Logs at step 4289: {'loss': 2.4237, 'grad_norm': 1.658785343170166, 'learning_rate': 2.450654247061433e-06, 'epoch': 2.8536260811709915}\n",
            "Captured loss: 2.4237 at step 4289\n",
            "Logs at step 4290: {'loss': 2.3405, 'grad_norm': 1.725263237953186, 'learning_rate': 2.439565313816811e-06, 'epoch': 2.8542914171656686}\n",
            "Captured loss: 2.3405 at step 4290\n",
            "Logs at step 4291: {'loss': 2.3761, 'grad_norm': 1.4174094200134277, 'learning_rate': 2.428476380572189e-06, 'epoch': 2.854956753160346}\n",
            "Captured loss: 2.3761 at step 4291\n",
            "Logs at step 4292: {'loss': 2.2831, 'grad_norm': 1.3552744388580322, 'learning_rate': 2.417387447327567e-06, 'epoch': 2.8556220891550232}\n",
            "Captured loss: 2.2831 at step 4292\n",
            "Logs at step 4293: {'loss': 2.3901, 'grad_norm': 1.2841334342956543, 'learning_rate': 2.4062985140829453e-06, 'epoch': 2.8562874251497004}\n",
            "Captured loss: 2.3901 at step 4293\n",
            "Logs at step 4294: {'loss': 2.4375, 'grad_norm': 2.018209934234619, 'learning_rate': 2.3952095808383237e-06, 'epoch': 2.856952761144378}\n",
            "Captured loss: 2.4375 at step 4294\n",
            "Logs at step 4295: {'loss': 2.1114, 'grad_norm': 1.8901933431625366, 'learning_rate': 2.3841206475937017e-06, 'epoch': 2.8576180971390555}\n",
            "Captured loss: 2.1114 at step 4295\n",
            "Logs at step 4296: {'loss': 2.6157, 'grad_norm': 1.8487114906311035, 'learning_rate': 2.3730317143490797e-06, 'epoch': 2.8582834331337326}\n",
            "Captured loss: 2.6157 at step 4296\n",
            "Logs at step 4297: {'loss': 2.2026, 'grad_norm': 1.7508533000946045, 'learning_rate': 2.3619427811044577e-06, 'epoch': 2.8589487691284097}\n",
            "Captured loss: 2.2026 at step 4297\n",
            "Logs at step 4298: {'loss': 2.0742, 'grad_norm': 2.5069162845611572, 'learning_rate': 2.350853847859836e-06, 'epoch': 2.8596141051230872}\n",
            "Captured loss: 2.0742 at step 4298\n",
            "Logs at step 4299: {'loss': 2.2426, 'grad_norm': 1.5613937377929688, 'learning_rate': 2.339764914615214e-06, 'epoch': 2.8602794411177643}\n",
            "Captured loss: 2.2426 at step 4299\n",
            "Logs at step 4300: {'loss': 2.4091, 'grad_norm': 1.691540002822876, 'learning_rate': 2.3286759813705924e-06, 'epoch': 2.860944777112442}\n",
            "Captured loss: 2.4091 at step 4300\n",
            "Logs at step 4301: {'loss': 2.3021, 'grad_norm': 1.727713704109192, 'learning_rate': 2.3175870481259704e-06, 'epoch': 2.861610113107119}\n",
            "Captured loss: 2.3021 at step 4301\n",
            "Logs at step 4302: {'loss': 2.0325, 'grad_norm': 1.9003053903579712, 'learning_rate': 2.3064981148813484e-06, 'epoch': 2.8622754491017965}\n",
            "Captured loss: 2.0325 at step 4302\n",
            "Logs at step 4303: {'loss': 2.3913, 'grad_norm': 1.2184555530548096, 'learning_rate': 2.2954091816367268e-06, 'epoch': 2.8629407850964737}\n",
            "Captured loss: 2.3913 at step 4303\n",
            "Logs at step 4304: {'loss': 2.1227, 'grad_norm': 2.1815881729125977, 'learning_rate': 2.2843202483921047e-06, 'epoch': 2.863606121091151}\n",
            "Captured loss: 2.1227 at step 4304\n",
            "Logs at step 4305: {'loss': 1.876, 'grad_norm': 1.6711822748184204, 'learning_rate': 2.273231315147483e-06, 'epoch': 2.8642714570858283}\n",
            "Captured loss: 1.876 at step 4305\n",
            "Logs at step 4306: {'loss': 2.4137, 'grad_norm': 2.0701332092285156, 'learning_rate': 2.262142381902861e-06, 'epoch': 2.8649367930805054}\n",
            "Captured loss: 2.4137 at step 4306\n",
            "Logs at step 4307: {'loss': 2.5635, 'grad_norm': 1.2415797710418701, 'learning_rate': 2.251053448658239e-06, 'epoch': 2.865602129075183}\n",
            "Captured loss: 2.5635 at step 4307\n",
            "Logs at step 4308: {'loss': 2.366, 'grad_norm': 1.5285961627960205, 'learning_rate': 2.2399645154136175e-06, 'epoch': 2.8662674650698605}\n",
            "Captured loss: 2.366 at step 4308\n",
            "Logs at step 4309: {'loss': 2.0984, 'grad_norm': 2.132664918899536, 'learning_rate': 2.2288755821689955e-06, 'epoch': 2.8669328010645376}\n",
            "Captured loss: 2.0984 at step 4309\n",
            "Logs at step 4310: {'loss': 1.8708, 'grad_norm': 2.1138784885406494, 'learning_rate': 2.2177866489243734e-06, 'epoch': 2.8675981370592147}\n",
            "Captured loss: 1.8708 at step 4310\n",
            "Logs at step 4311: {'loss': 2.5603, 'grad_norm': 2.1699764728546143, 'learning_rate': 2.206697715679752e-06, 'epoch': 2.8682634730538923}\n",
            "Captured loss: 2.5603 at step 4311\n",
            "Logs at step 4312: {'loss': 2.3878, 'grad_norm': 1.522314190864563, 'learning_rate': 2.19560878243513e-06, 'epoch': 2.8689288090485694}\n",
            "Captured loss: 2.3878 at step 4312\n",
            "Logs at step 4313: {'loss': 1.946, 'grad_norm': 1.4563794136047363, 'learning_rate': 2.184519849190508e-06, 'epoch': 2.869594145043247}\n",
            "Captured loss: 1.946 at step 4313\n",
            "Logs at step 4314: {'loss': 2.108, 'grad_norm': 1.4249738454818726, 'learning_rate': 2.173430915945886e-06, 'epoch': 2.870259481037924}\n",
            "Captured loss: 2.108 at step 4314\n",
            "Logs at step 4315: {'loss': 1.9738, 'grad_norm': 2.0630369186401367, 'learning_rate': 2.162341982701264e-06, 'epoch': 2.8709248170326016}\n",
            "Captured loss: 1.9738 at step 4315\n",
            "Logs at step 4316: {'loss': 2.6182, 'grad_norm': 1.9277501106262207, 'learning_rate': 2.151253049456642e-06, 'epoch': 2.8715901530272787}\n",
            "Captured loss: 2.6182 at step 4316\n",
            "Logs at step 4317: {'loss': 2.5013, 'grad_norm': 1.846375584602356, 'learning_rate': 2.1401641162120205e-06, 'epoch': 2.8722554890219563}\n",
            "Captured loss: 2.5013 at step 4317\n",
            "Logs at step 4318: {'loss': 2.5026, 'grad_norm': 1.178291916847229, 'learning_rate': 2.1290751829673985e-06, 'epoch': 2.8729208250166334}\n",
            "Captured loss: 2.5026 at step 4318\n",
            "Logs at step 4319: {'loss': 2.5893, 'grad_norm': 1.756504774093628, 'learning_rate': 2.117986249722777e-06, 'epoch': 2.8735861610113105}\n",
            "Captured loss: 2.5893 at step 4319\n",
            "Logs at step 4320: {'loss': 2.0751, 'grad_norm': 1.6900361776351929, 'learning_rate': 2.106897316478155e-06, 'epoch': 2.874251497005988}\n",
            "Captured loss: 2.0751 at step 4320\n",
            "Logs at step 4321: {'loss': 2.3467, 'grad_norm': 2.763951539993286, 'learning_rate': 2.095808383233533e-06, 'epoch': 2.8749168330006656}\n",
            "Captured loss: 2.3467 at step 4321\n",
            "Logs at step 4322: {'loss': 2.5245, 'grad_norm': 1.4561673402786255, 'learning_rate': 2.0847194499889112e-06, 'epoch': 2.8755821689953427}\n",
            "Captured loss: 2.5245 at step 4322\n",
            "Logs at step 4323: {'loss': 2.0979, 'grad_norm': 2.2755324840545654, 'learning_rate': 2.0736305167442892e-06, 'epoch': 2.87624750499002}\n",
            "Captured loss: 2.0979 at step 4323\n",
            "Logs at step 4324: {'loss': 2.2663, 'grad_norm': 2.0604283809661865, 'learning_rate': 2.0625415834996676e-06, 'epoch': 2.8769128409846974}\n",
            "Captured loss: 2.2663 at step 4324\n",
            "Logs at step 4325: {'loss': 2.3765, 'grad_norm': 1.3968549966812134, 'learning_rate': 2.0514526502550456e-06, 'epoch': 2.8775781769793745}\n",
            "Captured loss: 2.3765 at step 4325\n",
            "Logs at step 4326: {'loss': 2.4482, 'grad_norm': 1.1770416498184204, 'learning_rate': 2.0403637170104236e-06, 'epoch': 2.878243512974052}\n",
            "Captured loss: 2.4482 at step 4326\n",
            "Logs at step 4327: {'loss': 2.2601, 'grad_norm': 1.6475193500518799, 'learning_rate': 2.0292747837658015e-06, 'epoch': 2.878908848968729}\n",
            "Captured loss: 2.2601 at step 4327\n",
            "Logs at step 4328: {'loss': 2.4451, 'grad_norm': 1.4348623752593994, 'learning_rate': 2.01818585052118e-06, 'epoch': 2.8795741849634067}\n",
            "Captured loss: 2.4451 at step 4328\n",
            "Logs at step 4329: {'loss': 2.3803, 'grad_norm': 1.6501593589782715, 'learning_rate': 2.0070969172765583e-06, 'epoch': 2.8802395209580838}\n",
            "Captured loss: 2.3803 at step 4329\n",
            "Logs at step 4330: {'loss': 1.9786, 'grad_norm': 2.334012031555176, 'learning_rate': 1.9960079840319363e-06, 'epoch': 2.8809048569527613}\n",
            "Captured loss: 1.9786 at step 4330\n",
            "Logs at step 4331: {'loss': 2.4063, 'grad_norm': 1.9552569389343262, 'learning_rate': 1.9849190507873143e-06, 'epoch': 2.8815701929474384}\n",
            "Captured loss: 2.4063 at step 4331\n",
            "Logs at step 4332: {'loss': 2.6173, 'grad_norm': 1.573020100593567, 'learning_rate': 1.9738301175426923e-06, 'epoch': 2.8822355289421155}\n",
            "Captured loss: 2.6173 at step 4332\n",
            "Logs at step 4333: {'loss': 2.1798, 'grad_norm': 1.404747486114502, 'learning_rate': 1.9627411842980706e-06, 'epoch': 2.882900864936793}\n",
            "Captured loss: 2.1798 at step 4333\n",
            "Logs at step 4334: {'loss': 1.7874, 'grad_norm': 2.0607759952545166, 'learning_rate': 1.951652251053449e-06, 'epoch': 2.8835662009314706}\n",
            "Captured loss: 1.7874 at step 4334\n",
            "Logs at step 4335: {'loss': 2.5054, 'grad_norm': 1.8452377319335938, 'learning_rate': 1.940563317808827e-06, 'epoch': 2.8842315369261478}\n",
            "Captured loss: 2.5054 at step 4335\n",
            "Logs at step 4336: {'loss': 2.0726, 'grad_norm': 2.029353618621826, 'learning_rate': 1.929474384564205e-06, 'epoch': 2.884896872920825}\n",
            "Captured loss: 2.0726 at step 4336\n",
            "Logs at step 4337: {'loss': 2.3876, 'grad_norm': 2.1211609840393066, 'learning_rate': 1.918385451319583e-06, 'epoch': 2.8855622089155024}\n",
            "Captured loss: 2.3876 at step 4337\n",
            "Logs at step 4338: {'loss': 2.6385, 'grad_norm': 1.5629758834838867, 'learning_rate': 1.9072965180749612e-06, 'epoch': 2.8862275449101795}\n",
            "Captured loss: 2.6385 at step 4338\n",
            "Logs at step 4339: {'loss': 2.4565, 'grad_norm': 1.6168041229248047, 'learning_rate': 1.8962075848303396e-06, 'epoch': 2.886892880904857}\n",
            "Captured loss: 2.4565 at step 4339\n",
            "Logs at step 4340: {'loss': 2.1853, 'grad_norm': 1.823919415473938, 'learning_rate': 1.8851186515857175e-06, 'epoch': 2.887558216899534}\n",
            "Captured loss: 2.1853 at step 4340\n",
            "Logs at step 4341: {'loss': 2.4329, 'grad_norm': 1.6657416820526123, 'learning_rate': 1.8740297183410957e-06, 'epoch': 2.8882235528942117}\n",
            "Captured loss: 2.4329 at step 4341\n",
            "Logs at step 4342: {'loss': 2.2895, 'grad_norm': 1.959765911102295, 'learning_rate': 1.8629407850964737e-06, 'epoch': 2.888888888888889}\n",
            "Captured loss: 2.2895 at step 4342\n",
            "Logs at step 4343: {'loss': 2.0855, 'grad_norm': 1.7785735130310059, 'learning_rate': 1.8518518518518519e-06, 'epoch': 2.8895542248835664}\n",
            "Captured loss: 2.0855 at step 4343\n",
            "Logs at step 4344: {'loss': 1.9459, 'grad_norm': 1.6113783121109009, 'learning_rate': 1.8407629186072298e-06, 'epoch': 2.8902195608782435}\n",
            "Captured loss: 1.9459 at step 4344\n",
            "Logs at step 4345: {'loss': 1.9486, 'grad_norm': 1.8238739967346191, 'learning_rate': 1.8296739853626082e-06, 'epoch': 2.8908848968729206}\n",
            "Captured loss: 1.9486 at step 4345\n",
            "Logs at step 4346: {'loss': 2.4721, 'grad_norm': 1.3261979818344116, 'learning_rate': 1.8185850521179864e-06, 'epoch': 2.891550232867598}\n",
            "Captured loss: 2.4721 at step 4346\n",
            "Logs at step 4347: {'loss': 2.3284, 'grad_norm': 1.8408430814743042, 'learning_rate': 1.8074961188733644e-06, 'epoch': 2.8922155688622757}\n",
            "Captured loss: 2.3284 at step 4347\n",
            "Logs at step 4348: {'loss': 2.3844, 'grad_norm': 1.0031110048294067, 'learning_rate': 1.7964071856287426e-06, 'epoch': 2.892880904856953}\n",
            "Captured loss: 2.3844 at step 4348\n",
            "Logs at step 4349: {'loss': 1.8814, 'grad_norm': 2.10440993309021, 'learning_rate': 1.7853182523841206e-06, 'epoch': 2.89354624085163}\n",
            "Captured loss: 1.8814 at step 4349\n",
            "Logs at step 4350: {'loss': 1.8286, 'grad_norm': 2.6112353801727295, 'learning_rate': 1.774229319139499e-06, 'epoch': 2.8942115768463075}\n",
            "Captured loss: 1.8286 at step 4350\n",
            "Logs at step 4351: {'loss': 2.1565, 'grad_norm': 2.5057475566864014, 'learning_rate': 1.7631403858948771e-06, 'epoch': 2.8948769128409846}\n",
            "Captured loss: 2.1565 at step 4351\n",
            "Logs at step 4352: {'loss': 2.0829, 'grad_norm': 2.6422293186187744, 'learning_rate': 1.7520514526502551e-06, 'epoch': 2.895542248835662}\n",
            "Captured loss: 2.0829 at step 4352\n",
            "Logs at step 4353: {'loss': 2.6205, 'grad_norm': 2.0998616218566895, 'learning_rate': 1.7409625194056333e-06, 'epoch': 2.8962075848303392}\n",
            "Captured loss: 2.6205 at step 4353\n",
            "Logs at step 4354: {'loss': 2.3789, 'grad_norm': 2.166423797607422, 'learning_rate': 1.7298735861610113e-06, 'epoch': 2.896872920825017}\n",
            "Captured loss: 2.3789 at step 4354\n",
            "Logs at step 4355: {'loss': 2.4466, 'grad_norm': 2.153430700302124, 'learning_rate': 1.7187846529163895e-06, 'epoch': 2.897538256819694}\n",
            "Captured loss: 2.4466 at step 4355\n",
            "Logs at step 4356: {'loss': 2.2579, 'grad_norm': 1.982633352279663, 'learning_rate': 1.7076957196717679e-06, 'epoch': 2.8982035928143715}\n",
            "Captured loss: 2.2579 at step 4356\n",
            "Logs at step 4357: {'loss': 2.2938, 'grad_norm': 1.602197527885437, 'learning_rate': 1.6966067864271458e-06, 'epoch': 2.8988689288090486}\n",
            "Captured loss: 2.2938 at step 4357\n",
            "Logs at step 4358: {'loss': 2.4229, 'grad_norm': 1.1349397897720337, 'learning_rate': 1.685517853182524e-06, 'epoch': 2.8995342648037257}\n",
            "Captured loss: 2.4229 at step 4358\n",
            "Logs at step 4359: {'loss': 2.1487, 'grad_norm': 2.0477962493896484, 'learning_rate': 1.674428919937902e-06, 'epoch': 2.900199600798403}\n",
            "Captured loss: 2.1487 at step 4359\n",
            "Logs at step 4360: {'loss': 2.3016, 'grad_norm': 2.0744452476501465, 'learning_rate': 1.66333998669328e-06, 'epoch': 2.9008649367930808}\n",
            "Captured loss: 2.3016 at step 4360\n",
            "Logs at step 4361: {'loss': 2.2558, 'grad_norm': 1.5905206203460693, 'learning_rate': 1.6522510534486582e-06, 'epoch': 2.901530272787758}\n",
            "Captured loss: 2.2558 at step 4361\n",
            "Logs at step 4362: {'loss': 2.2809, 'grad_norm': 1.512357473373413, 'learning_rate': 1.6411621202040366e-06, 'epoch': 2.902195608782435}\n",
            "Captured loss: 2.2809 at step 4362\n",
            "Logs at step 4363: {'loss': 2.3674, 'grad_norm': 1.3712385892868042, 'learning_rate': 1.6300731869594147e-06, 'epoch': 2.9028609447771125}\n",
            "Captured loss: 2.3674 at step 4363\n",
            "Logs at step 4364: {'loss': 2.6918, 'grad_norm': 2.5087788105010986, 'learning_rate': 1.6189842537147927e-06, 'epoch': 2.9035262807717896}\n",
            "Captured loss: 2.6918 at step 4364\n",
            "Logs at step 4365: {'loss': 2.3746, 'grad_norm': 2.029423713684082, 'learning_rate': 1.6078953204701707e-06, 'epoch': 2.904191616766467}\n",
            "Captured loss: 2.3746 at step 4365\n",
            "Logs at step 4366: {'loss': 2.2638, 'grad_norm': 2.0582175254821777, 'learning_rate': 1.5968063872255489e-06, 'epoch': 2.9048569527611443}\n",
            "Captured loss: 2.2638 at step 4366\n",
            "Logs at step 4367: {'loss': 1.8778, 'grad_norm': 1.6654740571975708, 'learning_rate': 1.5857174539809273e-06, 'epoch': 2.9055222887558214}\n",
            "Captured loss: 1.8778 at step 4367\n",
            "Logs at step 4368: {'loss': 2.1961, 'grad_norm': 1.5798557996749878, 'learning_rate': 1.5746285207363052e-06, 'epoch': 2.906187624750499}\n",
            "Captured loss: 2.1961 at step 4368\n",
            "Logs at step 4369: {'loss': 2.5803, 'grad_norm': 2.3281424045562744, 'learning_rate': 1.5635395874916834e-06, 'epoch': 2.9068529607451765}\n",
            "Captured loss: 2.5803 at step 4369\n",
            "Logs at step 4370: {'loss': 2.5232, 'grad_norm': 1.5127742290496826, 'learning_rate': 1.5524506542470614e-06, 'epoch': 2.9075182967398536}\n",
            "Captured loss: 2.5232 at step 4370\n",
            "Logs at step 4371: {'loss': 2.4049, 'grad_norm': 1.6910473108291626, 'learning_rate': 1.5413617210024398e-06, 'epoch': 2.9081836327345307}\n",
            "Captured loss: 2.4049 at step 4371\n",
            "Logs at step 4372: {'loss': 2.1459, 'grad_norm': 2.0187206268310547, 'learning_rate': 1.5302727877578178e-06, 'epoch': 2.9088489687292083}\n",
            "Captured loss: 2.1459 at step 4372\n",
            "Logs at step 4373: {'loss': 2.4745, 'grad_norm': 1.8074547052383423, 'learning_rate': 1.519183854513196e-06, 'epoch': 2.909514304723886}\n",
            "Captured loss: 2.4745 at step 4373\n",
            "Logs at step 4374: {'loss': 2.5364, 'grad_norm': 1.626926064491272, 'learning_rate': 1.5080949212685741e-06, 'epoch': 2.910179640718563}\n",
            "Captured loss: 2.5364 at step 4374\n",
            "Logs at step 4375: {'loss': 1.4886, 'grad_norm': 1.9063982963562012, 'learning_rate': 1.4970059880239521e-06, 'epoch': 2.91084497671324}\n",
            "Captured loss: 1.4886 at step 4375\n",
            "Logs at step 4376: {'loss': 2.2879, 'grad_norm': 1.421032190322876, 'learning_rate': 1.4859170547793303e-06, 'epoch': 2.9115103127079176}\n",
            "Captured loss: 2.2879 at step 4376\n",
            "Logs at step 4377: {'loss': 2.1964, 'grad_norm': 1.596171498298645, 'learning_rate': 1.4748281215347085e-06, 'epoch': 2.9121756487025947}\n",
            "Captured loss: 2.1964 at step 4377\n",
            "Logs at step 4378: {'loss': 2.0431, 'grad_norm': 1.7711411714553833, 'learning_rate': 1.4637391882900865e-06, 'epoch': 2.9128409846972723}\n",
            "Captured loss: 2.0431 at step 4378\n",
            "Logs at step 4379: {'loss': 2.1362, 'grad_norm': 1.898690938949585, 'learning_rate': 1.4526502550454647e-06, 'epoch': 2.9135063206919494}\n",
            "Captured loss: 2.1362 at step 4379\n",
            "Logs at step 4380: {'loss': 2.3683, 'grad_norm': 1.8568555116653442, 'learning_rate': 1.4415613218008428e-06, 'epoch': 2.9141716566866265}\n",
            "Captured loss: 2.3683 at step 4380\n",
            "Logs at step 4381: {'loss': 2.4376, 'grad_norm': 1.2206377983093262, 'learning_rate': 1.430472388556221e-06, 'epoch': 2.914836992681304}\n",
            "Captured loss: 2.4376 at step 4381\n",
            "Logs at step 4382: {'loss': 1.9512, 'grad_norm': 1.7316679954528809, 'learning_rate': 1.419383455311599e-06, 'epoch': 2.9155023286759816}\n",
            "Captured loss: 1.9512 at step 4382\n",
            "Logs at step 4383: {'loss': 2.5239, 'grad_norm': 1.9409446716308594, 'learning_rate': 1.4082945220669772e-06, 'epoch': 2.9161676646706587}\n",
            "Captured loss: 2.5239 at step 4383\n",
            "Logs at step 4384: {'loss': 2.5199, 'grad_norm': 1.4899694919586182, 'learning_rate': 1.3972055888223554e-06, 'epoch': 2.916833000665336}\n",
            "Captured loss: 2.5199 at step 4384\n",
            "Logs at step 4385: {'loss': 2.2754, 'grad_norm': 1.3602834939956665, 'learning_rate': 1.3861166555777336e-06, 'epoch': 2.9174983366600133}\n",
            "Captured loss: 2.2754 at step 4385\n",
            "Logs at step 4386: {'loss': 2.5816, 'grad_norm': 1.211118221282959, 'learning_rate': 1.3750277223331117e-06, 'epoch': 2.9181636726546905}\n",
            "Captured loss: 2.5816 at step 4386\n",
            "Logs at step 4387: {'loss': 2.569, 'grad_norm': 1.3491370677947998, 'learning_rate': 1.3639387890884897e-06, 'epoch': 2.918829008649368}\n",
            "Captured loss: 2.569 at step 4387\n",
            "Logs at step 4388: {'loss': 1.9669, 'grad_norm': 2.6234652996063232, 'learning_rate': 1.352849855843868e-06, 'epoch': 2.919494344644045}\n",
            "Captured loss: 1.9669 at step 4388\n",
            "Logs at step 4389: {'loss': 1.8418, 'grad_norm': 1.9304842948913574, 'learning_rate': 1.341760922599246e-06, 'epoch': 2.9201596806387227}\n",
            "Captured loss: 1.8418 at step 4389\n",
            "Logs at step 4390: {'loss': 2.0828, 'grad_norm': 1.736134648323059, 'learning_rate': 1.330671989354624e-06, 'epoch': 2.9208250166333998}\n",
            "Captured loss: 2.0828 at step 4390\n",
            "Logs at step 4391: {'loss': 2.3683, 'grad_norm': 1.2735276222229004, 'learning_rate': 1.3195830561100022e-06, 'epoch': 2.9214903526280773}\n",
            "Captured loss: 2.3683 at step 4391\n",
            "Logs at step 4392: {'loss': 2.2632, 'grad_norm': 1.7468804121017456, 'learning_rate': 1.3084941228653804e-06, 'epoch': 2.9221556886227544}\n",
            "Captured loss: 2.2632 at step 4392\n",
            "Logs at step 4393: {'loss': 2.4932, 'grad_norm': 1.0839751958847046, 'learning_rate': 1.2974051896207584e-06, 'epoch': 2.9228210246174315}\n",
            "Captured loss: 2.4932 at step 4393\n",
            "Logs at step 4394: {'loss': 2.4029, 'grad_norm': 1.3485567569732666, 'learning_rate': 1.2863162563761368e-06, 'epoch': 2.923486360612109}\n",
            "Captured loss: 2.4029 at step 4394\n",
            "Logs at step 4395: {'loss': 2.4143, 'grad_norm': 1.3832257986068726, 'learning_rate': 1.2752273231315148e-06, 'epoch': 2.9241516966067866}\n",
            "Captured loss: 2.4143 at step 4395\n",
            "Logs at step 4396: {'loss': 2.5643, 'grad_norm': 1.2008330821990967, 'learning_rate': 1.264138389886893e-06, 'epoch': 2.9248170326014638}\n",
            "Captured loss: 2.5643 at step 4396\n",
            "Logs at step 4397: {'loss': 1.9672, 'grad_norm': 1.714972734451294, 'learning_rate': 1.2530494566422711e-06, 'epoch': 2.925482368596141}\n",
            "Captured loss: 1.9672 at step 4397\n",
            "Logs at step 4398: {'loss': 2.1851, 'grad_norm': 1.509682059288025, 'learning_rate': 1.2419605233976491e-06, 'epoch': 2.9261477045908184}\n",
            "Captured loss: 2.1851 at step 4398\n",
            "Logs at step 4399: {'loss': 2.3752, 'grad_norm': 1.2623045444488525, 'learning_rate': 1.2308715901530275e-06, 'epoch': 2.9268130405854955}\n",
            "Captured loss: 2.3752 at step 4399\n",
            "Logs at step 4400: {'loss': 2.4782, 'grad_norm': 1.508540153503418, 'learning_rate': 1.2197826569084055e-06, 'epoch': 2.927478376580173}\n",
            "Captured loss: 2.4782 at step 4400\n",
            "Logs at step 4401: {'loss': 2.0486, 'grad_norm': 1.7998919486999512, 'learning_rate': 1.2086937236637835e-06, 'epoch': 2.92814371257485}\n",
            "Captured loss: 2.0486 at step 4401\n",
            "Logs at step 4402: {'loss': 2.2324, 'grad_norm': 2.1415374279022217, 'learning_rate': 1.1976047904191619e-06, 'epoch': 2.9288090485695277}\n",
            "Captured loss: 2.2324 at step 4402\n",
            "Logs at step 4403: {'loss': 2.7061, 'grad_norm': 2.1823408603668213, 'learning_rate': 1.1865158571745398e-06, 'epoch': 2.929474384564205}\n",
            "Captured loss: 2.7061 at step 4403\n",
            "Logs at step 4404: {'loss': 1.4857, 'grad_norm': 2.0468270778656006, 'learning_rate': 1.175426923929918e-06, 'epoch': 2.9301397205588824}\n",
            "Captured loss: 1.4857 at step 4404\n",
            "Logs at step 4405: {'loss': 2.1947, 'grad_norm': 1.533184289932251, 'learning_rate': 1.1643379906852962e-06, 'epoch': 2.9308050565535595}\n",
            "Captured loss: 2.1947 at step 4405\n",
            "Logs at step 4406: {'loss': 2.5018, 'grad_norm': 1.256641149520874, 'learning_rate': 1.1532490574406742e-06, 'epoch': 2.9314703925482366}\n",
            "Captured loss: 2.5018 at step 4406\n",
            "Logs at step 4407: {'loss': 2.4042, 'grad_norm': 1.8988298177719116, 'learning_rate': 1.1421601241960524e-06, 'epoch': 2.932135728542914}\n",
            "Captured loss: 2.4042 at step 4407\n",
            "Logs at step 4408: {'loss': 2.1819, 'grad_norm': 4.082489013671875, 'learning_rate': 1.1310711909514306e-06, 'epoch': 2.9328010645375917}\n",
            "Captured loss: 2.1819 at step 4408\n",
            "Logs at step 4409: {'loss': 2.181, 'grad_norm': 2.634153366088867, 'learning_rate': 1.1199822577068087e-06, 'epoch': 2.933466400532269}\n",
            "Captured loss: 2.181 at step 4409\n",
            "Logs at step 4410: {'loss': 2.3593, 'grad_norm': 1.1100356578826904, 'learning_rate': 1.1088933244621867e-06, 'epoch': 2.934131736526946}\n",
            "Captured loss: 2.3593 at step 4410\n",
            "Logs at step 4411: {'loss': 2.395, 'grad_norm': 1.387218713760376, 'learning_rate': 1.097804391217565e-06, 'epoch': 2.9347970725216235}\n",
            "Captured loss: 2.395 at step 4411\n",
            "Logs at step 4412: {'loss': 1.7746, 'grad_norm': 1.6933311223983765, 'learning_rate': 1.086715457972943e-06, 'epoch': 2.9354624085163006}\n",
            "Captured loss: 1.7746 at step 4412\n",
            "Logs at step 4413: {'loss': 2.1527, 'grad_norm': 2.3277738094329834, 'learning_rate': 1.075626524728321e-06, 'epoch': 2.936127744510978}\n",
            "Captured loss: 2.1527 at step 4413\n",
            "Logs at step 4414: {'loss': 1.9185, 'grad_norm': 1.8037642240524292, 'learning_rate': 1.0645375914836992e-06, 'epoch': 2.9367930805056552}\n",
            "Captured loss: 1.9185 at step 4414\n",
            "Logs at step 4415: {'loss': 2.3052, 'grad_norm': 2.1262121200561523, 'learning_rate': 1.0534486582390774e-06, 'epoch': 2.937458416500333}\n",
            "Captured loss: 2.3052 at step 4415\n",
            "Logs at step 4416: {'loss': 2.3878, 'grad_norm': 1.1864534616470337, 'learning_rate': 1.0423597249944556e-06, 'epoch': 2.93812375249501}\n",
            "Captured loss: 2.3878 at step 4416\n",
            "Logs at step 4417: {'loss': 2.0213, 'grad_norm': 1.793087124824524, 'learning_rate': 1.0312707917498338e-06, 'epoch': 2.9387890884896875}\n",
            "Captured loss: 2.0213 at step 4417\n",
            "Logs at step 4418: {'loss': 2.4494, 'grad_norm': 1.2603254318237305, 'learning_rate': 1.0201818585052118e-06, 'epoch': 2.9394544244843646}\n",
            "Captured loss: 2.4494 at step 4418\n",
            "Logs at step 4419: {'loss': 2.2068, 'grad_norm': 2.43727445602417, 'learning_rate': 1.00909292526059e-06, 'epoch': 2.9401197604790417}\n",
            "Captured loss: 2.2068 at step 4419\n",
            "Logs at step 4420: {'loss': 2.5471, 'grad_norm': 1.5367486476898193, 'learning_rate': 9.980039920159682e-07, 'epoch': 2.940785096473719}\n",
            "Captured loss: 2.5471 at step 4420\n",
            "Logs at step 4421: {'loss': 2.1258, 'grad_norm': 1.5773687362670898, 'learning_rate': 9.869150587713461e-07, 'epoch': 2.9414504324683968}\n",
            "Captured loss: 2.1258 at step 4421\n",
            "Logs at step 4422: {'loss': 2.341, 'grad_norm': 1.6165918111801147, 'learning_rate': 9.758261255267245e-07, 'epoch': 2.942115768463074}\n",
            "Captured loss: 2.341 at step 4422\n",
            "Logs at step 4423: {'loss': 1.8244, 'grad_norm': 2.2674808502197266, 'learning_rate': 9.647371922821025e-07, 'epoch': 2.942781104457751}\n",
            "Captured loss: 1.8244 at step 4423\n",
            "Logs at step 4424: {'loss': 2.3833, 'grad_norm': 2.0490875244140625, 'learning_rate': 9.536482590374806e-07, 'epoch': 2.9434464404524285}\n",
            "Captured loss: 2.3833 at step 4424\n",
            "Logs at step 4425: {'loss': 1.6807, 'grad_norm': 1.8206239938735962, 'learning_rate': 9.425593257928588e-07, 'epoch': 2.9441117764471056}\n",
            "Captured loss: 1.6807 at step 4425\n",
            "Logs at step 4426: {'loss': 1.8656, 'grad_norm': 1.8331395387649536, 'learning_rate': 9.314703925482368e-07, 'epoch': 2.944777112441783}\n",
            "Captured loss: 1.8656 at step 4426\n",
            "Logs at step 4427: {'loss': 2.0192, 'grad_norm': 1.8924767971038818, 'learning_rate': 9.203814593036149e-07, 'epoch': 2.9454424484364603}\n",
            "Captured loss: 2.0192 at step 4427\n",
            "Logs at step 4428: {'loss': 1.4795, 'grad_norm': 1.8961478471755981, 'learning_rate': 9.092925260589932e-07, 'epoch': 2.946107784431138}\n",
            "Captured loss: 1.4795 at step 4428\n",
            "Logs at step 4429: {'loss': 2.5564, 'grad_norm': 1.3235528469085693, 'learning_rate': 8.982035928143713e-07, 'epoch': 2.946773120425815}\n",
            "Captured loss: 2.5564 at step 4429\n",
            "Logs at step 4430: {'loss': 2.4686, 'grad_norm': 1.4184846878051758, 'learning_rate': 8.871146595697495e-07, 'epoch': 2.9474384564204925}\n",
            "Captured loss: 2.4686 at step 4430\n",
            "Logs at step 4431: {'loss': 2.024, 'grad_norm': 2.2324886322021484, 'learning_rate': 8.760257263251276e-07, 'epoch': 2.9481037924151696}\n",
            "Captured loss: 2.024 at step 4431\n",
            "Logs at step 4432: {'loss': 2.4303, 'grad_norm': 1.5418143272399902, 'learning_rate': 8.649367930805056e-07, 'epoch': 2.9487691284098467}\n",
            "Captured loss: 2.4303 at step 4432\n",
            "Logs at step 4433: {'loss': 2.4373, 'grad_norm': 2.1347603797912598, 'learning_rate': 8.538478598358839e-07, 'epoch': 2.9494344644045243}\n",
            "Captured loss: 2.4373 at step 4433\n",
            "Logs at step 4434: {'loss': 2.0493, 'grad_norm': 1.8998610973358154, 'learning_rate': 8.42758926591262e-07, 'epoch': 2.950099800399202}\n",
            "Captured loss: 2.0493 at step 4434\n",
            "Logs at step 4435: {'loss': 2.5283, 'grad_norm': 1.3545503616333008, 'learning_rate': 8.3166999334664e-07, 'epoch': 2.950765136393879}\n",
            "Captured loss: 2.5283 at step 4435\n",
            "Logs at step 4436: {'loss': 2.4541, 'grad_norm': 1.2801110744476318, 'learning_rate': 8.205810601020183e-07, 'epoch': 2.951430472388556}\n",
            "Captured loss: 2.4541 at step 4436\n",
            "Logs at step 4437: {'loss': 2.4679, 'grad_norm': 1.3887794017791748, 'learning_rate': 8.094921268573964e-07, 'epoch': 2.9520958083832336}\n",
            "Captured loss: 2.4679 at step 4437\n",
            "Logs at step 4438: {'loss': 2.2399, 'grad_norm': 1.5138651132583618, 'learning_rate': 7.984031936127744e-07, 'epoch': 2.9527611443779107}\n",
            "Captured loss: 2.2399 at step 4438\n",
            "Logs at step 4439: {'loss': 2.5629, 'grad_norm': 1.6128392219543457, 'learning_rate': 7.873142603681526e-07, 'epoch': 2.9534264803725883}\n",
            "Captured loss: 2.5629 at step 4439\n",
            "Logs at step 4440: {'loss': 2.3774, 'grad_norm': 1.902303695678711, 'learning_rate': 7.762253271235307e-07, 'epoch': 2.9540918163672654}\n",
            "Captured loss: 2.3774 at step 4440\n",
            "Logs at step 4441: {'loss': 1.9121, 'grad_norm': 1.496541142463684, 'learning_rate': 7.651363938789089e-07, 'epoch': 2.954757152361943}\n",
            "Captured loss: 1.9121 at step 4441\n",
            "Logs at step 4442: {'loss': 1.9323, 'grad_norm': 1.6623598337173462, 'learning_rate': 7.540474606342871e-07, 'epoch': 2.95542248835662}\n",
            "Captured loss: 1.9323 at step 4442\n",
            "Logs at step 4443: {'loss': 2.4584, 'grad_norm': 2.107759952545166, 'learning_rate': 7.429585273896652e-07, 'epoch': 2.9560878243512976}\n",
            "Captured loss: 2.4584 at step 4443\n",
            "Logs at step 4444: {'loss': 2.5194, 'grad_norm': 1.9186843633651733, 'learning_rate': 7.318695941450432e-07, 'epoch': 2.9567531603459747}\n",
            "Captured loss: 2.5194 at step 4444\n",
            "Logs at step 4445: {'loss': 2.4632, 'grad_norm': 1.496265172958374, 'learning_rate': 7.207806609004214e-07, 'epoch': 2.957418496340652}\n",
            "Captured loss: 2.4632 at step 4445\n",
            "Logs at step 4446: {'loss': 1.7487, 'grad_norm': 1.7628178596496582, 'learning_rate': 7.096917276557995e-07, 'epoch': 2.9580838323353293}\n",
            "Captured loss: 1.7487 at step 4446\n",
            "Logs at step 4447: {'loss': 1.866, 'grad_norm': 2.0654566287994385, 'learning_rate': 6.986027944111777e-07, 'epoch': 2.958749168330007}\n",
            "Captured loss: 1.866 at step 4447\n",
            "Logs at step 4448: {'loss': 2.6383, 'grad_norm': 2.5794546604156494, 'learning_rate': 6.875138611665559e-07, 'epoch': 2.959414504324684}\n",
            "Captured loss: 2.6383 at step 4448\n",
            "Logs at step 4449: {'loss': 2.5122, 'grad_norm': 1.5016493797302246, 'learning_rate': 6.76424927921934e-07, 'epoch': 2.960079840319361}\n",
            "Captured loss: 2.5122 at step 4449\n",
            "Logs at step 4450: {'loss': 2.1721, 'grad_norm': 1.6778684854507446, 'learning_rate': 6.65335994677312e-07, 'epoch': 2.9607451763140387}\n",
            "Captured loss: 2.1721 at step 4450\n",
            "Logs at step 4451: {'loss': 2.2059, 'grad_norm': 1.6655375957489014, 'learning_rate': 6.542470614326902e-07, 'epoch': 2.9614105123087158}\n",
            "Captured loss: 2.2059 at step 4451\n",
            "Logs at step 4452: {'loss': 2.376, 'grad_norm': 1.6700522899627686, 'learning_rate': 6.431581281880684e-07, 'epoch': 2.9620758483033933}\n",
            "Captured loss: 2.376 at step 4452\n",
            "Logs at step 4453: {'loss': 2.4396, 'grad_norm': 1.2374531030654907, 'learning_rate': 6.320691949434465e-07, 'epoch': 2.9627411842980704}\n",
            "Captured loss: 2.4396 at step 4453\n",
            "Logs at step 4454: {'loss': 2.4427, 'grad_norm': 1.291466236114502, 'learning_rate': 6.209802616988246e-07, 'epoch': 2.963406520292748}\n",
            "Captured loss: 2.4427 at step 4454\n",
            "Logs at step 4455: {'loss': 2.387, 'grad_norm': 1.2423632144927979, 'learning_rate': 6.098913284542027e-07, 'epoch': 2.964071856287425}\n",
            "Captured loss: 2.387 at step 4455\n",
            "Logs at step 4456: {'loss': 2.3521, 'grad_norm': 2.1975739002227783, 'learning_rate': 5.988023952095809e-07, 'epoch': 2.9647371922821026}\n",
            "Captured loss: 2.3521 at step 4456\n",
            "Logs at step 4457: {'loss': 2.3811, 'grad_norm': 1.6592718362808228, 'learning_rate': 5.87713461964959e-07, 'epoch': 2.9654025282767797}\n",
            "Captured loss: 2.3811 at step 4457\n",
            "Logs at step 4458: {'loss': 1.8752, 'grad_norm': 2.1031501293182373, 'learning_rate': 5.766245287203371e-07, 'epoch': 2.966067864271457}\n",
            "Captured loss: 1.8752 at step 4458\n",
            "Logs at step 4459: {'loss': 2.0644, 'grad_norm': 1.9736244678497314, 'learning_rate': 5.655355954757153e-07, 'epoch': 2.9667332002661344}\n",
            "Captured loss: 2.0644 at step 4459\n",
            "Logs at step 4460: {'loss': 2.35, 'grad_norm': 1.425635814666748, 'learning_rate': 5.544466622310934e-07, 'epoch': 2.967398536260812}\n",
            "Captured loss: 2.35 at step 4460\n",
            "Logs at step 4461: {'loss': 2.3346, 'grad_norm': 1.7561722993850708, 'learning_rate': 5.433577289864715e-07, 'epoch': 2.968063872255489}\n",
            "Captured loss: 2.3346 at step 4461\n",
            "Logs at step 4462: {'loss': 2.5052, 'grad_norm': 1.447715163230896, 'learning_rate': 5.322687957418496e-07, 'epoch': 2.968729208250166}\n",
            "Captured loss: 2.5052 at step 4462\n",
            "Logs at step 4463: {'loss': 2.3594, 'grad_norm': 2.278383255004883, 'learning_rate': 5.211798624972278e-07, 'epoch': 2.9693945442448437}\n",
            "Captured loss: 2.3594 at step 4463\n",
            "Logs at step 4464: {'loss': 2.2365, 'grad_norm': 1.4024971723556519, 'learning_rate': 5.100909292526059e-07, 'epoch': 2.970059880239521}\n",
            "Captured loss: 2.2365 at step 4464\n",
            "Logs at step 4465: {'loss': 2.6391, 'grad_norm': 2.253049850463867, 'learning_rate': 4.990019960079841e-07, 'epoch': 2.9707252162341984}\n",
            "Captured loss: 2.6391 at step 4465\n",
            "Logs at step 4466: {'loss': 2.5129, 'grad_norm': 1.402450680732727, 'learning_rate': 4.879130627633623e-07, 'epoch': 2.9713905522288755}\n",
            "Captured loss: 2.5129 at step 4466\n",
            "Logs at step 4467: {'loss': 1.8765, 'grad_norm': 1.629951000213623, 'learning_rate': 4.768241295187403e-07, 'epoch': 2.972055888223553}\n",
            "Captured loss: 1.8765 at step 4467\n",
            "Logs at step 4468: {'loss': 1.9496, 'grad_norm': 1.6761304140090942, 'learning_rate': 4.657351962741184e-07, 'epoch': 2.97272122421823}\n",
            "Captured loss: 1.9496 at step 4468\n",
            "Logs at step 4469: {'loss': 2.429, 'grad_norm': 1.8188725709915161, 'learning_rate': 4.546462630294966e-07, 'epoch': 2.9733865602129077}\n",
            "Captured loss: 2.429 at step 4469\n",
            "Logs at step 4470: {'loss': 2.502, 'grad_norm': 1.5617640018463135, 'learning_rate': 4.4355732978487474e-07, 'epoch': 2.974051896207585}\n",
            "Captured loss: 2.502 at step 4470\n",
            "Logs at step 4471: {'loss': 2.298, 'grad_norm': 2.532186508178711, 'learning_rate': 4.324683965402528e-07, 'epoch': 2.974717232202262}\n",
            "Captured loss: 2.298 at step 4471\n",
            "Logs at step 4472: {'loss': 2.2168, 'grad_norm': 1.7456696033477783, 'learning_rate': 4.21379463295631e-07, 'epoch': 2.9753825681969395}\n",
            "Captured loss: 2.2168 at step 4472\n",
            "Logs at step 4473: {'loss': 2.2765, 'grad_norm': 1.5901671648025513, 'learning_rate': 4.1029053005100914e-07, 'epoch': 2.976047904191617}\n",
            "Captured loss: 2.2765 at step 4473\n",
            "Logs at step 4474: {'loss': 2.1806, 'grad_norm': 2.087151527404785, 'learning_rate': 3.992015968063872e-07, 'epoch': 2.976713240186294}\n",
            "Captured loss: 2.1806 at step 4474\n",
            "Logs at step 4475: {'loss': 1.8456, 'grad_norm': 1.5753264427185059, 'learning_rate': 3.8811266356176535e-07, 'epoch': 2.9773785761809712}\n",
            "Captured loss: 1.8456 at step 4475\n",
            "Logs at step 4476: {'loss': 1.9992, 'grad_norm': 1.9002798795700073, 'learning_rate': 3.7702373031714354e-07, 'epoch': 2.978043912175649}\n",
            "Captured loss: 1.9992 at step 4476\n",
            "Logs at step 4477: {'loss': 2.5686, 'grad_norm': 2.6245436668395996, 'learning_rate': 3.659347970725216e-07, 'epoch': 2.978709248170326}\n",
            "Captured loss: 2.5686 at step 4477\n",
            "Logs at step 4478: {'loss': 2.5416, 'grad_norm': 1.8726733922958374, 'learning_rate': 3.5484586382789975e-07, 'epoch': 2.9793745841650034}\n",
            "Captured loss: 2.5416 at step 4478\n",
            "Logs at step 4479: {'loss': 2.2724, 'grad_norm': 2.5399773120880127, 'learning_rate': 3.4375693058327794e-07, 'epoch': 2.9800399201596806}\n",
            "Captured loss: 2.2724 at step 4479\n",
            "Logs at step 4480: {'loss': 2.4581, 'grad_norm': 2.182291030883789, 'learning_rate': 3.32667997338656e-07, 'epoch': 2.980705256154358}\n",
            "Captured loss: 2.4581 at step 4480\n",
            "Logs at step 4481: {'loss': 1.8671, 'grad_norm': 2.3829612731933594, 'learning_rate': 3.215790640940342e-07, 'epoch': 2.981370592149035}\n",
            "Captured loss: 1.8671 at step 4481\n",
            "Logs at step 4482: {'loss': 1.7919, 'grad_norm': 1.9675112962722778, 'learning_rate': 3.104901308494123e-07, 'epoch': 2.9820359281437128}\n",
            "Captured loss: 1.7919 at step 4482\n",
            "Logs at step 4483: {'loss': 2.3471, 'grad_norm': 3.0227129459381104, 'learning_rate': 2.9940119760479047e-07, 'epoch': 2.98270126413839}\n",
            "Captured loss: 2.3471 at step 4483\n",
            "Logs at step 4484: {'loss': 2.6498, 'grad_norm': 2.317840099334717, 'learning_rate': 2.8831226436016855e-07, 'epoch': 2.983366600133067}\n",
            "Captured loss: 2.6498 at step 4484\n",
            "Logs at step 4485: {'loss': 2.1716, 'grad_norm': 1.3669166564941406, 'learning_rate': 2.772233311155467e-07, 'epoch': 2.9840319361277445}\n",
            "Captured loss: 2.1716 at step 4485\n",
            "Logs at step 4486: {'loss': 2.4754, 'grad_norm': 2.018857955932617, 'learning_rate': 2.661343978709248e-07, 'epoch': 2.984697272122422}\n",
            "Captured loss: 2.4754 at step 4486\n",
            "Logs at step 4487: {'loss': 1.9546, 'grad_norm': 1.807343602180481, 'learning_rate': 2.5504546462630294e-07, 'epoch': 2.985362608117099}\n",
            "Captured loss: 1.9546 at step 4487\n",
            "Logs at step 4488: {'loss': 2.0121, 'grad_norm': 2.2739055156707764, 'learning_rate': 2.4395653138168113e-07, 'epoch': 2.9860279441117763}\n",
            "Captured loss: 2.0121 at step 4488\n",
            "Logs at step 4489: {'loss': 2.3554, 'grad_norm': 1.2077243328094482, 'learning_rate': 2.328675981370592e-07, 'epoch': 2.986693280106454}\n",
            "Captured loss: 2.3554 at step 4489\n",
            "Logs at step 4490: {'loss': 2.3365, 'grad_norm': 1.4384310245513916, 'learning_rate': 2.2177866489243737e-07, 'epoch': 2.987358616101131}\n",
            "Captured loss: 2.3365 at step 4490\n",
            "Logs at step 4491: {'loss': 2.4028, 'grad_norm': 1.2873514890670776, 'learning_rate': 2.106897316478155e-07, 'epoch': 2.9880239520958085}\n",
            "Captured loss: 2.4028 at step 4491\n",
            "Logs at step 4492: {'loss': 2.0753, 'grad_norm': 1.6764317750930786, 'learning_rate': 1.996007984031936e-07, 'epoch': 2.9886892880904856}\n",
            "Captured loss: 2.0753 at step 4492\n",
            "Logs at step 4493: {'loss': 2.2989, 'grad_norm': 1.8048707246780396, 'learning_rate': 1.8851186515857177e-07, 'epoch': 2.989354624085163}\n",
            "Captured loss: 2.2989 at step 4493\n",
            "Logs at step 4494: {'loss': 1.9584, 'grad_norm': 1.743423581123352, 'learning_rate': 1.7742293191394987e-07, 'epoch': 2.9900199600798403}\n",
            "Captured loss: 1.9584 at step 4494\n",
            "Logs at step 4495: {'loss': 2.5637, 'grad_norm': 1.371991515159607, 'learning_rate': 1.66333998669328e-07, 'epoch': 2.990685296074518}\n",
            "Captured loss: 2.5637 at step 4495\n",
            "Logs at step 4496: {'loss': 2.4544, 'grad_norm': 1.4961575269699097, 'learning_rate': 1.5524506542470614e-07, 'epoch': 2.991350632069195}\n",
            "Captured loss: 2.4544 at step 4496\n",
            "Logs at step 4497: {'loss': 2.2568, 'grad_norm': 1.9576241970062256, 'learning_rate': 1.4415613218008427e-07, 'epoch': 2.992015968063872}\n",
            "Captured loss: 2.2568 at step 4497\n",
            "Logs at step 4498: {'loss': 2.2581, 'grad_norm': 1.4096765518188477, 'learning_rate': 1.330671989354624e-07, 'epoch': 2.9926813040585496}\n",
            "Captured loss: 2.2581 at step 4498\n",
            "Logs at step 4499: {'loss': 2.0396, 'grad_norm': 1.5449271202087402, 'learning_rate': 1.2197826569084057e-07, 'epoch': 2.993346640053227}\n",
            "Captured loss: 2.0396 at step 4499\n",
            "Logs at step 4500: {'loss': 2.151, 'grad_norm': 1.6243640184402466, 'learning_rate': 1.1088933244621868e-07, 'epoch': 2.9940119760479043}\n",
            "Captured loss: 2.151 at step 4500\n",
            "Logs at step 4501: {'loss': 2.3935, 'grad_norm': 1.556188941001892, 'learning_rate': 9.98003992015968e-08, 'epoch': 2.9946773120425814}\n",
            "Captured loss: 2.3935 at step 4501\n",
            "Logs at step 4502: {'loss': 2.0289, 'grad_norm': 2.1917779445648193, 'learning_rate': 8.871146595697494e-08, 'epoch': 2.995342648037259}\n",
            "Captured loss: 2.0289 at step 4502\n",
            "Logs at step 4503: {'loss': 2.3769, 'grad_norm': 1.3387709856033325, 'learning_rate': 7.762253271235307e-08, 'epoch': 2.996007984031936}\n",
            "Captured loss: 2.3769 at step 4503\n",
            "Logs at step 4504: {'loss': 2.2509, 'grad_norm': 1.7150194644927979, 'learning_rate': 6.65335994677312e-08, 'epoch': 2.9966733200266136}\n",
            "Captured loss: 2.2509 at step 4504\n",
            "Logs at step 4505: {'loss': 2.1125, 'grad_norm': 2.0989434719085693, 'learning_rate': 5.544466622310934e-08, 'epoch': 2.9973386560212907}\n",
            "Captured loss: 2.1125 at step 4505\n",
            "Logs at step 4506: {'loss': 2.3026, 'grad_norm': 1.5214570760726929, 'learning_rate': 4.435573297848747e-08, 'epoch': 2.998003992015968}\n",
            "Captured loss: 2.3026 at step 4506\n",
            "Logs at step 4507: {'loss': 2.2659, 'grad_norm': 1.6235506534576416, 'learning_rate': 3.32667997338656e-08, 'epoch': 2.9986693280106453}\n",
            "Captured loss: 2.2659 at step 4507\n",
            "Logs at step 4508: {'loss': 2.4824, 'grad_norm': 2.515723943710327, 'learning_rate': 2.2177866489243734e-08, 'epoch': 2.999334664005323}\n",
            "Captured loss: 2.4824 at step 4508\n",
            "Logs at step 4509: {'loss': 2.4973, 'grad_norm': 1.409873366355896, 'learning_rate': 1.1088933244621867e-08, 'epoch': 3.0}\n",
            "Captured loss: 2.4973 at step 4509\n",
            "Logs at step 4509: {'train_runtime': 4204.9901, 'train_samples_per_second': 2.145, 'train_steps_per_second': 1.072, 'total_flos': 9425241073778688.0, 'train_loss': 2.48870525134855, 'epoch': 3.0}\n",
            "✅ Training complete. Model and tokenizer saved.\n",
            "Steps: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363, 364, 365, 366, 367, 368, 369, 370, 371, 372, 373, 374, 375, 376, 377, 378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389, 390, 391, 392, 393, 394, 395, 396, 397, 398, 399, 400, 401, 402, 403, 404, 405, 406, 407, 408, 409, 410, 411, 412, 413, 414, 415, 416, 417, 418, 419, 420, 421, 422, 423, 424, 425, 426, 427, 428, 429, 430, 431, 432, 433, 434, 435, 436, 437, 438, 439, 440, 441, 442, 443, 444, 445, 446, 447, 448, 449, 450, 451, 452, 453, 454, 455, 456, 457, 458, 459, 460, 461, 462, 463, 464, 465, 466, 467, 468, 469, 470, 471, 472, 473, 474, 475, 476, 477, 478, 479, 480, 481, 482, 483, 484, 485, 486, 487, 488, 489, 490, 491, 492, 493, 494, 495, 496, 497, 498, 499, 500, 501, 502, 503, 504, 505, 506, 507, 508, 509, 510, 511, 512, 513, 514, 515, 516, 517, 518, 519, 520, 521, 522, 523, 524, 525, 526, 527, 528, 529, 530, 531, 532, 533, 534, 535, 536, 537, 538, 539, 540, 541, 542, 543, 544, 545, 546, 547, 548, 549, 550, 551, 552, 553, 554, 555, 556, 557, 558, 559, 560, 561, 562, 563, 564, 565, 566, 567, 568, 569, 570, 571, 572, 573, 574, 575, 576, 577, 578, 579, 580, 581, 582, 583, 584, 585, 586, 587, 588, 589, 590, 591, 592, 593, 594, 595, 596, 597, 598, 599, 600, 601, 602, 603, 604, 605, 606, 607, 608, 609, 610, 611, 612, 613, 614, 615, 616, 617, 618, 619, 620, 621, 622, 623, 624, 625, 626, 627, 628, 629, 630, 631, 632, 633, 634, 635, 636, 637, 638, 639, 640, 641, 642, 643, 644, 645, 646, 647, 648, 649, 650, 651, 652, 653, 654, 655, 656, 657, 658, 659, 660, 661, 662, 663, 664, 665, 666, 667, 668, 669, 670, 671, 672, 673, 674, 675, 676, 677, 678, 679, 680, 681, 682, 683, 684, 685, 686, 687, 688, 689, 690, 691, 692, 693, 694, 695, 696, 697, 698, 699, 700, 701, 702, 703, 704, 705, 706, 707, 708, 709, 710, 711, 712, 713, 714, 715, 716, 717, 718, 719, 720, 721, 722, 723, 724, 725, 726, 727, 728, 729, 730, 731, 732, 733, 734, 735, 736, 737, 738, 739, 740, 741, 742, 743, 744, 745, 746, 747, 748, 749, 750, 751, 752, 753, 754, 755, 756, 757, 758, 759, 760, 761, 762, 763, 764, 765, 766, 767, 768, 769, 770, 771, 772, 773, 774, 775, 776, 777, 778, 779, 780, 781, 782, 783, 784, 785, 786, 787, 788, 789, 790, 791, 792, 793, 794, 795, 796, 797, 798, 799, 800, 801, 802, 803, 804, 805, 806, 807, 808, 809, 810, 811, 812, 813, 814, 815, 816, 817, 818, 819, 820, 821, 822, 823, 824, 825, 826, 827, 828, 829, 830, 831, 832, 833, 834, 835, 836, 837, 838, 839, 840, 841, 842, 843, 844, 845, 846, 847, 848, 849, 850, 851, 852, 853, 854, 855, 856, 857, 858, 859, 860, 861, 862, 863, 864, 865, 866, 867, 868, 869, 870, 871, 872, 873, 874, 875, 876, 877, 878, 879, 880, 881, 882, 883, 884, 885, 886, 887, 888, 889, 890, 891, 892, 893, 894, 895, 896, 897, 898, 899, 900, 901, 902, 903, 904, 905, 906, 907, 908, 909, 910, 911, 912, 913, 914, 915, 916, 917, 918, 919, 920, 921, 922, 923, 924, 925, 926, 927, 928, 929, 930, 931, 932, 933, 934, 935, 936, 937, 938, 939, 940, 941, 942, 943, 944, 945, 946, 947, 948, 949, 950, 951, 952, 953, 954, 955, 956, 957, 958, 959, 960, 961, 962, 963, 964, 965, 966, 967, 968, 969, 970, 971, 972, 973, 974, 975, 976, 977, 978, 979, 980, 981, 982, 983, 984, 985, 986, 987, 988, 989, 990, 991, 992, 993, 994, 995, 996, 997, 998, 999, 1000, 1001, 1002, 1003, 1004, 1005, 1006, 1007, 1008, 1009, 1010, 1011, 1012, 1013, 1014, 1015, 1016, 1017, 1018, 1019, 1020, 1021, 1022, 1023, 1024, 1025, 1026, 1027, 1028, 1029, 1030, 1031, 1032, 1033, 1034, 1035, 1036, 1037, 1038, 1039, 1040, 1041, 1042, 1043, 1044, 1045, 1046, 1047, 1048, 1049, 1050, 1051, 1052, 1053, 1054, 1055, 1056, 1057, 1058, 1059, 1060, 1061, 1062, 1063, 1064, 1065, 1066, 1067, 1068, 1069, 1070, 1071, 1072, 1073, 1074, 1075, 1076, 1077, 1078, 1079, 1080, 1081, 1082, 1083, 1084, 1085, 1086, 1087, 1088, 1089, 1090, 1091, 1092, 1093, 1094, 1095, 1096, 1097, 1098, 1099, 1100, 1101, 1102, 1103, 1104, 1105, 1106, 1107, 1108, 1109, 1110, 1111, 1112, 1113, 1114, 1115, 1116, 1117, 1118, 1119, 1120, 1121, 1122, 1123, 1124, 1125, 1126, 1127, 1128, 1129, 1130, 1131, 1132, 1133, 1134, 1135, 1136, 1137, 1138, 1139, 1140, 1141, 1142, 1143, 1144, 1145, 1146, 1147, 1148, 1149, 1150, 1151, 1152, 1153, 1154, 1155, 1156, 1157, 1158, 1159, 1160, 1161, 1162, 1163, 1164, 1165, 1166, 1167, 1168, 1169, 1170, 1171, 1172, 1173, 1174, 1175, 1176, 1177, 1178, 1179, 1180, 1181, 1182, 1183, 1184, 1185, 1186, 1187, 1188, 1189, 1190, 1191, 1192, 1193, 1194, 1195, 1196, 1197, 1198, 1199, 1200, 1201, 1202, 1203, 1204, 1205, 1206, 1207, 1208, 1209, 1210, 1211, 1212, 1213, 1214, 1215, 1216, 1217, 1218, 1219, 1220, 1221, 1222, 1223, 1224, 1225, 1226, 1227, 1228, 1229, 1230, 1231, 1232, 1233, 1234, 1235, 1236, 1237, 1238, 1239, 1240, 1241, 1242, 1243, 1244, 1245, 1246, 1247, 1248, 1249, 1250, 1251, 1252, 1253, 1254, 1255, 1256, 1257, 1258, 1259, 1260, 1261, 1262, 1263, 1264, 1265, 1266, 1267, 1268, 1269, 1270, 1271, 1272, 1273, 1274, 1275, 1276, 1277, 1278, 1279, 1280, 1281, 1282, 1283, 1284, 1285, 1286, 1287, 1288, 1289, 1290, 1291, 1292, 1293, 1294, 1295, 1296, 1297, 1298, 1299, 1300, 1301, 1302, 1303, 1304, 1305, 1306, 1307, 1308, 1309, 1310, 1311, 1312, 1313, 1314, 1315, 1316, 1317, 1318, 1319, 1320, 1321, 1322, 1323, 1324, 1325, 1326, 1327, 1328, 1329, 1330, 1331, 1332, 1333, 1334, 1335, 1336, 1337, 1338, 1339, 1340, 1341, 1342, 1343, 1344, 1345, 1346, 1347, 1348, 1349, 1350, 1351, 1352, 1353, 1354, 1355, 1356, 1357, 1358, 1359, 1360, 1361, 1362, 1363, 1364, 1365, 1366, 1367, 1368, 1369, 1370, 1371, 1372, 1373, 1374, 1375, 1376, 1377, 1378, 1379, 1380, 1381, 1382, 1383, 1384, 1385, 1386, 1387, 1388, 1389, 1390, 1391, 1392, 1393, 1394, 1395, 1396, 1397, 1398, 1399, 1400, 1401, 1402, 1403, 1404, 1405, 1406, 1407, 1408, 1409, 1410, 1411, 1412, 1413, 1414, 1415, 1416, 1417, 1418, 1419, 1420, 1421, 1422, 1423, 1424, 1425, 1426, 1427, 1428, 1429, 1430, 1431, 1432, 1433, 1434, 1435, 1436, 1437, 1438, 1439, 1440, 1441, 1442, 1443, 1444, 1445, 1446, 1447, 1448, 1449, 1450, 1451, 1452, 1453, 1454, 1455, 1456, 1457, 1458, 1459, 1460, 1461, 1462, 1463, 1464, 1465, 1466, 1467, 1468, 1469, 1470, 1471, 1472, 1473, 1474, 1475, 1476, 1477, 1478, 1479, 1480, 1481, 1482, 1483, 1484, 1485, 1486, 1487, 1488, 1489, 1490, 1491, 1492, 1493, 1494, 1495, 1496, 1497, 1498, 1499, 1500, 1501, 1502, 1503, 1504, 1505, 1506, 1507, 1508, 1509, 1510, 1511, 1512, 1513, 1514, 1515, 1516, 1517, 1518, 1519, 1520, 1521, 1522, 1523, 1524, 1525, 1526, 1527, 1528, 1529, 1530, 1531, 1532, 1533, 1534, 1535, 1536, 1537, 1538, 1539, 1540, 1541, 1542, 1543, 1544, 1545, 1546, 1547, 1548, 1549, 1550, 1551, 1552, 1553, 1554, 1555, 1556, 1557, 1558, 1559, 1560, 1561, 1562, 1563, 1564, 1565, 1566, 1567, 1568, 1569, 1570, 1571, 1572, 1573, 1574, 1575, 1576, 1577, 1578, 1579, 1580, 1581, 1582, 1583, 1584, 1585, 1586, 1587, 1588, 1589, 1590, 1591, 1592, 1593, 1594, 1595, 1596, 1597, 1598, 1599, 1600, 1601, 1602, 1603, 1604, 1605, 1606, 1607, 1608, 1609, 1610, 1611, 1612, 1613, 1614, 1615, 1616, 1617, 1618, 1619, 1620, 1621, 1622, 1623, 1624, 1625, 1626, 1627, 1628, 1629, 1630, 1631, 1632, 1633, 1634, 1635, 1636, 1637, 1638, 1639, 1640, 1641, 1642, 1643, 1644, 1645, 1646, 1647, 1648, 1649, 1650, 1651, 1652, 1653, 1654, 1655, 1656, 1657, 1658, 1659, 1660, 1661, 1662, 1663, 1664, 1665, 1666, 1667, 1668, 1669, 1670, 1671, 1672, 1673, 1674, 1675, 1676, 1677, 1678, 1679, 1680, 1681, 1682, 1683, 1684, 1685, 1686, 1687, 1688, 1689, 1690, 1691, 1692, 1693, 1694, 1695, 1696, 1697, 1698, 1699, 1700, 1701, 1702, 1703, 1704, 1705, 1706, 1707, 1708, 1709, 1710, 1711, 1712, 1713, 1714, 1715, 1716, 1717, 1718, 1719, 1720, 1721, 1722, 1723, 1724, 1725, 1726, 1727, 1728, 1729, 1730, 1731, 1732, 1733, 1734, 1735, 1736, 1737, 1738, 1739, 1740, 1741, 1742, 1743, 1744, 1745, 1746, 1747, 1748, 1749, 1750, 1751, 1752, 1753, 1754, 1755, 1756, 1757, 1758, 1759, 1760, 1761, 1762, 1763, 1764, 1765, 1766, 1767, 1768, 1769, 1770, 1771, 1772, 1773, 1774, 1775, 1776, 1777, 1778, 1779, 1780, 1781, 1782, 1783, 1784, 1785, 1786, 1787, 1788, 1789, 1790, 1791, 1792, 1793, 1794, 1795, 1796, 1797, 1798, 1799, 1800, 1801, 1802, 1803, 1804, 1805, 1806, 1807, 1808, 1809, 1810, 1811, 1812, 1813, 1814, 1815, 1816, 1817, 1818, 1819, 1820, 1821, 1822, 1823, 1824, 1825, 1826, 1827, 1828, 1829, 1830, 1831, 1832, 1833, 1834, 1835, 1836, 1837, 1838, 1839, 1840, 1841, 1842, 1843, 1844, 1845, 1846, 1847, 1848, 1849, 1850, 1851, 1852, 1853, 1854, 1855, 1856, 1857, 1858, 1859, 1860, 1861, 1862, 1863, 1864, 1865, 1866, 1867, 1868, 1869, 1870, 1871, 1872, 1873, 1874, 1875, 1876, 1877, 1878, 1879, 1880, 1881, 1882, 1883, 1884, 1885, 1886, 1887, 1888, 1889, 1890, 1891, 1892, 1893, 1894, 1895, 1896, 1897, 1898, 1899, 1900, 1901, 1902, 1903, 1904, 1905, 1906, 1907, 1908, 1909, 1910, 1911, 1912, 1913, 1914, 1915, 1916, 1917, 1918, 1919, 1920, 1921, 1922, 1923, 1924, 1925, 1926, 1927, 1928, 1929, 1930, 1931, 1932, 1933, 1934, 1935, 1936, 1937, 1938, 1939, 1940, 1941, 1942, 1943, 1944, 1945, 1946, 1947, 1948, 1949, 1950, 1951, 1952, 1953, 1954, 1955, 1956, 1957, 1958, 1959, 1960, 1961, 1962, 1963, 1964, 1965, 1966, 1967, 1968, 1969, 1970, 1971, 1972, 1973, 1974, 1975, 1976, 1977, 1978, 1979, 1980, 1981, 1982, 1983, 1984, 1985, 1986, 1987, 1988, 1989, 1990, 1991, 1992, 1993, 1994, 1995, 1996, 1997, 1998, 1999, 2000, 2001, 2002, 2003, 2004, 2005, 2006, 2007, 2008, 2009, 2010, 2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020, 2021, 2022, 2023, 2024, 2025, 2026, 2027, 2028, 2029, 2030, 2031, 2032, 2033, 2034, 2035, 2036, 2037, 2038, 2039, 2040, 2041, 2042, 2043, 2044, 2045, 2046, 2047, 2048, 2049, 2050, 2051, 2052, 2053, 2054, 2055, 2056, 2057, 2058, 2059, 2060, 2061, 2062, 2063, 2064, 2065, 2066, 2067, 2068, 2069, 2070, 2071, 2072, 2073, 2074, 2075, 2076, 2077, 2078, 2079, 2080, 2081, 2082, 2083, 2084, 2085, 2086, 2087, 2088, 2089, 2090, 2091, 2092, 2093, 2094, 2095, 2096, 2097, 2098, 2099, 2100, 2101, 2102, 2103, 2104, 2105, 2106, 2107, 2108, 2109, 2110, 2111, 2112, 2113, 2114, 2115, 2116, 2117, 2118, 2119, 2120, 2121, 2122, 2123, 2124, 2125, 2126, 2127, 2128, 2129, 2130, 2131, 2132, 2133, 2134, 2135, 2136, 2137, 2138, 2139, 2140, 2141, 2142, 2143, 2144, 2145, 2146, 2147, 2148, 2149, 2150, 2151, 2152, 2153, 2154, 2155, 2156, 2157, 2158, 2159, 2160, 2161, 2162, 2163, 2164, 2165, 2166, 2167, 2168, 2169, 2170, 2171, 2172, 2173, 2174, 2175, 2176, 2177, 2178, 2179, 2180, 2181, 2182, 2183, 2184, 2185, 2186, 2187, 2188, 2189, 2190, 2191, 2192, 2193, 2194, 2195, 2196, 2197, 2198, 2199, 2200, 2201, 2202, 2203, 2204, 2205, 2206, 2207, 2208, 2209, 2210, 2211, 2212, 2213, 2214, 2215, 2216, 2217, 2218, 2219, 2220, 2221, 2222, 2223, 2224, 2225, 2226, 2227, 2228, 2229, 2230, 2231, 2232, 2233, 2234, 2235, 2236, 2237, 2238, 2239, 2240, 2241, 2242, 2243, 2244, 2245, 2246, 2247, 2248, 2249, 2250, 2251, 2252, 2253, 2254, 2255, 2256, 2257, 2258, 2259, 2260, 2261, 2262, 2263, 2264, 2265, 2266, 2267, 2268, 2269, 2270, 2271, 2272, 2273, 2274, 2275, 2276, 2277, 2278, 2279, 2280, 2281, 2282, 2283, 2284, 2285, 2286, 2287, 2288, 2289, 2290, 2291, 2292, 2293, 2294, 2295, 2296, 2297, 2298, 2299, 2300, 2301, 2302, 2303, 2304, 2305, 2306, 2307, 2308, 2309, 2310, 2311, 2312, 2313, 2314, 2315, 2316, 2317, 2318, 2319, 2320, 2321, 2322, 2323, 2324, 2325, 2326, 2327, 2328, 2329, 2330, 2331, 2332, 2333, 2334, 2335, 2336, 2337, 2338, 2339, 2340, 2341, 2342, 2343, 2344, 2345, 2346, 2347, 2348, 2349, 2350, 2351, 2352, 2353, 2354, 2355, 2356, 2357, 2358, 2359, 2360, 2361, 2362, 2363, 2364, 2365, 2366, 2367, 2368, 2369, 2370, 2371, 2372, 2373, 2374, 2375, 2376, 2377, 2378, 2379, 2380, 2381, 2382, 2383, 2384, 2385, 2386, 2387, 2388, 2389, 2390, 2391, 2392, 2393, 2394, 2395, 2396, 2397, 2398, 2399, 2400, 2401, 2402, 2403, 2404, 2405, 2406, 2407, 2408, 2409, 2410, 2411, 2412, 2413, 2414, 2415, 2416, 2417, 2418, 2419, 2420, 2421, 2422, 2423, 2424, 2425, 2426, 2427, 2428, 2429, 2430, 2431, 2432, 2433, 2434, 2435, 2436, 2437, 2438, 2439, 2440, 2441, 2442, 2443, 2444, 2445, 2446, 2447, 2448, 2449, 2450, 2451, 2452, 2453, 2454, 2455, 2456, 2457, 2458, 2459, 2460, 2461, 2462, 2463, 2464, 2465, 2466, 2467, 2468, 2469, 2470, 2471, 2472, 2473, 2474, 2475, 2476, 2477, 2478, 2479, 2480, 2481, 2482, 2483, 2484, 2485, 2486, 2487, 2488, 2489, 2490, 2491, 2492, 2493, 2494, 2495, 2496, 2497, 2498, 2499, 2500, 2501, 2502, 2503, 2504, 2505, 2506, 2507, 2508, 2509, 2510, 2511, 2512, 2513, 2514, 2515, 2516, 2517, 2518, 2519, 2520, 2521, 2522, 2523, 2524, 2525, 2526, 2527, 2528, 2529, 2530, 2531, 2532, 2533, 2534, 2535, 2536, 2537, 2538, 2539, 2540, 2541, 2542, 2543, 2544, 2545, 2546, 2547, 2548, 2549, 2550, 2551, 2552, 2553, 2554, 2555, 2556, 2557, 2558, 2559, 2560, 2561, 2562, 2563, 2564, 2565, 2566, 2567, 2568, 2569, 2570, 2571, 2572, 2573, 2574, 2575, 2576, 2577, 2578, 2579, 2580, 2581, 2582, 2583, 2584, 2585, 2586, 2587, 2588, 2589, 2590, 2591, 2592, 2593, 2594, 2595, 2596, 2597, 2598, 2599, 2600, 2601, 2602, 2603, 2604, 2605, 2606, 2607, 2608, 2609, 2610, 2611, 2612, 2613, 2614, 2615, 2616, 2617, 2618, 2619, 2620, 2621, 2622, 2623, 2624, 2625, 2626, 2627, 2628, 2629, 2630, 2631, 2632, 2633, 2634, 2635, 2636, 2637, 2638, 2639, 2640, 2641, 2642, 2643, 2644, 2645, 2646, 2647, 2648, 2649, 2650, 2651, 2652, 2653, 2654, 2655, 2656, 2657, 2658, 2659, 2660, 2661, 2662, 2663, 2664, 2665, 2666, 2667, 2668, 2669, 2670, 2671, 2672, 2673, 2674, 2675, 2676, 2677, 2678, 2679, 2680, 2681, 2682, 2683, 2684, 2685, 2686, 2687, 2688, 2689, 2690, 2691, 2692, 2693, 2694, 2695, 2696, 2697, 2698, 2699, 2700, 2701, 2702, 2703, 2704, 2705, 2706, 2707, 2708, 2709, 2710, 2711, 2712, 2713, 2714, 2715, 2716, 2717, 2718, 2719, 2720, 2721, 2722, 2723, 2724, 2725, 2726, 2727, 2728, 2729, 2730, 2731, 2732, 2733, 2734, 2735, 2736, 2737, 2738, 2739, 2740, 2741, 2742, 2743, 2744, 2745, 2746, 2747, 2748, 2749, 2750, 2751, 2752, 2753, 2754, 2755, 2756, 2757, 2758, 2759, 2760, 2761, 2762, 2763, 2764, 2765, 2766, 2767, 2768, 2769, 2770, 2771, 2772, 2773, 2774, 2775, 2776, 2777, 2778, 2779, 2780, 2781, 2782, 2783, 2784, 2785, 2786, 2787, 2788, 2789, 2790, 2791, 2792, 2793, 2794, 2795, 2796, 2797, 2798, 2799, 2800, 2801, 2802, 2803, 2804, 2805, 2806, 2807, 2808, 2809, 2810, 2811, 2812, 2813, 2814, 2815, 2816, 2817, 2818, 2819, 2820, 2821, 2822, 2823, 2824, 2825, 2826, 2827, 2828, 2829, 2830, 2831, 2832, 2833, 2834, 2835, 2836, 2837, 2838, 2839, 2840, 2841, 2842, 2843, 2844, 2845, 2846, 2847, 2848, 2849, 2850, 2851, 2852, 2853, 2854, 2855, 2856, 2857, 2858, 2859, 2860, 2861, 2862, 2863, 2864, 2865, 2866, 2867, 2868, 2869, 2870, 2871, 2872, 2873, 2874, 2875, 2876, 2877, 2878, 2879, 2880, 2881, 2882, 2883, 2884, 2885, 2886, 2887, 2888, 2889, 2890, 2891, 2892, 2893, 2894, 2895, 2896, 2897, 2898, 2899, 2900, 2901, 2902, 2903, 2904, 2905, 2906, 2907, 2908, 2909, 2910, 2911, 2912, 2913, 2914, 2915, 2916, 2917, 2918, 2919, 2920, 2921, 2922, 2923, 2924, 2925, 2926, 2927, 2928, 2929, 2930, 2931, 2932, 2933, 2934, 2935, 2936, 2937, 2938, 2939, 2940, 2941, 2942, 2943, 2944, 2945, 2946, 2947, 2948, 2949, 2950, 2951, 2952, 2953, 2954, 2955, 2956, 2957, 2958, 2959, 2960, 2961, 2962, 2963, 2964, 2965, 2966, 2967, 2968, 2969, 2970, 2971, 2972, 2973, 2974, 2975, 2976, 2977, 2978, 2979, 2980, 2981, 2982, 2983, 2984, 2985, 2986, 2987, 2988, 2989, 2990, 2991, 2992, 2993, 2994, 2995, 2996, 2997, 2998, 2999, 3000, 3001, 3002, 3003, 3004, 3005, 3006, 3007, 3008, 3009, 3010, 3011, 3012, 3013, 3014, 3015, 3016, 3017, 3018, 3019, 3020, 3021, 3022, 3023, 3024, 3025, 3026, 3027, 3028, 3029, 3030, 3031, 3032, 3033, 3034, 3035, 3036, 3037, 3038, 3039, 3040, 3041, 3042, 3043, 3044, 3045, 3046, 3047, 3048, 3049, 3050, 3051, 3052, 3053, 3054, 3055, 3056, 3057, 3058, 3059, 3060, 3061, 3062, 3063, 3064, 3065, 3066, 3067, 3068, 3069, 3070, 3071, 3072, 3073, 3074, 3075, 3076, 3077, 3078, 3079, 3080, 3081, 3082, 3083, 3084, 3085, 3086, 3087, 3088, 3089, 3090, 3091, 3092, 3093, 3094, 3095, 3096, 3097, 3098, 3099, 3100, 3101, 3102, 3103, 3104, 3105, 3106, 3107, 3108, 3109, 3110, 3111, 3112, 3113, 3114, 3115, 3116, 3117, 3118, 3119, 3120, 3121, 3122, 3123, 3124, 3125, 3126, 3127, 3128, 3129, 3130, 3131, 3132, 3133, 3134, 3135, 3136, 3137, 3138, 3139, 3140, 3141, 3142, 3143, 3144, 3145, 3146, 3147, 3148, 3149, 3150, 3151, 3152, 3153, 3154, 3155, 3156, 3157, 3158, 3159, 3160, 3161, 3162, 3163, 3164, 3165, 3166, 3167, 3168, 3169, 3170, 3171, 3172, 3173, 3174, 3175, 3176, 3177, 3178, 3179, 3180, 3181, 3182, 3183, 3184, 3185, 3186, 3187, 3188, 3189, 3190, 3191, 3192, 3193, 3194, 3195, 3196, 3197, 3198, 3199, 3200, 3201, 3202, 3203, 3204, 3205, 3206, 3207, 3208, 3209, 3210, 3211, 3212, 3213, 3214, 3215, 3216, 3217, 3218, 3219, 3220, 3221, 3222, 3223, 3224, 3225, 3226, 3227, 3228, 3229, 3230, 3231, 3232, 3233, 3234, 3235, 3236, 3237, 3238, 3239, 3240, 3241, 3242, 3243, 3244, 3245, 3246, 3247, 3248, 3249, 3250, 3251, 3252, 3253, 3254, 3255, 3256, 3257, 3258, 3259, 3260, 3261, 3262, 3263, 3264, 3265, 3266, 3267, 3268, 3269, 3270, 3271, 3272, 3273, 3274, 3275, 3276, 3277, 3278, 3279, 3280, 3281, 3282, 3283, 3284, 3285, 3286, 3287, 3288, 3289, 3290, 3291, 3292, 3293, 3294, 3295, 3296, 3297, 3298, 3299, 3300, 3301, 3302, 3303, 3304, 3305, 3306, 3307, 3308, 3309, 3310, 3311, 3312, 3313, 3314, 3315, 3316, 3317, 3318, 3319, 3320, 3321, 3322, 3323, 3324, 3325, 3326, 3327, 3328, 3329, 3330, 3331, 3332, 3333, 3334, 3335, 3336, 3337, 3338, 3339, 3340, 3341, 3342, 3343, 3344, 3345, 3346, 3347, 3348, 3349, 3350, 3351, 3352, 3353, 3354, 3355, 3356, 3357, 3358, 3359, 3360, 3361, 3362, 3363, 3364, 3365, 3366, 3367, 3368, 3369, 3370, 3371, 3372, 3373, 3374, 3375, 3376, 3377, 3378, 3379, 3380, 3381, 3382, 3383, 3384, 3385, 3386, 3387, 3388, 3389, 3390, 3391, 3392, 3393, 3394, 3395, 3396, 3397, 3398, 3399, 3400, 3401, 3402, 3403, 3404, 3405, 3406, 3407, 3408, 3409, 3410, 3411, 3412, 3413, 3414, 3415, 3416, 3417, 3418, 3419, 3420, 3421, 3422, 3423, 3424, 3425, 3426, 3427, 3428, 3429, 3430, 3431, 3432, 3433, 3434, 3435, 3436, 3437, 3438, 3439, 3440, 3441, 3442, 3443, 3444, 3445, 3446, 3447, 3448, 3449, 3450, 3451, 3452, 3453, 3454, 3455, 3456, 3457, 3458, 3459, 3460, 3461, 3462, 3463, 3464, 3465, 3466, 3467, 3468, 3469, 3470, 3471, 3472, 3473, 3474, 3475, 3476, 3477, 3478, 3479, 3480, 3481, 3482, 3483, 3484, 3485, 3486, 3487, 3488, 3489, 3490, 3491, 3492, 3493, 3494, 3495, 3496, 3497, 3498, 3499, 3500, 3501, 3502, 3503, 3504, 3505, 3506, 3507, 3508, 3509, 3510, 3511, 3512, 3513, 3514, 3515, 3516, 3517, 3518, 3519, 3520, 3521, 3522, 3523, 3524, 3525, 3526, 3527, 3528, 3529, 3530, 3531, 3532, 3533, 3534, 3535, 3536, 3537, 3538, 3539, 3540, 3541, 3542, 3543, 3544, 3545, 3546, 3547, 3548, 3549, 3550, 3551, 3552, 3553, 3554, 3555, 3556, 3557, 3558, 3559, 3560, 3561, 3562, 3563, 3564, 3565, 3566, 3567, 3568, 3569, 3570, 3571, 3572, 3573, 3574, 3575, 3576, 3577, 3578, 3579, 3580, 3581, 3582, 3583, 3584, 3585, 3586, 3587, 3588, 3589, 3590, 3591, 3592, 3593, 3594, 3595, 3596, 3597, 3598, 3599, 3600, 3601, 3602, 3603, 3604, 3605, 3606, 3607, 3608, 3609, 3610, 3611, 3612, 3613, 3614, 3615, 3616, 3617, 3618, 3619, 3620, 3621, 3622, 3623, 3624, 3625, 3626, 3627, 3628, 3629, 3630, 3631, 3632, 3633, 3634, 3635, 3636, 3637, 3638, 3639, 3640, 3641, 3642, 3643, 3644, 3645, 3646, 3647, 3648, 3649, 3650, 3651, 3652, 3653, 3654, 3655, 3656, 3657, 3658, 3659, 3660, 3661, 3662, 3663, 3664, 3665, 3666, 3667, 3668, 3669, 3670, 3671, 3672, 3673, 3674, 3675, 3676, 3677, 3678, 3679, 3680, 3681, 3682, 3683, 3684, 3685, 3686, 3687, 3688, 3689, 3690, 3691, 3692, 3693, 3694, 3695, 3696, 3697, 3698, 3699, 3700, 3701, 3702, 3703, 3704, 3705, 3706, 3707, 3708, 3709, 3710, 3711, 3712, 3713, 3714, 3715, 3716, 3717, 3718, 3719, 3720, 3721, 3722, 3723, 3724, 3725, 3726, 3727, 3728, 3729, 3730, 3731, 3732, 3733, 3734, 3735, 3736, 3737, 3738, 3739, 3740, 3741, 3742, 3743, 3744, 3745, 3746, 3747, 3748, 3749, 3750, 3751, 3752, 3753, 3754, 3755, 3756, 3757, 3758, 3759, 3760, 3761, 3762, 3763, 3764, 3765, 3766, 3767, 3768, 3769, 3770, 3771, 3772, 3773, 3774, 3775, 3776, 3777, 3778, 3779, 3780, 3781, 3782, 3783, 3784, 3785, 3786, 3787, 3788, 3789, 3790, 3791, 3792, 3793, 3794, 3795, 3796, 3797, 3798, 3799, 3800, 3801, 3802, 3803, 3804, 3805, 3806, 3807, 3808, 3809, 3810, 3811, 3812, 3813, 3814, 3815, 3816, 3817, 3818, 3819, 3820, 3821, 3822, 3823, 3824, 3825, 3826, 3827, 3828, 3829, 3830, 3831, 3832, 3833, 3834, 3835, 3836, 3837, 3838, 3839, 3840, 3841, 3842, 3843, 3844, 3845, 3846, 3847, 3848, 3849, 3850, 3851, 3852, 3853, 3854, 3855, 3856, 3857, 3858, 3859, 3860, 3861, 3862, 3863, 3864, 3865, 3866, 3867, 3868, 3869, 3870, 3871, 3872, 3873, 3874, 3875, 3876, 3877, 3878, 3879, 3880, 3881, 3882, 3883, 3884, 3885, 3886, 3887, 3888, 3889, 3890, 3891, 3892, 3893, 3894, 3895, 3896, 3897, 3898, 3899, 3900, 3901, 3902, 3903, 3904, 3905, 3906, 3907, 3908, 3909, 3910, 3911, 3912, 3913, 3914, 3915, 3916, 3917, 3918, 3919, 3920, 3921, 3922, 3923, 3924, 3925, 3926, 3927, 3928, 3929, 3930, 3931, 3932, 3933, 3934, 3935, 3936, 3937, 3938, 3939, 3940, 3941, 3942, 3943, 3944, 3945, 3946, 3947, 3948, 3949, 3950, 3951, 3952, 3953, 3954, 3955, 3956, 3957, 3958, 3959, 3960, 3961, 3962, 3963, 3964, 3965, 3966, 3967, 3968, 3969, 3970, 3971, 3972, 3973, 3974, 3975, 3976, 3977, 3978, 3979, 3980, 3981, 3982, 3983, 3984, 3985, 3986, 3987, 3988, 3989, 3990, 3991, 3992, 3993, 3994, 3995, 3996, 3997, 3998, 3999, 4000, 4001, 4002, 4003, 4004, 4005, 4006, 4007, 4008, 4009, 4010, 4011, 4012, 4013, 4014, 4015, 4016, 4017, 4018, 4019, 4020, 4021, 4022, 4023, 4024, 4025, 4026, 4027, 4028, 4029, 4030, 4031, 4032, 4033, 4034, 4035, 4036, 4037, 4038, 4039, 4040, 4041, 4042, 4043, 4044, 4045, 4046, 4047, 4048, 4049, 4050, 4051, 4052, 4053, 4054, 4055, 4056, 4057, 4058, 4059, 4060, 4061, 4062, 4063, 4064, 4065, 4066, 4067, 4068, 4069, 4070, 4071, 4072, 4073, 4074, 4075, 4076, 4077, 4078, 4079, 4080, 4081, 4082, 4083, 4084, 4085, 4086, 4087, 4088, 4089, 4090, 4091, 4092, 4093, 4094, 4095, 4096, 4097, 4098, 4099, 4100, 4101, 4102, 4103, 4104, 4105, 4106, 4107, 4108, 4109, 4110, 4111, 4112, 4113, 4114, 4115, 4116, 4117, 4118, 4119, 4120, 4121, 4122, 4123, 4124, 4125, 4126, 4127, 4128, 4129, 4130, 4131, 4132, 4133, 4134, 4135, 4136, 4137, 4138, 4139, 4140, 4141, 4142, 4143, 4144, 4145, 4146, 4147, 4148, 4149, 4150, 4151, 4152, 4153, 4154, 4155, 4156, 4157, 4158, 4159, 4160, 4161, 4162, 4163, 4164, 4165, 4166, 4167, 4168, 4169, 4170, 4171, 4172, 4173, 4174, 4175, 4176, 4177, 4178, 4179, 4180, 4181, 4182, 4183, 4184, 4185, 4186, 4187, 4188, 4189, 4190, 4191, 4192, 4193, 4194, 4195, 4196, 4197, 4198, 4199, 4200, 4201, 4202, 4203, 4204, 4205, 4206, 4207, 4208, 4209, 4210, 4211, 4212, 4213, 4214, 4215, 4216, 4217, 4218, 4219, 4220, 4221, 4222, 4223, 4224, 4225, 4226, 4227, 4228, 4229, 4230, 4231, 4232, 4233, 4234, 4235, 4236, 4237, 4238, 4239, 4240, 4241, 4242, 4243, 4244, 4245, 4246, 4247, 4248, 4249, 4250, 4251, 4252, 4253, 4254, 4255, 4256, 4257, 4258, 4259, 4260, 4261, 4262, 4263, 4264, 4265, 4266, 4267, 4268, 4269, 4270, 4271, 4272, 4273, 4274, 4275, 4276, 4277, 4278, 4279, 4280, 4281, 4282, 4283, 4284, 4285, 4286, 4287, 4288, 4289, 4290, 4291, 4292, 4293, 4294, 4295, 4296, 4297, 4298, 4299, 4300, 4301, 4302, 4303, 4304, 4305, 4306, 4307, 4308, 4309, 4310, 4311, 4312, 4313, 4314, 4315, 4316, 4317, 4318, 4319, 4320, 4321, 4322, 4323, 4324, 4325, 4326, 4327, 4328, 4329, 4330, 4331, 4332, 4333, 4334, 4335, 4336, 4337, 4338, 4339, 4340, 4341, 4342, 4343, 4344, 4345, 4346, 4347, 4348, 4349, 4350, 4351, 4352, 4353, 4354, 4355, 4356, 4357, 4358, 4359, 4360, 4361, 4362, 4363, 4364, 4365, 4366, 4367, 4368, 4369, 4370, 4371, 4372, 4373, 4374, 4375, 4376, 4377, 4378, 4379, 4380, 4381, 4382, 4383, 4384, 4385, 4386, 4387, 4388, 4389, 4390, 4391, 4392, 4393, 4394, 4395, 4396, 4397, 4398, 4399, 4400, 4401, 4402, 4403, 4404, 4405, 4406, 4407, 4408, 4409, 4410, 4411, 4412, 4413, 4414, 4415, 4416, 4417, 4418, 4419, 4420, 4421, 4422, 4423, 4424, 4425, 4426, 4427, 4428, 4429, 4430, 4431, 4432, 4433, 4434, 4435, 4436, 4437, 4438, 4439, 4440, 4441, 4442, 4443, 4444, 4445, 4446, 4447, 4448, 4449, 4450, 4451, 4452, 4453, 4454, 4455, 4456, 4457, 4458, 4459, 4460, 4461, 4462, 4463, 4464, 4465, 4466, 4467, 4468, 4469, 4470, 4471, 4472, 4473, 4474, 4475, 4476, 4477, 4478, 4479, 4480, 4481, 4482, 4483, 4484, 4485, 4486, 4487, 4488, 4489, 4490, 4491, 4492, 4493, 4494, 4495, 4496, 4497, 4498, 4499, 4500, 4501, 4502, 4503, 4504, 4505, 4506, 4507, 4508, 4509]\n",
            "Losses: [5.7114, 4.4205, 5.998, 4.5756, 4.8569, 3.9238, 4.0531, 3.8825, 3.7747, 3.8667, 3.7186, 3.7584, 3.7769, 3.58, 3.6376, 3.6371, 3.9276, 3.7947, 3.7348, 3.724, 3.5192, 3.4075, 5.8955, 3.2817, 3.4686, 3.441, 3.3315, 3.3714, 3.5322, 3.2496, 3.2495, 3.2446, 3.2337, 3.1344, 3.337, 3.7952, 3.3579, 3.2696, 3.2172, 3.3148, 3.0655, 3.1184, 3.1121, 3.0306, 2.8962, 3.2017, 3.1155, 3.1631, 3.1313, 2.9122, 3.1385, 2.8797, 2.8893, 2.8638, 3.0867, 2.8049, 3.2271, 2.9713, 3.0496, 2.9811, 2.9218, 2.8457, 2.8417, 2.9391, 2.6908, 2.9754, 2.9979, 2.9977, 3.1589, 3.0877, 2.926, 2.8981, 2.9375, 2.9977, 3.0238, 3.0264, 2.8813, 2.9716, 2.8134, 3.2448, 3.0497, 2.9197, 2.9388, 2.8778, 2.7613, 2.8887, 2.8196, 2.8259, 2.7202, 2.8963, 2.9441, 2.9079, 2.9831, 2.985, 2.8559, 2.9203, 2.8475, 2.7068, 2.909, 2.8958, 2.6472, 3.0904, 2.7974, 2.718, 2.8818, 2.9393, 2.9345, 2.8616, 3.0794, 2.6454, 2.9499, 2.7707, 2.7843, 2.8987, 2.7338, 2.6782, 2.9128, 2.8491, 2.8535, 2.6334, 2.9649, 2.718, 2.7457, 2.6532, 2.8196, 2.9547, 2.5921, 2.9138, 2.857, 2.8683, 2.8392, 2.827, 2.894, 2.7125, 2.8318, 2.9211, 2.7962, 2.8075, 2.6447, 2.8052, 2.8155, 2.732, 2.6654, 2.5834, 2.8785, 2.8699, 2.8079, 2.8568, 2.8857, 2.8589, 2.8333, 2.7305, 2.7935, 2.7567, 2.7758, 2.8177, 2.5593, 2.8133, 2.7416, 3.021, 2.7686, 2.8704, 2.7197, 2.829, 2.8867, 2.7154, 2.9771, 2.7758, 2.6607, 2.614, 2.588, 2.8461, 2.7475, 2.9863, 2.7417, 2.8691, 3.0659, 2.6845, 2.6002, 2.7498, 2.8932, 2.7674, 2.826, 2.7285, 2.7438, 2.841, 2.8553, 2.6625, 2.8232, 2.7645, 2.7354, 2.5613, 2.8382, 2.6739, 2.9942, 2.6705, 2.7913, 2.6465, 2.6973, 2.7, 2.7822, 2.6907, 2.6603, 2.6049, 2.768, 2.7201, 2.6162, 2.7738, 2.6597, 2.7084, 2.7163, 2.8068, 2.4803, 2.6739, 2.8791, 2.8678, 3.0093, 2.5936, 2.4227, 2.6086, 2.7228, 2.5982, 2.8782, 2.7103, 2.7747, 2.6919, 2.5277, 2.5575, 2.7404, 2.7827, 2.767, 2.7304, 2.7752, 2.6851, 2.6729, 2.6868, 2.7441, 2.7556, 3.0609, 2.6078, 2.8585, 2.7431, 2.6571, 2.7429, 2.8618, 2.8313, 2.5724, 2.5971, 2.5982, 2.5762, 2.8289, 2.8564, 2.7191, 2.8796, 2.5985, 2.8105, 2.6771, 2.4909, 2.7237, 2.7852, 2.5785, 2.9759, 2.6448, 2.6127, 2.7394, 2.6541, 2.9701, 2.748, 2.7002, 2.7377, 2.8142, 2.8148, 2.706, 2.61, 2.7312, 2.7862, 2.679, 2.5851, 2.5729, 2.7536, 2.7717, 2.9395, 2.7219, 2.715, 2.6895, 2.6493, 2.6442, 2.6609, 2.5108, 2.6701, 2.4579, 2.703, 2.7399, 2.512, 2.6397, 2.9102, 2.7391, 2.7779, 2.6852, 2.7433, 2.8042, 2.6586, 2.6756, 2.7015, 2.7903, 2.753, 2.8475, 2.3957, 2.7576, 2.7043, 2.52, 2.7445, 2.7352, 2.746, 2.6479, 2.7788, 2.8218, 2.792, 2.7649, 2.7667, 2.7772, 2.5139, 2.7665, 2.6855, 2.6923, 2.8083, 2.7066, 2.6428, 2.8488, 2.8406, 2.6938, 2.7196, 2.4774, 2.7876, 2.4875, 2.7006, 2.7417, 2.5334, 2.7798, 2.6682, 2.759, 2.9004, 2.7628, 2.6167, 2.6579, 2.5435, 2.5706, 2.65, 2.8034, 2.4862, 2.812, 2.6361, 2.7391, 2.8076, 2.761, 2.5154, 2.4387, 2.4328, 2.4009, 2.6461, 2.7837, 2.7151, 2.7359, 2.191, 2.4802, 2.6829, 2.8196, 2.7027, 2.7687, 2.7699, 2.7374, 2.6347, 2.7248, 2.9269, 2.711, 2.6602, 2.6858, 2.6344, 2.9307, 2.7076, 2.7355, 2.5568, 2.8158, 2.7648, 2.7289, 2.5391, 2.76, 2.8105, 2.7889, 2.6633, 2.6734, 2.4605, 2.7643, 2.7262, 2.6183, 2.6645, 2.7364, 2.5503, 2.7075, 2.7585, 2.6689, 2.7687, 2.4957, 2.8083, 2.6739, 2.8194, 2.7832, 2.6024, 2.6541, 2.8033, 2.5198, 2.9408, 2.4674, 2.5797, 2.9127, 2.4726, 2.5339, 2.8029, 2.4984, 2.6328, 2.5291, 2.8358, 2.7087, 2.6884, 2.5808, 2.8395, 2.712, 2.6311, 2.4417, 2.7755, 2.6549, 2.8295, 2.7293, 2.6316, 2.7071, 2.5258, 2.6535, 2.4569, 2.8178, 3.058, 2.8427, 2.6231, 2.7764, 2.6489, 2.6948, 2.7841, 2.2617, 2.6513, 2.5357, 2.6793, 2.6949, 2.6975, 2.6688, 2.7165, 2.6367, 2.7174, 2.7695, 2.6549, 2.6838, 2.4454, 2.6741, 2.2668, 2.72, 2.6255, 2.5961, 2.6834, 2.291, 2.6753, 2.6834, 2.7013, 2.6056, 2.5443, 2.6971, 2.9329, 2.7654, 2.6713, 2.5972, 2.6529, 2.694, 2.8026, 2.7019, 2.8747, 2.6509, 2.3967, 2.7169, 2.6664, 2.5953, 2.6635, 2.6782, 2.7493, 2.7928, 2.4355, 2.744, 2.6721, 2.6354, 2.3713, 2.48, 2.6494, 2.4893, 2.5793, 2.6229, 2.7447, 2.4051, 2.6557, 2.6726, 2.8201, 2.6782, 2.5917, 2.5363, 2.7058, 2.6022, 2.7131, 2.7337, 2.7094, 2.3546, 2.661, 2.659, 2.7489, 2.4742, 2.714, 2.7478, 2.6722, 2.4106, 2.7396, 2.6842, 2.6993, 2.5609, 2.4956, 2.5693, 2.5897, 2.6901, 2.7015, 2.6528, 2.8398, 2.6774, 2.5388, 2.4753, 2.4073, 2.6468, 2.5084, 2.3532, 2.6799, 2.5809, 2.6213, 2.653, 2.6659, 2.7136, 2.6646, 2.6272, 2.5098, 2.5961, 2.6931, 2.7686, 2.4262, 2.7186, 2.726, 2.6361, 2.5036, 2.7092, 2.6581, 2.6852, 2.6101, 2.703, 2.1682, 2.627, 2.5185, 2.7727, 2.2897, 2.3587, 2.6573, 2.587, 2.677, 2.9366, 2.7457, 2.3922, 2.7563, 2.696, 2.7705, 2.472, 2.6906, 2.507, 2.3284, 2.7626, 2.734, 2.6889, 2.6191, 2.2975, 2.6122, 2.4484, 2.7952, 2.7366, 2.5917, 2.4639, 2.7351, 2.7201, 2.7146, 2.6413, 2.7257, 2.6245, 2.4561, 2.6474, 2.6141, 2.5415, 2.4435, 2.6415, 2.6831, 2.6431, 2.7783, 2.6757, 2.7191, 2.7176, 2.7805, 2.7033, 2.6324, 2.7094, 2.7285, 2.5303, 2.7844, 2.6438, 2.3892, 2.3655, 2.6776, 2.7516, 2.3992, 2.4183, 2.6022, 2.6015, 2.6775, 2.7727, 2.6457, 2.6935, 2.4031, 2.7525, 2.8872, 2.7025, 2.7056, 2.4221, 2.6883, 2.6826, 2.7221, 2.4323, 2.7342, 2.7311, 2.4274, 2.657, 2.4775, 2.6567, 2.6618, 2.8037, 2.6687, 2.6684, 2.6332, 2.5956, 2.5271, 2.6441, 2.5248, 2.7235, 2.7057, 2.6911, 2.6232, 2.6452, 2.6573, 2.7289, 2.7378, 2.6557, 2.7542, 2.6005, 2.7217, 2.5503, 2.6714, 2.6963, 2.5746, 2.7476, 2.4019, 2.7033, 2.5929, 2.7447, 2.6174, 2.6747, 2.4309, 2.6709, 2.395, 2.6593, 2.7247, 2.6856, 2.1655, 2.7452, 2.6466, 2.5526, 2.695, 2.7415, 2.6336, 2.6865, 2.6809, 2.7192, 2.5711, 2.5432, 2.4503, 2.6195, 2.4486, 2.7518, 2.586, 2.4285, 2.6433, 2.6295, 2.4442, 2.4755, 2.7352, 2.3952, 2.3833, 2.7556, 2.3276, 2.697, 2.83, 2.5493, 2.6556, 2.6337, 2.403, 2.5065, 2.6661, 2.6721, 2.6827, 2.7256, 2.6077, 2.5737, 2.6751, 2.8128, 2.4412, 2.6479, 2.7133, 2.8614, 2.6735, 2.7155, 2.6144, 2.6982, 2.5911, 2.5156, 2.7398, 2.595, 2.7439, 2.7774, 2.7416, 2.5958, 2.6908, 2.6446, 2.2131, 2.6406, 2.5622, 2.7129, 2.6421, 2.7117, 2.6618, 2.3991, 2.7114, 2.6842, 2.6645, 2.7956, 2.6297, 2.7188, 2.6065, 2.3729, 2.55, 2.6616, 2.4566, 2.7531, 2.6515, 2.6061, 2.3216, 2.5992, 2.9031, 2.7685, 2.4181, 2.5425, 2.6118, 2.6499, 2.4701, 2.6992, 2.7261, 2.7003, 2.3154, 2.6681, 2.6727, 2.6981, 2.405, 2.5652, 2.7151, 2.4815, 2.6966, 2.7837, 2.6401, 2.395, 2.7256, 2.4614, 2.6913, 2.6171, 2.6772, 2.6776, 2.6927, 2.8596, 2.5553, 2.6574, 2.7054, 2.5715, 2.5685, 2.5368, 2.4105, 2.3375, 2.7293, 2.7168, 2.6573, 2.7632, 2.4518, 2.7003, 2.7114, 2.6583, 2.5712, 2.8143, 2.7567, 2.3463, 2.4209, 2.3802, 2.7756, 2.6293, 2.5521, 2.9585, 2.7119, 2.6422, 2.5929, 2.6676, 2.6201, 2.6393, 2.8019, 2.5648, 2.3728, 2.5437, 2.6139, 2.6782, 2.7907, 2.6833, 2.3833, 2.6313, 2.6524, 2.7817, 2.6257, 2.6591, 2.5777, 2.6165, 2.5458, 2.7581, 2.7323, 2.4105, 2.5571, 2.5974, 2.7641, 2.6887, 2.3275, 2.5425, 2.7736, 2.6024, 2.4564, 2.7516, 2.5712, 2.6845, 2.4273, 2.7461, 2.3711, 2.6181, 2.5098, 2.6357, 2.6813, 2.3862, 2.3839, 2.4048, 2.5512, 2.7717, 2.2134, 2.6314, 2.3338, 2.7026, 2.4701, 2.5976, 2.6403, 2.6418, 2.3044, 2.5839, 2.586, 2.6702, 2.7288, 2.6369, 2.5102, 2.6634, 2.3512, 2.6493, 2.7006, 2.648, 2.5859, 2.4516, 2.6008, 2.4755, 2.6245, 2.6062, 2.5295, 2.6597, 2.6849, 2.6966, 2.6929, 2.6434, 2.123, 2.6292, 2.783, 2.6399, 2.3816, 2.5979, 2.6718, 2.3242, 2.7475, 2.6014, 2.6754, 2.7478, 2.666, 2.6151, 2.6039, 2.6919, 2.5641, 2.3757, 2.6495, 2.4713, 2.7121, 2.5035, 2.6928, 2.5651, 2.6154, 2.8715, 2.7525, 2.4284, 2.7371, 2.6648, 2.5952, 2.5875, 2.6214, 2.45, 2.3387, 2.6926, 2.69, 2.4023, 2.6942, 2.6328, 2.5992, 2.6699, 2.5804, 2.7972, 2.8191, 2.7175, 2.5343, 2.6483, 2.6753, 2.7, 2.5795, 2.6512, 2.5337, 2.6494, 2.3659, 2.8324, 2.6891, 2.5467, 2.726, 2.5916, 2.6292, 2.6739, 2.8397, 2.8775, 2.4969, 2.7264, 2.7347, 2.4902, 2.7397, 2.5806, 2.3772, 2.6353, 2.6697, 2.873, 2.6686, 2.5578, 2.6153, 2.5692, 2.6732, 2.6142, 2.7047, 2.4995, 2.4825, 2.6577, 2.6402, 2.4535, 2.7983, 2.4768, 2.4272, 2.6778, 2.5941, 2.5664, 2.3948, 2.6301, 2.6851, 2.3845, 2.6924, 2.5331, 2.6611, 2.4079, 2.5908, 2.5034, 2.6266, 2.661, 2.5631, 2.5867, 2.7001, 2.6629, 2.4613, 2.6377, 2.6143, 2.441, 2.6819, 2.6178, 2.352, 2.6089, 2.608, 2.6434, 2.6062, 2.7583, 2.6533, 2.602, 2.5082, 2.6336, 2.6232, 2.6108, 2.5865, 2.6053, 2.5914, 2.7234, 2.4153, 2.4265, 2.4683, 2.6459, 2.5953, 2.7783, 2.4067, 2.5404, 2.5498, 2.5907, 2.4087, 2.5614, 2.6611, 2.5648, 2.4503, 2.6228, 2.5878, 2.3647, 2.642, 2.6028, 2.3242, 2.6477, 2.5658, 2.8513, 2.7494, 2.312, 2.535, 2.024, 2.6585, 2.7655, 2.601, 2.4372, 2.8105, 2.439, 2.132, 2.7119, 2.6288, 2.5291, 2.6021, 2.6696, 2.7282, 2.6396, 2.2737, 2.5811, 2.2699, 2.6709, 2.3447, 2.6416, 2.4764, 2.6292, 2.3334, 2.5729, 2.6886, 2.6652, 2.6887, 2.5991, 2.65, 2.8987, 2.6372, 2.6767, 2.5627, 2.682, 2.2991, 2.3134, 2.7461, 2.6612, 2.7266, 2.6463, 2.4874, 2.7769, 2.4098, 2.3529, 2.66, 2.594, 2.6047, 2.5626, 2.3903, 2.6398, 2.8407, 2.527, 2.5185, 2.5331, 2.6193, 2.5955, 2.3682, 2.5623, 2.6028, 2.6114, 2.4514, 2.3654, 2.3219, 2.489, 2.5683, 2.5799, 2.7166, 2.6307, 2.5007, 2.6361, 2.7517, 2.5258, 2.6595, 2.6462, 2.3649, 2.5867, 2.5933, 2.6981, 2.5141, 2.7573, 2.4543, 2.5671, 2.5988, 2.6099, 2.4681, 2.6269, 2.5844, 2.5953, 2.6462, 2.7024, 2.6077, 2.6618, 2.5717, 2.5908, 2.7605, 2.6186, 2.3427, 2.5696, 2.3413, 2.5889, 2.6739, 2.5051, 2.5557, 2.2791, 2.6429, 2.7072, 2.6181, 2.3561, 2.525, 2.5798, 2.4688, 2.5803, 2.6355, 2.3281, 2.6011, 2.7101, 2.5975, 2.6497, 2.4445, 2.6376, 2.772, 2.5389, 2.5506, 2.5872, 2.5831, 2.4387, 2.5777, 2.3908, 2.4575, 2.6276, 2.5519, 2.3552, 2.5948, 2.5869, 2.6617, 2.6251, 2.5169, 2.5583, 2.7147, 2.6739, 2.718, 2.6356, 2.6271, 2.5498, 2.7065, 2.3975, 2.4774, 2.529, 2.6474, 2.6044, 2.7745, 2.3225, 2.537, 2.6155, 2.6288, 2.5494, 2.5905, 2.6885, 2.5371, 2.6279, 2.7378, 2.66, 2.6504, 2.3795, 2.4285, 2.325, 2.6663, 2.63, 2.6577, 2.6339, 2.4739, 2.6744, 2.5373, 2.0666, 2.8271, 2.5402, 2.4831, 2.6072, 2.5493, 2.8022, 2.6095, 2.6364, 2.5279, 2.6831, 2.5275, 2.6524, 2.6941, 2.7381, 2.7516, 2.6812, 2.3348, 2.6416, 2.0889, 2.6955, 2.3909, 2.8839, 2.6509, 2.6303, 2.5098, 2.5482, 2.2568, 2.5258, 2.5229, 2.5744, 2.4186, 2.46, 2.6959, 2.4887, 2.4175, 2.3633, 2.6506, 2.5908, 2.5038, 2.3265, 2.4016, 2.6451, 2.749, 2.3535, 2.371, 2.5943, 2.6486, 2.6743, 2.3304, 2.5614, 2.5794, 2.786, 2.6372, 2.7565, 2.6338, 2.6817, 2.4597, 2.5154, 2.6604, 2.5575, 2.5341, 2.4865, 2.7199, 2.4503, 2.672, 2.7948, 2.4683, 2.3442, 2.7109, 2.2669, 2.4849, 2.5414, 2.5147, 2.5268, 2.7823, 2.6246, 2.586, 2.3327, 2.2392, 2.7552, 2.5668, 2.4684, 2.6298, 2.7844, 2.5179, 2.4561, 2.6162, 2.6576, 2.5486, 2.6056, 2.6363, 2.6193, 2.5354, 2.6148, 2.6773, 2.6159, 2.4666, 2.3547, 2.2864, 2.5772, 2.4689, 2.7001, 2.5228, 2.5785, 2.4532, 2.6052, 2.544, 2.6484, 2.3319, 2.4127, 2.5534, 2.5062, 2.544, 2.488, 2.6943, 2.5867, 2.1419, 2.5962, 2.7281, 2.655, 2.5062, 2.3094, 2.623, 2.5728, 2.4714, 2.5332, 2.563, 2.611, 2.6079, 2.7869, 2.1489, 2.604, 2.896, 2.6872, 2.6076, 2.4108, 2.7832, 2.2565, 2.2709, 2.7178, 2.3761, 2.7106, 2.5368, 2.2569, 2.6193, 2.715, 2.1242, 2.6192, 2.6076, 2.3722, 2.285, 2.0633, 2.5694, 2.5764, 2.4951, 2.1257, 2.6944, 2.5192, 2.5407, 2.5199, 2.5858, 2.6173, 2.6146, 2.5748, 2.5749, 2.4329, 2.5953, 2.546, 2.6596, 2.6043, 2.4575, 2.6832, 2.6417, 2.5749, 2.6762, 2.3283, 2.579, 2.5639, 2.687, 2.3809, 2.5057, 2.5593, 2.6007, 2.4467, 2.5112, 2.5298, 2.6199, 2.5337, 2.7164, 2.6804, 2.3408, 2.2686, 2.6767, 2.6538, 2.6135, 2.6055, 2.592, 2.624, 2.3881, 2.7534, 2.3849, 2.5238, 2.5522, 2.5835, 2.6145, 2.521, 2.5499, 2.6011, 2.6593, 2.4843, 2.4562, 2.4568, 2.3707, 2.2922, 2.3338, 2.194, 2.5723, 2.7147, 2.549, 2.3657, 2.433, 2.4855, 2.691, 2.6777, 2.3477, 2.7969, 2.7959, 2.2532, 2.2903, 2.6165, 2.5484, 2.5253, 2.6592, 2.5676, 2.5717, 2.6337, 2.6378, 2.4403, 2.5375, 2.3263, 2.5888, 2.6805, 2.6094, 2.5009, 2.557, 2.5614, 2.7694, 2.3426, 2.5365, 2.7195, 2.5417, 2.618, 2.4124, 2.3731, 2.7314, 2.2889, 2.39, 2.6012, 2.7082, 2.3463, 2.3023, 2.4933, 2.6192, 2.5117, 2.5856, 2.661, 2.6626, 2.6275, 2.5296, 2.676, 2.2274, 2.6713, 2.6413, 2.6813, 2.3245, 2.5407, 2.544, 2.5539, 2.776, 2.5983, 2.6782, 2.6231, 2.7344, 2.5744, 2.6407, 2.37, 2.5569, 2.6258, 2.5596, 2.5118, 2.5725, 2.3499, 2.5098, 2.5558, 2.6205, 2.4964, 2.5503, 2.475, 1.9507, 2.5725, 2.3201, 2.5874, 2.5429, 2.614, 2.4812, 2.7292, 2.5689, 2.6362, 2.6102, 2.2582, 2.2794, 2.0222, 2.4263, 2.4649, 2.8351, 2.6496, 2.6195, 2.7398, 2.4105, 2.5663, 2.5428, 2.6251, 2.5586, 2.5129, 2.3824, 2.4584, 2.7082, 2.5433, 2.5831, 2.7831, 2.5612, 2.6875, 2.6883, 2.48, 2.6188, 2.4985, 2.2587, 2.2916, 2.7353, 2.5948, 2.5898, 2.4056, 2.6131, 2.5588, 2.6165, 2.5837, 2.3225, 2.583, 2.5116, 2.5885, 2.3471, 2.566, 2.6744, 2.6614, 2.5058, 2.4064, 2.3317, 2.641, 2.6199, 2.0906, 2.6009, 2.6207, 2.6648, 2.5671, 2.3444, 2.3655, 2.6242, 2.6661, 2.3435, 2.3832, 2.535, 2.6295, 2.4573, 2.4665, 2.4878, 2.6408, 2.7153, 2.255, 2.6485, 2.6464, 2.6315, 2.6449, 2.5097, 2.645, 2.3678, 2.7378, 2.5681, 2.4313, 2.3862, 2.5084, 2.6607, 2.4754, 2.6048, 2.5585, 2.5051, 2.9205, 2.4892, 2.5474, 2.4468, 2.5703, 2.6838, 2.7197, 2.6354, 2.3467, 2.5988, 2.5463, 2.818, 2.5219, 2.5097, 2.7372, 2.2178, 2.5022, 2.657, 2.6538, 2.8473, 2.273, 2.7423, 2.6952, 2.5183, 2.5625, 2.5766, 2.6683, 2.8128, 2.5156, 2.2926, 2.3727, 2.2734, 2.2721, 2.5063, 2.7924, 2.7666, 2.5998, 2.4989, 2.3213, 2.3327, 2.6076, 2.6206, 2.4584, 2.1841, 2.6025, 2.5783, 2.4842, 2.5564, 2.7508, 2.2115, 2.3977, 2.5486, 2.5251, 2.5093, 2.5534, 2.3874, 2.4883, 2.6228, 2.3621, 2.536, 2.5646, 2.3228, 2.268, 2.5686, 2.6235, 2.6408, 2.354, 2.6219, 2.2822, 2.3391, 2.5467, 2.5551, 2.5814, 2.368, 2.6204, 2.2708, 2.7089, 2.581, 2.6026, 2.6026, 2.581, 2.1903, 2.0985, 2.4712, 2.5706, 2.6874, 2.294, 2.6538, 2.6213, 2.5032, 2.2207, 2.5292, 2.5053, 2.5496, 2.3256, 2.5565, 2.3582, 2.281, 2.3289, 2.4954, 2.3367, 2.4802, 2.6219, 2.6299, 2.3464, 2.5017, 2.1613, 2.6554, 2.5016, 2.2971, 2.5708, 2.5388, 2.5626, 2.5705, 2.3323, 2.6099, 2.6464, 2.4871, 2.4965, 2.3588, 2.5009, 2.3393, 2.4454, 2.7285, 2.6007, 2.6868, 2.382, 2.2002, 2.6767, 2.39, 2.4278, 2.5571, 2.3461, 2.4511, 2.2928, 2.754, 2.6142, 2.5157, 2.61, 2.5347, 2.5638, 2.5194, 2.5697, 2.0272, 2.5708, 2.482, 2.661, 2.4266, 2.4285, 2.303, 1.9468, 2.2461, 2.6181, 2.5558, 2.2491, 2.6277, 2.2185, 2.7151, 2.6629, 2.6802, 2.6078, 2.4885, 2.5047, 2.5624, 2.4973, 2.79, 2.59, 2.2517, 2.5332, 2.6289, 2.5709, 2.802, 2.5928, 2.7597, 2.6621, 2.2151, 2.3215, 2.4822, 2.6858, 2.76, 2.5448, 2.1999, 2.7104, 2.2379, 2.3955, 2.5842, 2.6551, 2.5036, 2.5069, 2.6849, 2.6641, 2.4901, 2.6488, 2.5654, 2.6401, 2.5737, 2.4595, 2.5162, 2.5059, 2.5425, 2.6018, 2.4249, 2.4641, 2.5467, 2.4291, 2.6439, 2.4628, 2.3924, 2.5266, 2.7076, 2.575, 2.3491, 2.3501, 2.3002, 2.5712, 2.5035, 2.5816, 2.5656, 2.5425, 2.6715, 2.5016, 2.7099, 2.6557, 2.4171, 2.3051, 2.4729, 2.7601, 2.5711, 2.4308, 2.7179, 2.591, 2.6711, 2.7023, 2.604, 2.5827, 2.6245, 2.6044, 2.5016, 2.5224, 2.1854, 2.5079, 2.5311, 2.4424, 2.4946, 2.2311, 2.585, 2.4988, 2.4564, 2.3947, 2.5785, 2.6189, 2.4019, 2.5996, 2.6456, 2.4716, 2.5768, 2.625, 2.5185, 2.515, 2.3658, 2.2298, 2.3533, 2.5794, 2.0909, 2.6691, 2.6156, 2.4824, 1.9932, 2.5649, 2.2939, 2.3043, 2.2369, 2.5736, 2.7234, 2.6608, 2.6076, 2.6289, 2.6709, 2.6513, 2.6197, 2.4676, 2.564, 2.6924, 1.9973, 2.5313, 2.5853, 2.3565, 2.5099, 2.5859, 2.194, 2.2742, 2.384, 2.5018, 2.1987, 2.5909, 2.351, 2.5565, 2.2908, 2.4502, 2.7345, 2.7094, 2.517, 2.6318, 2.5887, 2.0562, 2.4807, 2.4166, 2.6174, 2.1982, 2.5688, 2.3459, 2.5993, 2.5526, 2.2556, 2.5442, 2.5146, 2.6232, 2.5063, 2.0084, 2.4733, 2.5336, 2.5324, 2.6087, 2.6152, 2.6439, 2.3551, 2.5207, 2.488, 2.5086, 2.5583, 2.1799, 2.289, 2.3537, 2.6028, 2.3206, 2.7816, 2.2049, 2.2834, 2.7261, 2.6142, 2.5575, 2.4701, 2.6096, 2.4302, 2.5661, 2.4561, 2.6753, 2.218, 2.6524, 2.3239, 2.5161, 2.5623, 2.1493, 2.253, 2.462, 2.5226, 2.5832, 2.5899, 2.7101, 2.2297, 2.6753, 2.5133, 2.6989, 2.5559, 2.2221, 2.3224, 2.6465, 2.2001, 2.5351, 2.42, 2.7441, 2.5997, 2.4189, 2.5163, 2.5882, 2.6794, 2.3936, 2.4737, 2.657, 2.6562, 2.6255, 2.4739, 2.6528, 2.4718, 2.4332, 2.6922, 2.4511, 2.4395, 2.7285, 2.2347, 2.8122, 2.5769, 2.6508, 2.5427, 2.795, 2.4328, 2.6839, 2.4765, 2.5755, 2.5191, 2.6318, 2.6197, 2.2249, 2.5808, 2.5478, 2.2253, 2.0134, 2.6319, 2.5983, 2.5918, 2.6199, 2.6121, 2.6813, 2.4572, 2.5691, 2.6812, 2.3877, 2.5449, 2.5184, 2.3378, 2.5739, 2.6169, 2.0423, 2.331, 2.4843, 2.2853, 2.2519, 2.5388, 2.5199, 2.7458, 2.4507, 2.6417, 2.6157, 2.5929, 2.7002, 2.0384, 2.3218, 2.5818, 2.6667, 2.1387, 2.5055, 2.3307, 2.4684, 2.2773, 2.3824, 2.5241, 2.1306, 2.5444, 2.2039, 2.5636, 2.4364, 2.6444, 2.7291, 2.5241, 2.8066, 2.6246, 2.344, 2.4685, 2.0831, 2.5696, 2.4911, 2.593, 2.4849, 2.3989, 2.4866, 2.442, 2.6262, 2.5334, 2.5133, 2.5959, 2.3947, 2.5634, 2.7696, 2.5123, 2.5793, 2.5059, 2.5927, 2.2398, 2.6487, 2.4143, 2.5613, 2.478, 2.5718, 2.4939, 2.5859, 2.333, 2.5697, 2.3289, 2.6959, 2.5137, 2.5887, 2.4592, 2.7549, 2.4065, 2.4313, 2.4995, 2.5366, 2.5717, 2.5664, 2.4383, 2.392, 2.0251, 2.6141, 2.4941, 2.2393, 2.6432, 2.5237, 2.2923, 2.3216, 2.3148, 2.6422, 2.6247, 2.5675, 2.4986, 2.4999, 2.4883, 2.3973, 2.8233, 2.5732, 2.595, 2.5677, 2.4785, 2.501, 2.6367, 2.5102, 2.078, 2.6983, 2.5654, 2.5715, 2.4154, 2.4457, 2.1411, 2.4325, 2.1678, 2.4973, 2.4261, 2.5894, 2.2828, 2.3226, 2.2357, 2.5895, 2.6509, 2.591, 2.4009, 2.4556, 2.5189, 2.4166, 2.3536, 2.512, 2.5952, 2.4323, 2.3034, 2.1943, 2.3823, 2.3887, 2.5719, 2.5133, 2.4406, 2.6292, 2.5142, 2.5236, 2.6931, 2.5402, 2.5985, 2.6638, 2.655, 2.5139, 2.448, 1.963, 2.4394, 2.3532, 2.479, 2.4027, 2.5568, 2.6499, 2.1437, 2.1709, 2.5327, 2.5036, 2.4967, 2.6391, 2.3119, 2.5469, 2.6116, 2.4941, 2.4603, 2.6164, 2.6574, 2.3234, 2.3851, 2.4973, 2.5295, 2.5232, 2.4624, 2.3838, 2.6382, 2.5915, 2.2431, 2.6096, 2.5717, 2.3776, 2.6577, 2.5366, 2.4781, 2.6174, 2.2488, 2.4842, 2.6651, 2.5336, 2.4774, 2.4087, 2.5681, 2.4884, 2.5635, 2.2676, 2.4172, 2.617, 2.4511, 2.3347, 2.4553, 2.5292, 2.1888, 2.4539, 2.4626, 2.3226, 2.6159, 2.6572, 2.5579, 2.5522, 2.6609, 2.534, 2.5624, 2.4815, 2.5403, 2.6086, 2.5692, 2.6119, 2.4746, 2.5556, 2.4293, 2.5776, 2.3781, 2.5081, 2.2999, 2.5968, 2.4754, 2.494, 2.6443, 2.2908, 2.5063, 2.616, 2.5089, 2.5594, 2.4821, 2.4931, 2.557, 2.5453, 2.3704, 2.3789, 2.2436, 2.545, 2.6795, 2.275, 2.5191, 2.5952, 2.6638, 2.456, 2.3543, 2.4889, 2.4125, 2.4965, 2.6508, 2.5033, 2.5333, 2.5216, 2.5289, 2.621, 2.3221, 2.5786, 2.7257, 2.3559, 2.6084, 2.5858, 2.3208, 2.578, 2.6393, 2.4636, 2.3664, 2.5909, 2.3827, 2.6401, 2.514, 2.244, 2.5304, 2.5614, 2.8409, 2.6758, 2.5875, 2.3756, 2.2515, 2.4683, 2.2776, 2.7833, 2.4143, 2.3519, 2.5533, 2.6316, 2.5242, 2.6196, 2.5279, 2.5321, 2.5063, 2.538, 2.4456, 2.2695, 2.4807, 2.6049, 2.3845, 2.5779, 2.4795, 2.2149, 2.342, 1.9859, 2.1213, 2.5307, 2.4655, 2.5619, 2.5314, 2.5465, 2.5684, 2.6107, 2.5141, 2.5018, 2.5918, 2.2679, 2.532, 2.5223, 2.5294, 2.4212, 2.6056, 2.4292, 2.5264, 2.4647, 2.0267, 2.0943, 2.723, 2.6537, 2.5144, 2.3964, 2.7768, 2.4548, 2.5171, 2.6392, 2.5496, 2.2712, 2.6423, 2.3107, 2.5602, 2.6611, 2.6689, 2.2726, 2.3037, 2.6101, 2.6065, 2.6499, 2.3815, 2.4726, 2.5607, 2.0164, 2.5035, 2.6765, 2.3056, 2.2216, 2.5275, 2.5576, 2.2536, 2.5275, 2.3291, 2.526, 2.4229, 2.5523, 2.4371, 2.4904, 2.3445, 2.5673, 2.4909, 2.6747, 2.6523, 2.5508, 2.1829, 2.534, 2.4671, 2.4072, 2.598, 2.4752, 2.68, 1.9765, 2.6011, 1.9217, 2.1614, 2.4339, 2.628, 2.2908, 2.3461, 2.3419, 2.4729, 2.4694, 2.4926, 2.3717, 2.6499, 2.4964, 2.5634, 2.4852, 2.4048, 2.4749, 2.4039, 2.5544, 2.864, 2.3813, 2.2365, 2.4315, 2.2963, 2.3043, 2.4088, 2.2104, 2.5571, 2.5595, 2.2048, 2.6136, 2.5336, 2.4777, 2.1796, 2.2993, 2.0919, 2.3293, 2.5717, 2.0527, 2.6371, 2.3852, 2.2197, 2.5053, 2.5211, 2.369, 2.5704, 2.4153, 2.5934, 2.6965, 2.5104, 2.5681, 2.3713, 2.6365, 2.5637, 2.5362, 2.0923, 2.576, 2.6959, 2.5674, 2.5448, 2.3795, 2.5926, 2.641, 2.2007, 2.279, 2.3488, 2.5879, 2.4869, 2.5043, 2.508, 2.19, 2.6117, 2.4355, 2.6309, 2.2137, 2.5939, 2.5585, 2.3293, 2.2886, 2.6254, 2.5303, 2.5373, 2.3383, 2.6162, 2.5615, 2.3969, 2.2585, 2.6241, 2.4636, 2.6978, 2.598, 2.2086, 2.6182, 2.3004, 1.9793, 2.6396, 2.4335, 2.1665, 2.8198, 2.4864, 2.6512, 2.357, 2.5103, 2.4734, 2.3952, 2.4064, 2.544, 2.4424, 2.2394, 2.6307, 2.5873, 2.3466, 2.5462, 2.5552, 2.438, 2.603, 2.5041, 2.2561, 2.6195, 2.5643, 2.5668, 2.5048, 2.4234, 2.6427, 2.5742, 2.3285, 2.567, 2.8675, 2.257, 2.5267, 2.375, 2.2382, 2.3876, 2.2724, 2.4843, 2.2224, 2.4655, 2.6179, 2.2042, 2.5734, 2.2394, 2.0899, 2.3918, 2.6095, 2.5927, 2.333, 2.5766, 2.2574, 2.5519, 2.6283, 2.1906, 2.2576, 2.4654, 2.2565, 2.2638, 2.3462, 2.5765, 2.5822, 2.4658, 2.2048, 2.5169, 2.5476, 2.1513, 2.5681, 2.6847, 2.8674, 2.1713, 2.3065, 2.2987, 2.6525, 2.6344, 2.495, 2.5828, 2.3725, 2.4211, 2.5664, 2.6068, 2.5464, 2.6168, 2.6394, 2.5998, 2.5241, 2.461, 2.6355, 2.4945, 2.447, 2.5155, 2.5771, 2.6057, 2.588, 2.6021, 2.5718, 2.4436, 2.8307, 2.665, 2.532, 2.126, 2.5561, 2.2791, 2.5696, 2.5919, 2.634, 2.2902, 2.409, 2.3709, 2.332, 2.4798, 2.4266, 2.5249, 2.5701, 2.2942, 2.2534, 2.5521, 2.1779, 2.1577, 2.1849, 2.5215, 2.5996, 2.5576, 2.6587, 2.5482, 2.539, 2.5649, 2.2792, 2.272, 2.5121, 2.1542, 2.6226, 2.53, 1.9562, 2.5879, 2.6071, 2.5101, 2.3586, 2.4612, 2.1999, 2.361, 2.6864, 2.3787, 2.4566, 2.2862, 2.7889, 2.2749, 2.6372, 2.4842, 2.3338, 2.5549, 2.4923, 2.4753, 2.6334, 2.57, 2.4302, 2.518, 2.5512, 2.217, 2.4842, 2.5226, 2.3003, 2.5434, 2.5905, 2.673, 2.3057, 2.6008, 2.1833, 2.6443, 2.611, 2.4932, 2.4712, 2.517, 1.9365, 2.4472, 2.5011, 2.5584, 2.5596, 2.6648, 2.422, 2.1673, 2.5953, 2.4894, 2.151, 2.4228, 2.5015, 2.6023, 2.5725, 2.3157, 2.5768, 2.502, 2.4492, 2.5556, 2.5189, 2.3, 2.4861, 2.4338, 2.6603, 2.3338, 2.0384, 2.5437, 2.3958, 2.3166, 2.5349, 2.2547, 2.5298, 2.2058, 2.2257, 2.5597, 2.2103, 2.7674, 2.5259, 2.5785, 2.2004, 2.5336, 2.5867, 2.6004, 2.6071, 2.5441, 2.5082, 2.5784, 2.4696, 2.6398, 2.5812, 2.2431, 2.52, 2.6425, 2.5493, 2.5913, 2.3933, 2.5232, 2.4057, 2.4733, 2.5824, 2.5212, 2.1305, 2.2197, 2.1316, 2.4757, 2.6119, 2.3971, 2.4252, 2.5002, 2.6587, 2.6044, 2.3196, 2.4691, 2.4908, 2.553, 2.3125, 2.5354, 2.5483, 2.4741, 2.6387, 2.4524, 2.3876, 2.5764, 2.5713, 2.5012, 2.6422, 2.4879, 2.3647, 2.6174, 2.4722, 2.5493, 2.2544, 2.3884, 2.2079, 2.2159, 1.9281, 2.5086, 2.6311, 2.6891, 2.546, 2.3717, 2.4749, 2.5115, 2.4954, 2.6825, 2.2298, 2.4623, 2.5633, 2.1649, 2.6197, 2.4713, 2.5714, 2.5268, 2.5152, 2.3236, 2.586, 2.5083, 2.3824, 2.5218, 2.3337, 2.2052, 2.1516, 2.2362, 2.4601, 2.4882, 2.3732, 2.6161, 2.631, 2.2596, 2.5268, 2.6597, 2.2344, 2.5156, 2.2065, 2.2782, 2.4528, 2.565, 2.69, 2.2688, 2.5456, 2.4941, 2.4215, 2.4949, 2.1579, 2.2238, 2.4402, 2.2734, 2.5771, 2.4843, 2.2036, 2.4995, 2.1745, 2.499, 2.3327, 2.3358, 2.518, 2.1661, 2.3644, 2.569, 2.3442, 2.583, 2.5457, 2.5035, 2.5065, 2.3084, 2.4609, 2.4849, 2.6783, 2.6057, 2.1865, 2.28, 2.6035, 2.4371, 2.5975, 2.5208, 2.5618, 2.5166, 2.6684, 2.3844, 2.3452, 2.6269, 2.4363, 2.1584, 2.5135, 2.8491, 2.5394, 2.435, 2.5557, 2.5604, 2.5058, 2.2269, 2.3792, 2.1589, 2.618, 2.4415, 2.5071, 2.5279, 2.6736, 2.3136, 2.1406, 2.344, 2.107, 2.699, 2.589, 2.3382, 2.2069, 2.2106, 2.6096, 2.203, 2.3541, 2.2269, 2.2187, 2.4711, 2.442, 2.2328, 2.5782, 2.4856, 2.4458, 2.6523, 2.1727, 2.4743, 2.5558, 2.2799, 2.2371, 2.1713, 2.6199, 2.5381, 2.6347, 2.3546, 2.4854, 2.4345, 2.5048, 2.0209, 2.4891, 2.4931, 2.4919, 2.3429, 2.6216, 2.4488, 2.6104, 2.3774, 2.4717, 2.5581, 2.3575, 2.3839, 1.9329, 2.6182, 2.3511, 2.8911, 2.3923, 2.4088, 2.3232, 2.4604, 2.5127, 2.3042, 2.548, 2.4019, 1.8889, 2.4963, 2.6028, 2.1726, 2.525, 2.4962, 2.4552, 2.2472, 2.2932, 2.2873, 2.0368, 2.2443, 2.5737, 2.3029, 2.6233, 2.4345, 2.3424, 2.7258, 2.1299, 2.2212, 2.5242, 2.6342, 2.2834, 2.5051, 2.559, 2.4473, 2.5297, 2.565, 2.0918, 2.469, 2.3244, 2.5604, 2.3852, 2.5026, 2.4833, 2.1339, 2.4429, 2.1432, 2.1563, 2.5863, 2.57, 2.5989, 2.1423, 2.4654, 2.4759, 2.5448, 2.5606, 2.4123, 2.5346, 2.4704, 2.127, 2.3253, 2.519, 2.507, 2.0043, 2.2693, 2.5761, 2.1553, 2.5692, 2.5677, 2.6468, 2.5775, 1.9795, 2.4985, 2.4032, 2.2219, 2.118, 2.4664, 2.1664, 2.4933, 2.2493, 2.5446, 2.3479, 2.213, 2.217, 2.5456, 2.5574, 2.4108, 2.5327, 2.4061, 2.5402, 2.6247, 2.3852, 1.8657, 2.4369, 2.2771, 2.2709, 2.1505, 1.9683, 2.1264, 2.5274, 2.2105, 2.4914, 2.549, 2.3918, 2.4774, 2.3908, 2.5258, 2.672, 1.9866, 2.3578, 2.344, 2.2548, 2.5317, 2.5118, 2.3193, 2.4968, 2.129, 2.5414, 1.9576, 2.3562, 2.6216, 2.5092, 2.5505, 2.7275, 2.2573, 2.5604, 2.0277, 2.5588, 2.6336, 2.5737, 2.6108, 2.4851, 2.2611, 2.3404, 2.566, 2.562, 2.1036, 2.4688, 2.4828, 2.5424, 2.1715, 2.3778, 2.5043, 2.1253, 2.4654, 2.0989, 2.3678, 2.5884, 2.4937, 2.0298, 1.9947, 2.5488, 2.217, 2.3146, 2.2155, 2.5035, 2.5805, 2.4436, 2.3567, 2.35, 2.2821, 2.8385, 2.3346, 2.175, 2.6111, 2.1173, 2.1417, 2.6872, 1.9584, 2.4139, 2.5221, 2.4358, 2.2371, 2.5779, 2.5433, 2.2186, 2.4749, 2.5711, 2.6656, 2.4018, 2.1445, 2.4838, 2.456, 2.7019, 2.6258, 2.2888, 2.5943, 2.3058, 2.5236, 2.483, 2.5697, 2.3406, 2.4427, 2.5662, 2.6352, 2.3574, 2.0881, 2.2547, 2.466, 2.4396, 2.5878, 2.5458, 2.5099, 2.5442, 2.1164, 2.4599, 2.5046, 2.5803, 2.487, 2.526, 2.4303, 2.1103, 2.3028, 2.3394, 2.5724, 2.426, 2.4722, 2.492, 2.5548, 2.4538, 2.471, 2.4411, 2.4174, 2.5626, 2.4607, 2.4443, 2.532, 2.145, 2.4159, 2.4097, 2.271, 2.2429, 2.5704, 2.3198, 2.5265, 2.2226, 2.5317, 2.5405, 2.5252, 2.4806, 2.567, 2.5221, 2.6763, 2.6376, 2.1283, 2.5915, 2.2203, 2.446, 2.1394, 2.4171, 2.02, 2.1912, 2.1623, 2.3784, 2.2461, 2.586, 2.2318, 2.4003, 1.9314, 2.5273, 2.2901, 2.5961, 2.6126, 2.5797, 2.4788, 2.2338, 2.6919, 2.501, 2.5204, 2.3846, 2.463, 2.4721, 2.446, 2.5279, 2.0944, 2.2757, 2.5714, 2.5874, 2.2959, 2.4752, 1.9366, 2.0979, 2.6184, 2.1043, 2.5269, 2.2093, 2.1365, 2.5131, 2.7298, 2.4081, 2.5343, 2.6592, 2.4581, 2.3246, 2.1557, 2.1488, 2.5551, 2.5496, 1.8411, 2.7019, 2.347, 2.8183, 2.1204, 2.4356, 2.5127, 2.5565, 2.5496, 2.441, 2.7608, 2.4122, 2.5181, 2.1846, 2.5216, 2.4842, 2.5064, 2.4886, 2.1999, 2.4825, 2.3088, 2.1145, 2.3848, 2.4163, 2.622, 2.2018, 2.5964, 2.0858, 2.547, 2.1975, 2.1204, 2.528, 2.5246, 2.4099, 2.459, 2.1925, 2.5187, 2.0683, 2.6505, 2.5905, 2.4406, 2.3151, 2.0345, 2.4372, 2.518, 2.2947, 2.4456, 2.5027, 2.3121, 2.1543, 2.1545, 2.5925, 2.1871, 2.6105, 2.5202, 1.9145, 2.4088, 2.5185, 2.5712, 2.42, 2.5161, 2.5126, 2.3893, 2.4806, 2.5176, 2.4068, 2.2567, 2.1145, 2.3371, 2.1426, 2.2482, 2.5094, 2.6613, 2.4121, 2.308, 2.1743, 2.5753, 2.3565, 2.0916, 2.351, 2.3958, 2.2198, 2.4793, 2.2956, 2.2181, 2.4086, 2.1387, 2.0286, 2.4666, 2.3954, 2.6303, 2.6969, 2.3251, 2.4866, 1.9129, 2.1992, 2.1275, 2.4238, 2.4588, 2.0888, 2.2544, 2.1928, 2.4249, 2.2673, 2.4286, 2.5115, 2.4924, 2.4617, 2.3981, 2.2094, 2.2052, 2.5913, 2.0533, 2.4439, 2.4753, 2.1392, 2.1263, 2.0278, 2.1573, 2.1298, 2.5821, 2.4025, 2.4367, 2.4774, 2.4063, 2.2891, 2.1956, 2.4439, 2.3626, 2.645, 2.4703, 2.5208, 2.2431, 2.3681, 2.5239, 2.3478, 2.4796, 2.0958, 2.4451, 2.3651, 2.5026, 2.5095, 2.0664, 2.5125, 2.2061, 1.9419, 2.0308, 2.6018, 2.5493, 2.1719, 2.3824, 2.5627, 2.1003, 2.2762, 2.3812, 2.3052, 2.3804, 2.5096, 1.8762, 2.5438, 2.4367, 2.1029, 2.554, 2.1415, 2.1796, 2.6386, 2.3557, 1.9331, 2.6461, 2.3867, 2.5402, 2.5558, 2.0673, 2.5226, 2.1183, 2.1073, 2.5837, 2.1263, 2.4453, 2.4199, 2.5743, 2.1537, 2.3748, 2.0607, 2.5784, 2.5617, 2.528, 2.2013, 2.2285, 2.5657, 1.8543, 2.4713, 2.5106, 2.1971, 2.3991, 2.402, 2.5233, 2.3282, 2.5794, 2.3667, 2.3771, 2.472, 2.5057, 2.3943, 2.2619, 2.6095, 2.4315, 2.4584, 2.4441, 2.551, 2.4503, 2.4437, 2.4767, 2.5892, 2.1395, 2.051, 2.5003, 2.1714, 2.6838, 2.3949, 2.4382, 2.3366, 2.4249, 2.2789, 2.3223, 2.0465, 2.5063, 2.1253, 2.0742, 2.59, 2.5415, 2.4667, 2.1253, 2.4688, 1.9914, 2.0809, 2.5309, 2.1204, 2.4865, 2.2403, 1.7992, 2.5319, 2.3497, 2.4456, 1.9461, 2.1904, 2.0046, 1.7484, 2.394, 2.5147, 2.0255, 2.2675, 1.7373, 2.3099, 2.353, 1.7722, 2.037, 2.3923, 2.5031, 2.5034, 2.0437, 2.4933, 2.4652, 2.5839, 2.4724, 1.9033, 2.3066, 2.3238, 2.1089, 2.4698, 2.1393, 2.3317, 2.4651, 2.3061, 2.5265, 2.1266, 2.5769, 2.5499, 2.1628, 1.9923, 2.6372, 2.2638, 2.5434, 2.4559, 2.6316, 2.2827, 2.3712, 2.4627, 2.4712, 2.2981, 2.5176, 2.5876, 2.5047, 2.2248, 2.6926, 2.5433, 2.1709, 2.0936, 2.5089, 2.1691, 2.1562, 2.2598, 2.3861, 2.3016, 2.4861, 2.1525, 2.1471, 2.5537, 2.5908, 2.5585, 1.9649, 2.4487, 2.6897, 2.3502, 2.5001, 1.6741, 2.3164, 2.5023, 2.359, 2.0659, 2.627, 2.5577, 2.5087, 2.0954, 2.5565, 2.4775, 2.5725, 2.3364, 2.2656, 2.3023, 2.6, 2.386, 2.4287, 1.9981, 2.3627, 2.6099, 2.3539, 1.9306, 2.2107, 2.4975, 2.4371, 2.5898, 2.7223, 2.0621, 1.9731, 2.1693, 2.4861, 2.3189, 1.8856, 2.406, 2.3899, 2.1799, 2.5197, 2.4648, 2.4204, 2.4115, 2.3482, 2.0353, 2.5179, 2.4619, 2.0423, 2.355, 2.0967, 2.0318, 2.3394, 2.1932, 2.5129, 2.2356, 2.0529, 2.5361, 2.0063, 1.9588, 2.46, 2.4034, 2.5466, 1.8963, 2.3192, 2.4631, 2.3171, 1.9324, 2.0829, 2.4696, 2.4994, 2.5223, 2.5279, 2.5221, 2.4535, 2.5094, 2.4383, 2.5383, 2.0608, 2.3788, 2.0269, 2.2785, 2.4316, 2.533, 2.0404, 2.3952, 2.4847, 1.9801, 2.3026, 2.6258, 2.3666, 2.4122, 2.4582, 2.5599, 2.4707, 2.508, 2.3584, 2.515, 2.1078, 2.0628, 2.4205, 2.3585, 2.2784, 2.4985, 2.2163, 2.4267, 2.3744, 2.5013, 2.1582, 2.5651, 2.5081, 2.0822, 2.5879, 2.4032, 2.2354, 2.3288, 1.9461, 1.6022, 2.0842, 2.0038, 2.2141, 2.3771, 2.3914, 2.4255, 2.2975, 2.2898, 2.2737, 2.0918, 2.4649, 2.0766, 1.9125, 2.6575, 2.579, 2.4452, 2.3916, 2.2497, 2.4436, 2.47, 2.5337, 2.4843, 2.4411, 2.2668, 2.462, 2.2015, 2.2688, 2.0523, 2.2388, 2.1264, 2.0027, 1.9687, 2.2354, 2.2556, 2.4323, 2.5352, 2.4796, 1.9027, 2.5264, 2.3986, 2.2093, 2.3317, 2.0075, 1.865, 2.2451, 2.427, 2.4018, 2.4637, 2.5355, 2.2898, 2.495, 2.5078, 2.4694, 2.5164, 1.9842, 2.5376, 2.0219, 2.3905, 1.7374, 2.4354, 2.0352, 2.3351, 2.5089, 2.3943, 2.2207, 2.3615, 2.5326, 2.5139, 2.5212, 2.3786, 2.2558, 2.3861, 2.4063, 2.254, 2.3118, 2.5651, 1.9302, 2.3746, 2.2968, 2.2306, 2.4286, 2.1198, 2.1828, 2.2764, 2.4548, 2.5398, 2.5102, 1.984, 2.4982, 2.5539, 2.2234, 2.251, 2.2882, 2.074, 2.1421, 2.0637, 2.4093, 2.3195, 2.2382, 2.5365, 2.0494, 2.4125, 1.95, 2.1119, 1.9817, 2.4952, 2.2978, 2.389, 2.3336, 2.6022, 2.4633, 1.5895, 1.7775, 2.4186, 2.5152, 2.6526, 2.4794, 2.1717, 2.347, 2.4587, 2.3737, 2.4308, 2.4101, 2.3535, 2.1018, 2.2236, 1.9263, 1.9684, 2.1446, 2.316, 2.4365, 2.2871, 2.5416, 2.0627, 2.0935, 2.3805, 2.3805, 2.6008, 2.2607, 2.4852, 2.7406, 2.4007, 2.0051, 2.5205, 2.5664, 2.099, 2.3461, 2.0183, 2.5059, 2.0888, 2.0937, 2.4125, 2.4326, 2.4263, 2.3506, 2.1439, 2.4822, 2.3993, 2.1021, 1.9648, 2.3395, 2.2223, 2.0838, 1.909, 2.6465, 2.4052, 2.3419, 2.5266, 2.3975, 2.5797, 2.3793, 2.3637, 2.53, 2.4348, 2.424, 2.3354, 2.3387, 2.4418, 2.2832, 2.41, 2.3043, 2.2934, 2.0298, 2.3381, 2.091, 2.0924, 2.3166, 2.4626, 2.1005, 2.309, 2.3952, 2.58, 2.3856, 1.8334, 2.5154, 2.4291, 2.5562, 2.4319, 2.382, 2.4106, 2.314, 2.4607, 1.6592, 2.4016, 2.3088, 2.281, 2.3307, 1.7958, 2.2783, 1.9739, 2.1864, 2.6539, 1.7046, 2.4267, 2.4247, 1.5425, 2.5003, 1.8017, 2.4917, 2.6039, 1.5314, 2.2728, 2.4194, 2.1054, 2.5203, 2.1794, 2.1823, 2.5022, 2.297, 2.1382, 2.4838, 2.3905, 2.5698, 2.5503, 2.263, 2.5694, 2.3015, 2.3229, 2.0364, 2.3127, 2.5056, 2.4927, 2.5228, 2.4671, 2.5474, 2.3612, 2.443, 2.5877, 2.5067, 1.6081, 1.5802, 2.351, 2.4609, 2.2949, 2.1592, 2.3964, 2.55, 2.5143, 2.6553, 2.0749, 2.4528, 2.3601, 2.6511, 1.8842, 2.0235, 2.34, 2.4115, 2.454, 2.2152, 2.3128, 2.2864, 2.3008, 1.8362, 2.5293, 2.3694, 2.3948, 2.3977, 2.4771, 2.4737, 2.0766, 2.0147, 2.6079, 2.3492, 1.9751, 2.122, 1.9739, 2.4393, 2.5618, 2.4396, 2.4686, 2.3736, 2.6802, 2.4191, 2.3624, 2.2412, 2.5539, 1.6351, 2.2595, 2.3066, 2.1377, 2.4346, 2.4578, 2.4314, 2.5174, 2.189, 2.2203, 2.2186, 2.3101, 2.3044, 2.5321, 2.2374, 2.3818, 2.221, 2.5959, 2.5709, 1.791, 2.5091, 1.9524, 2.516, 2.5493, 2.5325, 2.1487, 2.2556, 2.2773, 2.6178, 2.3811, 2.4253, 2.5589, 2.382, 1.9812, 2.377, 2.0085, 1.9517, 1.9047, 2.3838, 2.3961, 2.269, 2.3651, 2.3404, 1.9536, 1.9115, 2.2515, 2.7328, 2.5133, 2.4768, 2.4163, 2.2842, 2.358, 2.3991, 2.526, 1.9418, 1.8842, 2.2353, 2.4453, 2.598, 2.1155, 2.3939, 1.5527, 2.2394, 2.4257, 2.428, 2.3401, 2.3807, 2.4056, 1.8727, 2.5444, 2.4916, 2.4063, 2.5476, 2.0474, 2.3693, 1.9657, 2.0756, 2.018, 2.3477, 2.1275, 2.5052, 2.1747, 2.3927, 2.4428, 2.3204, 2.48, 2.3799, 2.5457, 1.788, 2.0195, 2.243, 2.5639, 2.3298, 2.2742, 2.3006, 2.377, 2.5139, 1.9821, 2.3341, 2.0307, 2.132, 1.9207, 2.5217, 2.5595, 2.1439, 1.684, 2.2022, 2.3659, 2.1586, 2.3567, 2.5849, 2.3626, 2.2827, 2.4519, 2.4526, 2.0633, 2.6439, 2.5021, 2.4516, 2.4367, 1.5709, 1.8484, 2.524, 2.4857, 2.3472, 2.3743, 1.8124, 2.1501, 2.1438, 2.2817, 2.4706, 2.1033, 2.3116, 1.8791, 1.6668, 2.434, 2.448, 1.6059, 2.0038, 2.3339, 2.4487, 1.8245, 2.0038, 2.2862, 2.3981, 1.9918, 2.1162, 2.3567, 2.2988, 2.059, 2.2973, 2.0059, 1.8038, 2.1333, 1.7968, 2.5974, 2.3542, 2.2487, 2.4594, 2.5943, 1.718, 1.914, 2.5472, 2.4632, 2.4397, 2.4071, 2.1286, 1.8251, 2.3546, 2.1084, 2.3725, 2.0724, 2.4289, 2.5548, 2.3852, 2.1649, 2.3737, 2.3819, 2.0772, 2.2822, 2.1606, 2.504, 2.1088, 2.4679, 1.5954, 2.6304, 1.9429, 2.4314, 2.421, 2.589, 2.1892, 2.4207, 1.8837, 2.4689, 2.4085, 2.2736, 2.5142, 2.3901, 1.5968, 2.5272, 2.4961, 2.231, 2.4074, 2.3103, 2.2579, 2.5047, 2.3929, 2.4726, 2.234, 2.3404, 2.4067, 2.332, 1.8442, 2.4626, 2.3521, 2.5063, 2.3523, 2.3707, 2.4369, 2.2183, 1.9827, 1.9796, 2.6123, 2.2932, 2.5052, 2.5145, 1.9197, 1.829, 2.4869, 2.3578, 2.415, 2.4313, 2.4087, 2.3841, 2.4685, 2.5095, 2.3869, 2.0015, 2.412, 2.3526, 2.3656, 1.8007, 2.4156, 2.249, 2.0701, 1.6678, 1.9473, 2.6198, 1.9006, 2.3773, 2.5167, 2.2261, 2.4606, 2.3275, 2.5193, 2.4568, 2.4182, 2.4145, 2.495, 1.4246, 2.5499, 1.9105, 2.3533, 2.3976, 2.0495, 2.3102, 2.276, 2.377, 2.4537, 2.4426, 2.4263, 2.4493, 1.6912, 1.9721, 2.5418, 1.9353, 2.5067, 1.7951, 1.6923, 2.0984, 2.0026, 2.453, 2.4513, 1.5337, 2.4578, 2.3704, 2.3667, 2.2136, 2.4291, 2.3769, 2.4977, 2.4214, 2.3647, 2.3872, 2.2132, 2.3732, 2.0081, 2.3607, 1.9264, 2.3962, 2.2067, 2.2542, 2.5795, 2.5694, 2.4123, 2.2225, 1.9311, 2.5225, 2.3456, 1.9375, 2.5436, 2.3971, 2.513, 1.9612, 2.4237, 2.3405, 2.3761, 2.2831, 2.3901, 2.4375, 2.1114, 2.6157, 2.2026, 2.0742, 2.2426, 2.4091, 2.3021, 2.0325, 2.3913, 2.1227, 1.876, 2.4137, 2.5635, 2.366, 2.0984, 1.8708, 2.5603, 2.3878, 1.946, 2.108, 1.9738, 2.6182, 2.5013, 2.5026, 2.5893, 2.0751, 2.3467, 2.5245, 2.0979, 2.2663, 2.3765, 2.4482, 2.2601, 2.4451, 2.3803, 1.9786, 2.4063, 2.6173, 2.1798, 1.7874, 2.5054, 2.0726, 2.3876, 2.6385, 2.4565, 2.1853, 2.4329, 2.2895, 2.0855, 1.9459, 1.9486, 2.4721, 2.3284, 2.3844, 1.8814, 1.8286, 2.1565, 2.0829, 2.6205, 2.3789, 2.4466, 2.2579, 2.2938, 2.4229, 2.1487, 2.3016, 2.2558, 2.2809, 2.3674, 2.6918, 2.3746, 2.2638, 1.8778, 2.1961, 2.5803, 2.5232, 2.4049, 2.1459, 2.4745, 2.5364, 1.4886, 2.2879, 2.1964, 2.0431, 2.1362, 2.3683, 2.4376, 1.9512, 2.5239, 2.5199, 2.2754, 2.5816, 2.569, 1.9669, 1.8418, 2.0828, 2.3683, 2.2632, 2.4932, 2.4029, 2.4143, 2.5643, 1.9672, 2.1851, 2.3752, 2.4782, 2.0486, 2.2324, 2.7061, 1.4857, 2.1947, 2.5018, 2.4042, 2.1819, 2.181, 2.3593, 2.395, 1.7746, 2.1527, 1.9185, 2.3052, 2.3878, 2.0213, 2.4494, 2.2068, 2.5471, 2.1258, 2.341, 1.8244, 2.3833, 1.6807, 1.8656, 2.0192, 1.4795, 2.5564, 2.4686, 2.024, 2.4303, 2.4373, 2.0493, 2.5283, 2.4541, 2.4679, 2.2399, 2.5629, 2.3774, 1.9121, 1.9323, 2.4584, 2.5194, 2.4632, 1.7487, 1.866, 2.6383, 2.5122, 2.1721, 2.2059, 2.376, 2.4396, 2.4427, 2.387, 2.3521, 2.3811, 1.8752, 2.0644, 2.35, 2.3346, 2.5052, 2.3594, 2.2365, 2.6391, 2.5129, 1.8765, 1.9496, 2.429, 2.502, 2.298, 2.2168, 2.2765, 2.1806, 1.8456, 1.9992, 2.5686, 2.5416, 2.2724, 2.4581, 1.8671, 1.7919, 2.3471, 2.6498, 2.1716, 2.4754, 1.9546, 2.0121, 2.3554, 2.3365, 2.4028, 2.0753, 2.2989, 1.9584, 2.5637, 2.4544, 2.2568, 2.2581, 2.0396, 2.151, 2.3935, 2.0289, 2.3769, 2.2509, 2.1125, 2.3026, 2.2659, 2.4824, 2.4973]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "!pip install transformers tokenizers datasets\n",
        "\n",
        "from transformers import GPT2Config, GPT2LMHeadModel, GPT2TokenizerFast, Trainer, TrainingArguments, TextDataset, DataCollatorForLanguageModeling, TrainerCallback\n",
        "from tokenizers import ByteLevelBPETokenizer\n",
        "import torch\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from google.colab import drive\n",
        "\n",
        "# Mount Google Drive for persistent storage\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Set up training directory\n",
        "training_dir = \"/content/drive/MyDrive/gpt2_training\"\n",
        "os.makedirs(training_dir, exist_ok=True)\n",
        "os.chdir(training_dir)\n",
        "\n",
        "# File path for the dataset\n",
        "file_path = \"/content/pastic_basic_filter.txt\"  # Adjust based on upload location\n",
        "fallback_text = \"This is a sample text to count tokens. It contains multiple sentences.\"\n",
        "\n",
        "# Step 1: Train or load the tokenizer\n",
        "def train_or_load_tokenizer(data_path, vocab_size=50257):  # Standard GPT-2 vocab size\n",
        "    tokenizer_dir = \"tokenizer\"\n",
        "    # Force retraining by checking for a specific flag or removing existing tokenizer\n",
        "    if not os.path.exists(os.path.join(tokenizer_dir, \"vocab.json\")) or os.path.getsize(os.path.join(tokenizer_dir, \"vocab.json\")) == 0:\n",
        "        print(f\"Training tokenizer on: {data_path}\")\n",
        "        if not os.path.exists(data_path):\n",
        "            raise FileNotFoundError(f\"Training file not found: {data_path}. Please upload the dataset file or adjust the file_path.\")\n",
        "        tokenizer = ByteLevelBPETokenizer()\n",
        "        tokenizer.train(files=[data_path], vocab_size=vocab_size, min_frequency=2, special_tokens=[\n",
        "            \"<s>\", \"<pad>\", \"</s>\", \"<unk>\", \"<mask>\"\n",
        "        ])\n",
        "        os.makedirs(tokenizer_dir, exist_ok=True)\n",
        "        tokenizer.save_model(tokenizer_dir)\n",
        "        print(\"Tokenizer trained and saved.\")\n",
        "    else:\n",
        "        tokenizer = ByteLevelBPETokenizer.from_file(\n",
        "            os.path.join(tokenizer_dir, \"vocab.json\"),\n",
        "            os.path.join(tokenizer_dir, \"merges.txt\")\n",
        "        )\n",
        "        print(\"Tokenizer loaded from existing files.\")\n",
        "    return tokenizer\n",
        "\n",
        "# Train or load the ByteLevelBPETokenizer\n",
        "if not os.path.exists(file_path):\n",
        "    print(f\"Dataset file not found: {file_path}. Creating a temporary file with fallback text.\")\n",
        "    temp_file = \"temp_fallback.txt\"\n",
        "    with open(temp_file, \"w\", encoding=\"utf-8\") as f:\n",
        "        f.write(fallback_text)\n",
        "    bpe_tokenizer = train_or_load_tokenizer(temp_file, vocab_size=1000)  # Smaller vocab for fallback\n",
        "    os.remove(temp_file)  # Clean up temporary file\n",
        "else:\n",
        "    bpe_tokenizer = train_or_load_tokenizer(file_path)\n",
        "\n",
        "# Step 2: Count tokens in the file\n",
        "def count_tokens(tokenizer, input_text):\n",
        "    encoded = tokenizer.encode(input_text)\n",
        "    return len(encoded.ids), encoded.tokens\n",
        "\n",
        "def count_tokens_in_file(tokenizer, file_path):\n",
        "    total_tokens = 0\n",
        "    with open(file_path, 'r', encoding='utf-8') as f:\n",
        "        while True:\n",
        "            chunk = f.read(1024 * 1024)  # Read 1MB at a time\n",
        "            if not chunk:\n",
        "                break\n",
        "            token_count, _ = count_tokens(tokenizer, chunk)\n",
        "            total_tokens += token_count\n",
        "    return total_tokens\n",
        "\n",
        "# Verify file and count tokens\n",
        "if os.path.exists(file_path):\n",
        "    print(f\"File size: {os.path.getsize(file_path)} bytes\")\n",
        "    with open(file_path, 'r', encoding='utf-8') as f:\n",
        "        preview_text = f.read(1000)  # Read first 1000 chars for preview\n",
        "        print(f\"First 100 characters: {preview_text[:100]}...\")\n",
        "    total_tokens = count_tokens_in_file(bpe_tokenizer, file_path)\n",
        "    print(f\"Total number of tokens: {total_tokens}\")\n",
        "else:\n",
        "    print(f\"File not found: {file_path}. Using fallback text.\")\n",
        "    token_count, tokens = count_tokens(bpe_tokenizer, fallback_text)\n",
        "    print(f\"Total number of tokens (using fallback text): {token_count}\")\n",
        "    print(f\"Tokens: {tokens[:10]}...\")\n",
        "    # Continue with fallback text for tokenization but warn about limited training\n",
        "    with open(\"temp_fallback.txt\", \"w\", encoding=\"utf-8\") as f:\n",
        "        f.write(fallback_text)\n",
        "    file_path = \"temp_fallback.txt\"\n",
        "\n",
        "# Step 3: Load tokenizer for GPT-2 training\n",
        "tokenizer = GPT2TokenizerFast.from_pretrained(\"tokenizer\")\n",
        "tokenizer.pad_token = \"<pad>\"\n",
        "tokenizer.bos_token = \"<s>\"\n",
        "tokenizer.eos_token = \"</s>\"\n",
        "\n",
        "# Step 4: Dataset preparation with train-validation split\n",
        "def load_and_split_dataset(file_path, tokenizer, block_size=1024, val_ratio=0.1):\n",
        "    if not os.path.exists(file_path):\n",
        "        raise FileNotFoundError(f\"Dataset file not found: {file_path}. Please upload the dataset file.\")\n",
        "    with open(file_path, 'r', encoding='utf-8') as f:\n",
        "        lines = f.readlines()\n",
        "    train_lines, val_lines = train_test_split(lines, test_size=val_ratio)\n",
        "    with open(\"train.txt\", \"w\", encoding='utf-8') as f:\n",
        "        f.writelines(train_lines)\n",
        "    with open(\"valid.txt\", \"w\", encoding='utf-8') as f:\n",
        "        f.writelines(val_lines)\n",
        "    train_dataset = TextDataset(tokenizer=tokenizer, file_path=\"train.txt\", block_size=block_size)\n",
        "    val_dataset = TextDataset(tokenizer=tokenizer, file_path=\"valid.txt\", block_size=block_size)\n",
        "    return train_dataset, val_dataset\n",
        "\n",
        "# Load dataset\n",
        "train_dataset, val_dataset = load_and_split_dataset(file_path, tokenizer)\n",
        "print(f\"Training dataset size: {len(train_dataset)} examples\")\n",
        "print(f\"Validation dataset size: {len(val_dataset)} examples\")\n",
        "\n",
        "# Step 5: Data Collator\n",
        "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
        "\n",
        "# Step 6: Model Configuration and Initialization\n",
        "config = GPT2Config(\n",
        "    vocab_size=tokenizer.vocab_size,\n",
        "    n_positions=1024,\n",
        "    n_ctx=1024,\n",
        "    n_embd=768,  # Increased from 768 to 1024\n",
        "    n_layer=24,   # Increased from 14 to 24\n",
        "    n_head=16,    # Increased from 12 to 16\n",
        "    bos_token_id=tokenizer.bos_token_id,\n",
        "    eos_token_id=tokenizer.eos_token_id\n",
        ")\n",
        "model = GPT2LMHeadModel(config)\n",
        "\n",
        "# Print the number of parameters in the model\n",
        "def count_parameters(model):\n",
        "    total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "    print(f\"Total number of trainable parameters: {total_params:,}\")\n",
        "    return total_params\n",
        "\n",
        "count_parameters(model)\n",
        "\n",
        "# Step 7: Custom callback to collect losses\n",
        "class LossLogger(TrainerCallback):\n",
        "    def __init__(self):\n",
        "        self.losses = []\n",
        "        self.steps = []\n",
        "    def on_log(self, args, state, control, logs=None, **kwargs):\n",
        "        print(f\"Logs at step {state.global_step}: {logs}\")\n",
        "        if logs and \"loss\" in logs:\n",
        "            self.losses.append(logs[\"loss\"])\n",
        "            self.steps.append(state.global_step)\n",
        "            print(f\"Captured loss: {logs['loss']} at step {state.global_step}\")\n",
        "\n",
        "loss_logger = LossLogger()\n",
        "\n",
        "# Step 8: Training Configuration\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"gpt2-from-scratch\",\n",
        "    overwrite_output_dir=True,\n",
        "    num_train_epochs=3,\n",
        "    per_device_train_batch_size=2,\n",
        "    save_steps=500,\n",
        "    save_total_limit=2,\n",
        "    logging_steps=1,\n",
        "    logging_dir=\"logs\",\n",
        "    report_to=[\"tensorboard\"],\n",
        "    do_eval=True,\n",
        "    eval_steps=10\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    data_collator=data_collator,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=val_dataset,\n",
        "    callbacks=[loss_logger]\n",
        ")\n",
        "\n",
        "# Step 9: Train and Save\n",
        "try:\n",
        "    trainer.train()\n",
        "except Exception as e:\n",
        "    print(f\"Training failed with error: {e}\")\n",
        "\n",
        "model.save_pretrained(\"gpt2-from-scratch\")\n",
        "tokenizer.save_pretrained(\"gpt2-from-scratch\")\n",
        "print(\"✅ Training complete. Model and tokenizer saved.\")\n",
        "\n",
        "# Step 10: Plot training loss\n",
        "print(f\"Steps: {loss_logger.steps}\")\n",
        "print(f\"Losses: {loss_logger.losses}\")\n",
        "if len(loss_logger.steps) == 0 or len(loss_logger.losses) == 0:\n",
        "    print(\"No loss data captured. Check logging configuration or dataset size.\")\n",
        "else:\n",
        "    plt.plot(loss_logger.steps, loss_logger.losses)\n",
        "    plt.xlabel(\"Steps\")\n",
        "    plt.ylabel(\"Loss\")\n",
        "    plt.title(\"Training Loss Over Time\")\n",
        "    plt.grid(True)\n",
        "    plt.savefig('training_loss.png')  # Save plot instead of showing\n",
        "\n",
        "# Clean up temporary file if created\n",
        "if os.path.exists(\"temp_fallback.txt\"):\n",
        "    os.remove(\"temp_fallback.txt\")"
      ]
    }
  ]
}
